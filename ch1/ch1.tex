\documentclass[../main.tex]{subfiles}
\begin{document}
	
	The finite element method is a powerful tool for solving differential equations.
	The method can easily deal with complex geometries and higher-order approximations of the solution. Figure \ref{fig:img_1} shows a two-dimensional domain with a non-trivial
	geometry. The idea is to divide the domain into triangles (elements) and seek
	a polynomial approximations to the unknown functions on each triangle. The
	method glues these piecewise approximations together to find a global solution.
	Linear and quadratic polynomials over the triangles are particularly popular.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_1}
		\caption{Domain for flow around a dolphin.}
		\label{fig:img_1}
	\end{figure}
	
	Many successful numerical methods for differential equations, including the finite element method, aim at approximating the unknown function by a sum
	\begin{equation}\label{eqa1}
		u(x)=\sum_{i=0}^{N} c_{i} \psi_{i}(x),
	\end{equation}
	where $\psi_{i}(x)$ are prescribed functions and $c_{0}, \ldots, c_{N}$ are unknown coefficients to be determined. Solution methods for differential equations utilizing (\ref{eqa1}) must have a \textit{principle} for constructing $N+1$ equations to determine $c_{0}, \ldots, c_{N}$. Then there is a \textit{machinery} regarding the actual constructions of the equations for $c_{0}, \ldots, c_{N}$, in a particular problem. Finally, there is a solve phase for computing the solution $c_{0}, \ldots, c_{N}$ of the $N+1$ equations.
	
	Especially in the finite element method, the machinery for constructing the discrete equations to be implemented on a computer is quite comprehensive, with many mathematical and implementational details entering the scene at the same time. From an ease-of-learning perspective it can therefore be wise to introduce the computational machinery for a trivial equation: $u=f$. Solving this equation with $f$ given and $u$ on the form (\ref{eqa1}) means that we seek an approximation $u$ to $f$. This approximation problem has the advantage of introducing most of the finite element toolbox, but with postponing demanding topics related to differential equations (e.g., integration by parts, boundary conditions, and coordinate mappings). This is the reason why we shall first become familiar with finite element \textit{approximation} before addressing finite element methods for differential equations.
	
	First, we refresh some linear algebra concepts about approximating vectors in vector spaces. Second, we extend these concepts to approximating functions in function spaces, using the same principles and the same notation. We present examples on approximating functions by global basis functions with support throughout the entire domain. Third, we introduce the finite element type of local basis functions and explain the computational algorithms for working with such functions. Three types of approximation principles are covered: 1) the least squares method, 2) the $L_{2}$ projection or Galerkin method, and 3 ) interpolation or collocation.
	\chapter{Approximation of vectors}
	\label{chap:chap_1}
	\pagenumbering{arabic}
	\setcounter{page}{1}
	\noindent We shall start with introducing two fundamental methods for determining the
	coefficients $c_{i}$
	in (\ref{eqa1}) and illustrate the methods on approximation of vectors,
	because vectors in vector spaces give a more intuitive understanding than starting
	directly with approximation of functions in function spaces. The extension
	from vectors to functions will be trivial as soon as the fundamental ideas are
	understood.
	
	The first method of approximation is called the \textit{least squares} method and
	consists in finding $c_{i}$ such that the difference \textit{u} - \textit{f}, measured in some norm, is
	minimized. That is, we aim at finding the best approximation \textit{u} to \textit{f} (in some
	norm). The second method is not as intuitive: we find u such that the error
	\textit{u} - \textit{f} is orthogonal to the space where we seek \textit{u}. This is known as \textit{projection},
	or we may also call it a \textit{Galerkin method}. When approximating vectors and
	functions, the two methods are equivalent, but this is no longer the case when
	applying the principles to differential equations
	
	\section[Approximation of planar vectors]{Approximation of planar vectors}
	\label{sec:sec_1_1}
	\noindent Suppose we have given a vector \textit{\textbf{f}} = (3, 5) in the \textit{xy} plane and that we want to
	approximate this vector by a vector aligned in the direction of the vector \textit{(a, b)}.
	Figure \ref{fig:img_2} depicts the situation.
	
	We introduce the vector space \textit{V} spanned by the vector $\psi_{0}$ = (a, b):
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_2}
		\caption{Approximation of a two-dimensional vector by a one-dimensional vector.}
		\label{fig:img_2}
	\end{figure}
	\begin{equation}\label{eqa2}
		V=\operatorname{span}\left\{\psi_{0}\right\}
	\end{equation}
	
	\noindent We say that $\psi_{0}$ is a basis vector in the space $V$. Our aim is to find the vector $\boldsymbol{u}=c_{0} \psi_{0} \in V$ which best approximates the given vector $\boldsymbol{f}=(3,5)$. A reasonable criterion for a best approximation could be to minimize the length of the difference between the approximate u and the given $\boldsymbol{f}$. The difference, or error $e=\boldsymbol{f}-\boldsymbol{u}$, has its length given by the \textit{norm}
	$$
	\|e\|=(e, e)^{\frac{1}{2}},
	$$
	where $(e, e)$ is the \textit{inner product} of $e$ and itself. The inner product, also called \textit{scalar product} or \textit{dot product}, of two vectors $\boldsymbol{u}=\left(u_{0}, u_{1}\right)$ and $\boldsymbol{v}=\left(v_{0}, v_{1}\right)$ is defined as
	\begin{equation}\label{eqa3}
		(\boldsymbol{u}, \boldsymbol{v})=u_{0} v_{0}+u_{1} v_{1}.
	\end{equation}
	
	\noindent \textbf{Remark 1.} We should point out that we use the notation $(\cdot, \cdot)$ for two different things: $(a, b)$ for scalar quantities $a$ and $b$ means the vector starting in the origin and ending in the point $(a, b)$, while $(\boldsymbol{u}, \boldsymbol{v})$ with vectors $\boldsymbol{u}$ and $\boldsymbol{v}$ means the inner product of these vectors. Since vectors are here written in boldface font there should be no confusion. We may add that the norm associated with this inner product is the usual Eucledian length of a vector.
	\bigbreak
	\noindent \textbf{Remark 2.} It might be wise to refresh some basic linear algebra by consulting a textbook. Exercises \hyperref[sec:sec_10_1]{1} and \hyperref[sec:sec_10_1]{2} suggest specific tasks to regain familiarity with fundamental operations on inner product vector spaces.
	\bigbreak
	\noindent \textbf{The least squares method}. We now want to find $c_{0}$ such that it minimizes $\|e\|$. The algebra is simplified if we minimize the square of the norm, $\|e\|^{2}=$ $(e, e)$, instead of the norm itself. Define the function
	\begin{equation}\label{eqa4}
		E\left(c_{0}\right)=(e, e)=\left(\boldsymbol{f}-c_{0} \psi_{0}, \boldsymbol{f}-c_{0} \psi_{0}\right).
	\end{equation}
	We can rewrite the expressions of the right-hand side in a more convenient form for further work:
	\begin{equation}
		\label{eqa5}
		E\left(c_{0}\right)=(\boldsymbol{f}, \boldsymbol{f})-2 c_{0}\left(\boldsymbol{f}, \psi_{0}\right)+c_{0}^{2}\left(\psi_{0}, \psi_{0}\right).
	\end{equation}
	The rewrite results from using the following fundamental rules for inner product spaces:
	
	\begin{equation}\label{eqa6}
		(\alpha u, v)=\alpha(u, v), \quad \alpha \in \mathbb{R},
	\end{equation}
	\begin{equation}\label{eqa7}
		(u + v, w)=(u, w)+(v, w),
	\end{equation}
	\begin{equation}\label{eqa8}
		(u, v)= (v, u)
	\end{equation}
	\indent Minimizing $E\left(c_{0}\right)$ implies finding $c_{0}$ such that
	$$
	\frac{\partial E}{\partial c_{0}}=0.
	$$
	Differentiating (\ref{eqa5}) with respect to $c_{0}$ gives
	\begin{equation}\label{eqa9}
		\frac{\partial E}{\partial c_{0}}=-2\left(\boldsymbol{f}, \psi_{0}\right)+2 c_{0}\left(\psi_{0}, \psi_{0}\right)
	\end{equation}
	Setting the above expression equal to zero and solving for $c_{0}$ gives
	\begin{equation}\label{eqa10}
		c_{0}=\frac{\left(\boldsymbol{f}, \psi_{0}\right)}{\left(\psi_{0}, \psi_{0}\right)}
	\end{equation}
	which in the present case with $\psi_{0}=(a, b)$ results in
	\begin{equation}\label{eqa11}
		c_{0}=\frac{3 a+5 b}{a^{2}+b^{2}}.
	\end{equation}
	For later, it is worth mentioning that setting the key equation (\hyperref[eqa9]{9}) to zero can be rewritten as
	$$
	\left(f-c 0 \psi_{0}, \psi_{0}\right)=0,
	$$
	or
	\begin{equation}\label{eqa12}
		\left(e, \psi_{0}\right)=0.
	\end{equation}
	\textbf{The projection method.} We shall now show that minimizing $\|e\|^{2}$ implies that $\boldsymbol{e}$ is orthogonal to \textit{any} vector $\boldsymbol{v}$ in the space $V$. This result is visually quite clear from Figure \hyperref[fig:img_2]{2} (think of other vectors along the line $(a, b)$ : all of them will lead to a larger distance between the approximation and $\boldsymbol{f})$. To see this result mathematically, we express any $v \in V$ as $v=s \psi_{0}$ for any scalar parameter $s$, recall that two vectors are orthogonal when their inner product vanishes, and calculate the inner product
	$$
	\begin{aligned}
		\left(e, s \psi_{0}\right) &=\left(\boldsymbol{f}-c_{0} \psi_{0}, s \psi_{0}\right) \\
		&=\left(\boldsymbol{f}, s \psi_{0}\right)-\left(c_{0} \psi_{0}, s \psi_{0}\right) \\
		&=s\left(\boldsymbol{f}, \psi_{0}\right)-s c_{0}\left(\psi_{0}, \psi_{0}\right) \\
		&=s\left(\boldsymbol{f}, \psi_{0}\right)-s \frac{\left(\boldsymbol{f}, \psi_{0}\right)}{\left(\psi_{0}, \psi_{0}\right)}\left(\psi_{0}, \psi_{0}\right) \\
		&=s\left(\left(\boldsymbol{f}, \psi_{0}\right)-\left(\boldsymbol{f}, \psi_{0}\right)\right) \\
		&=0 .
	\end{aligned}
	$$
	Therefore, instead of minimizing the square of the norm, we could demand that $\boldsymbol{e}$ is orthogonal to any vector in $V$. This method is known as \textit{projection}, because it is the same as projecting the vector onto the subspace. (The approach can also be referred to as a Galerkin method as explained at the end of Section ??.)
	Mathematically the projection method is stated by the equation
	\begin{equation}\label{eqa13}
		(e, v)=0, \quad \forall v \in V.
	\end{equation}
	An arbitrary $\boldsymbol{v} \in V$ can be expressed as $s \psi_{0}, s \in \mathbb{R}$, and therefore (\hyperref[eqa13]{13}) implies
	$$
	\left(e, s \psi_{0}\right)=s\left(e, \psi_{0}\right)=0
	$$
	which means that the error must be orthogonal to the basis vector in the space $V$ :
	$$
	\left(e, \psi_{0}\right)=0 \quad \text { or } \quad\left(f-c_{0} \psi_{0}, \psi_{0}\right)=0.
	$$
	The latter equation gives (10) and it also arose from least squares computations in (\hyperref[eqa12]{12}).
	
	\section[Approximation of general vectors]{Approximation of general vectors}
	\label{sec:sec_1_2}
	\noindent Let us gencralize the vector approximation from the previous section to vectors in spaces with arbitrary dimension. Given some vector $\boldsymbol{f}$, we want to find the best approximation to this vector in the space
	$$
	V=\operatorname{span}\left\{\psi_{0}, \ldots, \psi_{N}\right\}.
	$$
	We assume that the \textit{basis vectors} $\psi_{0}, \ldots, \psi_{N}$ are linearly independent so that none of them are redundant and the space has dimension $N+1$. Any vector $\boldsymbol{u} \in V$ can be written as a linear combination of the basis vectors,
	$$
	\boldsymbol{u}=\sum_{j=0}^{N} c_{j} \psi_{j},
	$$
	where $c_{j} \in \mathbb{R}$ are scalar coefficients to be determined.
	\bigbreak
	\noindent \textbf{The least squares method.} Now we want to find $c_{0}, \ldots, c_{N}$, such that u is the best approximation to $\boldsymbol{f}$ in the sense that the distance (error) $e=\boldsymbol{f}-u$ is minimized. Again, we define the squared distance as a function of the free parameters $c_{0}, \ldots, c_{N}$,
	\begin{equation}\label{eqa14}
		\begin{aligned}
			E\left(c_{0}, \ldots, c_{N}\right) &=(e, e)=\left(\boldsymbol{f}-\sum_{j} c_{j} \psi_{j}, \boldsymbol{f}-\sum_{j} c_{j} \psi_{j}\right) \\
			&=(\boldsymbol{f}, \boldsymbol{f})-2 \sum_{j=0}^{N} c_{j}\left(\boldsymbol{f}, \psi_{j}\right)+\sum_{p=0}^{N} \sum_{q=0}^{N} c_{p} c_{q}\left(\psi_{p}, \psi_{q}\right)
		\end{aligned}
	\end{equation}
	Minimizing this $E$ with respect to the independent variables $c_{0}, \ldots, c_{N}$ is obtained by requiring
	$$
	\frac{\partial E}{\partial c_{i}}=0, \quad i=0, \ldots, N
	$$
	The second term in (\hyperref[eqa14]{14}) is differentiated as follows:
	\begin{equation}\label{eqa15}
		\frac{\partial}{\partial c_{i}} \sum_{j=0}^{N} c_{j}\left(\boldsymbol{f}, \psi_{j}\right)=\left(\boldsymbol{f}, \psi_{i}\right),
	\end{equation}
	since the expression to be differentiated is a sum and only one term, $c_{i}\left(\boldsymbol{f}, \psi_{i}\right)$, contains $c_{i}$ and this term is linear in $c_{i}$. To understand this differentiation in detail, write out the sum specifically for, e.g, $N=3$ and $i=1$.
	
	The last term in (\hyperref[eqa14]{14}) is more tedious to differentiate. We start with
	\begin{equation}\label{eqa16}
		\frac{\partial}{\partial c_{i}} c_{p} c_{q}= \begin{cases}0, & \text { if } p \neq i \text { and } q \neq i, \\ c_{q}, & \text { if } p=i \text { and } q \neq i \\ c_{p}, & \text { if } p \neq i \text { and } q=i, \\ 2 c_{i}, & \text { if } p=q=i,\end{cases}
	\end{equation}
	Then
	$$
	\frac{\partial}{\partial c_{i}} \sum_{p=0}^{N} \sum_{q=0}^{N} c_{p} c_{q}\left(\psi_{p}, \psi_{q}\right)=\sum_{p=0, p \neq i}^{N} c_{p}\left(\psi_{p}, \psi_{i}\right)+\sum_{q=0, q \neq i}^{N} c_{q}\left(\psi_{q}, \psi_{i}\right)+2 c_{i}\left(\psi_{i}, \psi_{i}\right).
	$$
	The last term can be included in the other two sums, resulting in
	\begin{equation}\label{eqa17}
		\frac{\partial}{\partial c_{i}} \sum_{p=0}^{N} \sum_{q=0}^{N} c_{p} c_{q}\left(\psi_{p}, \psi_{q}\right)=2 \sum_{j=0}^{N} c_{i}\left(\psi_{j}, \psi_{i}\right).
	\end{equation}
	It then follows that setting
	\begin{equation}\label{eqa18}
		\frac{\partial E}{\partial c_{i}}=0, \quad i=0, \ldots, N,
		$$
		leads to a linear system for $c_{0}, \ldots, c_{N}$ :
		$$
		\sum_{j=0}^{N} A_{i, j} c_{j}=b_{i}, \quad i=0, \ldots, N,
	\end{equation}	
	where
	\begin{equation}\label{eqa19}
		\begin{aligned}
			A_{i, j} &=\left(\psi_{i}, \psi_{j}\right),
		\end{aligned}
	\end{equation}
	\begin{equation}\label{eqa20}
		\begin{aligned}
			b_{i} &=\left(\psi_{i}, \boldsymbol{f}\right).
		\end{aligned}
	\end{equation}
	We have changed the order of the two vectors in the inner product according to (\hyperref[sec:sec_1_1]{1.1}):
	$$
	A_{i, j}=\left(\psi_{j}, \psi_{i}\right)=\left(\psi_{i}, \psi_{j}\right),
	$$
	simply because the sequence $i$ - $j$ looks more aesthetic.
	\bigbreak
	\noindent \textbf{The Galerkin or projection method.} In analogy with the "one-dimensional" example in Section \hyperref[sec:sec_1_1]{1.1}, it holds also here in the general case that minimizing the distance (error) $\boldsymbol{e}$ is equivalent to demanding that $\boldsymbol{e}$ is orthogonal to all $\boldsymbol{v} \in V$ :
	\begin{equation}\label{eqa21}
		(e, v)=0, \quad \forall v \in V.
	\end{equation}
	Since any $\boldsymbol{v} \in V$ can be written as $\boldsymbol{v}=\sum_{i=0}^{N} c_{i} \psi_{i}$, the statement (\hyperref[eqa21]{21}) is equivalent to saying that
	$$
	\left(e, \sum_{i=0}^{N} c_{i} \psi_{i}\right)=0,
	$$
	for any choice of coefficients $c_{0}, \ldots, c_{N}$. The latter equation can be rewritten as
	
	$$
	\sum_{i=0}^{N} c_{i}\left(e, \psi_{i}\right)=0.
	$$
	If this is to hold for arbitrary values of $c_{0}, \ldots, c_{N}$ we must require that each term in the sum vanishes,
	\begin{equation}\label{eqa22}
		\left(e, \psi_{i}\right)=0, \quad i=0, \ldots, N.
	\end{equation}
	These $N+1$ equations result in the same linear system as (\hyperref[eqa18]{18}):
	$$
	\left(\boldsymbol{f}-\sum_{j=0}^{N} c_{j} \psi_{j}, \psi_{i}\right)=\left(\boldsymbol{f}, \psi_{i}\right)-\sum_{j \in \mathcal{I}_{\mathrm{a}}}\left(\psi_{i}, \psi_{j}\right) c_{j}=0,
	$$
	and hence
	$$
	\sum_{j=0}^{N}\left(\psi_{i}, \psi_{j}\right) c_{j}=\left(\boldsymbol{f}, \psi_{i}\right), \quad i=0, \ldots, N.
	$$
	So, instead of differentiating the $E\left(c_{0}, \ldots, c_{N}\right)$ function, we could simply use (\hyperref[eqa21]{21}) as the principle for determining $c_{0}, \ldots, c_{N}$, resulting in the $N+1$ equations (\hyperref[eqa22]{22}).
	
	The names \textit{least squares method} or \textit{least squares approximation} are natural since the calculations consists of minimizing $\|e\|^{2}$, and $\|e\|^{2}$ is a sum of squares of differences between the components in $\boldsymbol{f}$ and $\boldsymbol{u}$. We find $\boldsymbol{u}$ such that this sum of squares is minimized.
	
	The principle (\hyperref[eqa21]{21}), or the equivalent form (\hyperref[eqa22]{22}), is known as projection. Almost the same mathematical idea was used by the Russian mathematician \href{https://en.wikipedia.org/wiki/Boris_Galerkin}{Boris Galerkin} to solve differential equations, resulting in what is widely known as \textit{Galerkin's method.}
	
	\chapter{Approximation of functions}
	\label{chap:chap_2}
	Let $V$ be a function space spanned by a set of \textit{basis functions} $\psi_{0}, \ldots, \psi_{N}$,
	$$
	V=\operatorname{span}\left\{\psi_{0}, \ldots, \psi_{N}\right\},
	$$
	such that any function $u \in V$ can be written as a linear combination of the basis functions:
	\begin{equation}\label{eqa23}
		u=\sum_{j \in \mathcal{I}_{s}} c_{j} \psi_{j}.
	\end{equation}
	The index set $\mathcal{I}_{s}$ is defined as $\mathcal{I}_{s}=\{0, \ldots, N\}$ and is used both for compact notation and for flexibility in the numbering of elements in sequences.
	
	For now, in this introduction, we shall look at functions of a single variable $x: u=u(x), \psi_{i}=\psi_{i}(x), i \in \mathcal{I}_{s}$. Later, we will almost trivially extend the mathematical details to functions of two- or three-dimensional physical spaces.
	The approximation (\hyperref[eqa23]{23}) is typically used to discretize a problem in space. Other
	methods, most notably finite differences, are common for time discretization,
	although the form (\hyperref[eqa23]{23}) can be used in time as well.
	\section[The least squares method]{The least squares method}
	\label{sec:sec_2_1}
	
	\noindent Given a function $f(x)$, how can we determine its best approximation $u(x) \in V$ ? A natural starting point is to apply the same reasoning as we did for vectors in Section \hyperref[sec:sec_1_2]{1.2}. That is, we minimize the distance between $u$ and $f$. However, this requires a norm for measuring distances, and a norm is most conveniently defined through an inner product. Viewing a function as a vector of infinitely many point values, one for each value of $x$, the inner product could intuitively be defined as the usual summation of pairwise components, with summation replaced by integration:
	$$
	(f, g)=\int f(x) g(x) \mathrm{d} x.
	$$
	To fix the integration domain, we let $f(x)$ and $\psi_{i}(x)$ be defined for a domain $\Omega \subset \mathbb{R}$. The inner product of two functions $f(x)$ and $g(x)$ is then
	\begin{equation}\label{eqa24}
		(f, g)=\int_{\Omega} f(x) g(x) \mathrm{d} x.	
	\end{equation}
	The distance between $f$ and any function $u \in V$ is simply $f-u$, and the squared norm of this distance is
	\begin{equation}\label{eqa25}
		E=\left(f(x)-\sum_{j \in \mathcal{I}_{s}} c_{j} \psi_{j}(x), f(x)-\sum_{j \in \mathcal{I}_{s}} c_{j} \psi_{j}(x)\right).	
	\end{equation}
	
	\noindent Note the analogy with (\hyperref[eqa14]{14}) : the given function $f$ plays the role of the given vector $\boldsymbol{f}$, and the basis function $\psi_{i}$ plays the role of the basis vector $\psi_{i}$. We can rewrite (\hyperref[eqa25]{25}), through similar steps as used for the result (\hyperref[eqa14]{14}), leading to 
	\begin{equation}\label{eqa26}
		E\left(c_{i}, \ldots, c_{N}\right)=(f, f)-2 \sum_{j \in \mathcal{I}_{s}} c_{j}\left(f, \psi_{i}\right)+\sum_{p \in \mathcal{I}_{s}} \sum_{q \in \mathcal{I}_{s}} c_{p} c_{q}\left(\psi_{p}, \psi_{q}\right).	
	\end{equation}
	Minimizing this function of $N+1$ scalar variables $\left\{c_{i}\right\}_{i \subset \mathcal{I}_{\mathrm{s}}}$, requires differentiation with respect to $c_{i}$, for all $i \in \mathcal{I}_{8}$. The resulting cquations are very similar to those we had in the vector case, and we hence end up with a linear system of the form (\hyperref[eqa18]{18}), with basically the same expressions:
	\begin{equation}\label{eqa27}
		A_{i, j} \left(\psi_{i}, \psi_{j}\right),
	\end{equation}
	\begin{equation}\label{eqa28}
		b_{i} \left(f, \psi_{i}\right).
	\end{equation}
	\section[The projection (or Galerkin) method]{The projection (or Galerkin) method}
	\label{sec:sec_2_2}
	As in Section \hyperref[sec:sec_1_2]{1.2}, the minimization of $(e, e)$ is equivalent to
	\begin{equation}\label{eqa29}
		(e, v)=0, \quad \forall v \in V.
	\end{equation}
	This is known as a projection of a function $f$ onto the subspace $V$. We may also call it a Galerkin method for approximating functions. Using the same reasoning as in (\hyperref[eqa21]{21})-(\hyperref[eqa22]{22}), it follows that (\hyperref[eqa29]{29}) is equivalent to
	\begin{equation}\label{eqa30}
		\left(e, \psi_{i}\right)=0, \quad i \in \mathcal{I}_{s}.
	\end{equation}
	Inserting $e=f-u$ in this equation and ordering terms, as in the multidimensional vector case, we end up with a linear system with a coefficient matrix (\hyperref[eqa27]{27}) and right-hand side vector (\hyperref[eqa28]{28}).
	
	Whether we work with vectors in the plane, general vectors, or functions in function spaces, the least squares principle and the projection or Galerkin method are equivalent.
	
	\section[Example: linear approximation]{Example: linear approximation}
	\label{sec:sec_2_3}
	\noindent Let us apply the theory in the previous section to a simple problem: given a parabola $f(x)=10(x-1)^{2}-1$ for $x \in \Omega=[1,2]$, find the best approximation $u(x)$ in the space of all linear functions:
	$$
	V=\operatorname{span}\{1, x\} .
	$$
	With our notation, $\psi_{0}(x)=1, \psi_{1}(x)=x$, and $N=1$. We seek
	$$
	u=c_{0} \psi_{0}(x)+c_{1} \psi_{1}(x)=c_{0}+c_{1} x,
	$$
	where $c_{0}$ and $c_{1}$ are found by solving a $2 \times 2$ the linear system. The coefficient matrix has elements
	\begin{equation}\label{eqa31}
		A_{0,0}=\left(\psi_{0}, \psi_{0}\right)=\int_{1}^{2} 1 \cdot 1 \mathrm{~d} x=1,
	\end{equation}
	\begin{equation}\label{eqa32}
		A_{0,1}=\left(\psi_{0}, \psi_{1}\right)=\int_{1}^{2} 1 \cdot x \mathrm{~d} x=3/2,
	\end{equation}
	\begin{equation}\label{eqa33}
		A_{1,0}=A_{0,1}=3 / 2,
	\end{equation}
	\begin{equation}\label{eqa34}
		A_{1,1}=\left(\psi_{1}, \psi_{1}\right)=\int_{1}^{2} x \cdot x \mathrm{~d} x=7 / 3.
	\end{equation}
	The corresponding right-hand side is
	\begin{equation}\label{eqa35}
		b_{1}=\left(f, \psi_{0}\right)=\int_{1}^{2}\left(10(x-1)^{2}-1\right) \cdot 1 \mathrm{~d} x=7 / 3,
	\end{equation}
	\begin{equation}\label{eqa36}
		b_{2}=\left(f, \psi_{1}\right)=\int_{1}^{2}\left(10(x-1)^{2}-1\right) \cdot x \mathrm{~d} x=13 / 3.
	\end{equation}
	Solving the linear system results in
	\begin{equation}\label{eqa37}
		c_{0}=-38 / 3, \quad c_{1}=10,
	\end{equation}
	and consequently
	\begin{equation}\label{eqa38}
		u(x)=10 x-\frac{38}{3}.
	\end{equation}
	Figure \hyperref[fig:img_3]{3} displays the parabola and its best approximation in the space of all linear functions.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_3}
		\caption{Best approximation of a parabola by a straight line.}
		\label{fig:img_3}
	\end{figure}
	\section[Implementation of the least squares method]{Implementation of the least squares method}
	\label{sec:sec_2_4}
	\noindent The linear system can be computed either symbolically or numerically (a numer-
	ical integration rule is needed in the latter case). Here is a function for symbolic
	computation of the linear system, where f (x) is given as a sympy expression f
	involving the symbol x, psi is a list of expressions for $\left\{\psi_{i}\right\}_{i \in \mathcal{I}_{s}}$, and Omega is a 2-tuple/list holding the limits of the domain $\Omega$ :
	
	\begin{lstlisting}[numbers=none]
		import sympy as sp
		def least_squares(f, psi, Omega):
		N = len(psi) - 1
		A = sp.zeros((N+1, N+1))
		b = sp.zeros((N+1, 1))
		x = sp.Symbol('x')
		for i in range(N+1):
		for j in range(i, N+1):
		A[i,j] = sp.integrate(psi[i]*psi[j],
		(x, Omega[0], Omega[1]))
		A[j,i] = A[i,j]
		b[i,0] = sp.integrate(psi[i]*f, (x, Omega[0], Omega[1]))
		c = A.LUsolve(b)
		u = 0
		for i in range(len(psi)):
		u += c[i,0]*psi[i]
		return u, c
	\end{lstlisting}
	Observe that we exploit the symmetry of the coefficient matrix: only the
	upper triangular part is computed. Symbolic integration in sympy is often
	time consuming, and (roughly) halving the work has noticeable effect on the
	waiting time for the function to finish execution.
	
	Comparing the given f (x) and the approximate u(x) visually is done by
	the following function, which with the aid of sympy's lambdify tool converts a
	sympy expression to a Python function for numerical computations:
	\begin{lstlisting}[numbers=none]
		def comparison_plot(f, u, Omega, filename='tmp.pdf'):
		x = sp.Symbol('x')
		f = sp.lambdify([x], f, modules="numpy")
		u = sp.lambdify([x], u, modules="numpy")
		resolution = 401 # no of points in plot
		xcoor = linspace(Omega[0], Omega[1], resolution)
		exact = f(xcoor)
		approx = u(xcoor)
		plot(xcoor, approx)
		hold('on')
		plot(xcoor, exact)
		legend(['approximation', 'exact'])
		savefig(filename)
	\end{lstlisting}
	The modules='numpy' argument to lambdify is important if there are mathe-
	matical functions, such as sin or exp in the symbolic expressions in f or u, and
	these mathematical functions are to be used with vector arguments, like xcoor
	above.
	
	Both the least\textunderscore squares and comparison\textunderscore plot are found and coded in the
	file \href{http://tinyurl.com/jvzzcfn/fem/approx1D.py}{approx1D.py.} The forthcoming examples on their use appear in ex\textunderscore approx1D.py.
	\section[Perfect approximation]{Perfect approximation}
	\label{sec:sec_2_5}
	Let us use the code above to recompute the problem from Section \hyperref[sec:sec_2_3]{2.3} where we want to approximate a parabola. What happens if we add an element $x^{2}$ to the basis and test what the best approximation is if V is the space of all parabolic
	functions? The answer is quickly found by running
	\begin{lstlisting}[numbers=none]
		>>> from approx1D import *
		>>> x = sp.Symbol('x')
		>>> f = 10*(x-1)**2-1
		>>> u, c = least_squares(f=f, psi=[1, x, x**2], Omega=[1, 2])
		>>> print u
		10*x**2 - 20*x + 9
		>>> print sp.expand(f)
		10*x**2 - 20*x + 9
	\end{lstlisting}
	Now, what if we use $\psi_{i}(x)=x^{i}$ for $i=0,1, \ldots, N=40$ ? The output from least\textunderscore squares gives $c_{i}=0$ for $i>2$, which means that the method finds the perfect approximation.
	
	In fact, we have a general result that if $f \in V$, the least squares and projection/Galerkin methods compute the exact solution $u=f$. The proof is straightforward: if $f \in V, f$ can be expanded in terms of the basis functions, $f=\sum_{j \in \mathcal{I}_{s}} d_{j} \psi_{j}$, for some coefficients $\left\{d_{i}\right\}_{i \in \mathcal{I}_{s}}$, and the right-hand side then has entries
	$$
	b_{i}=\left(f, \psi_{i}\right)=\sum_{j \in \mathcal{I}_{s}} d_{j}\left(\psi_{j}, \psi_{i}\right)=\sum_{j \in \mathcal{I}_{s}} d_{j} A_{i, j}.
	$$
	The linear system $\sum_{j} A_{i, j} c_{j}=b_{i}, i \in \mathcal{I}_{s}$, is then
	$$
	\sum_{j \in \mathcal{I}_{s}} c_{j} A_{i, j}=\sum_{j \in \mathcal{I}_{s}} d_{j} A_{i, j}, \quad i \in \mathcal{I}_{s},
	$$
	which implies that $c_{i}=d_{i}$ for $i \in \mathcal{I}_{s}$.
	\section[Ill-conditioning]{Ill-conditioning}
	\label{sec:sec_2_6}
	The computational example in Section \hyperref[sec:sec_2_5]{2.5} applies the least\textunderscore squares function which invokes symbolic methods to calculate and solve the linear system. The correct solution $c_{0}=9, c_{1}=-20, c_{2}=10, c_{i}=0$ for $i \geq 3$ is perfectly recovered.
	
	Suppose we convert the matrix and right-hand side to floating-point arrays and then solve the system using finite-precision arithmetics, which is what one will (almost) always do in real life. This time we get astonishing results! Up to about $N=7$ we get a solution that is reasonably close to the exact one. Increasing $N$ shows that seriously wrong coefficients are computed. Below is a table showing the solution of the linear system arising from approximating a parabola by functions on the form $u(x)=c_{0}+c_{1} x+c_{2} x^{2}+\cdots+c_{10} x^{10}$. Analytically, we know that $c_{j}=0$ for $j>2$, but numerically we may get $c_{j} \neq 0$ for $j>2$.
	
	\begin{tabular}{rrrr}
		\hline exact & sympy & numpy32 & numpy64 \\
		\hline 9 & $9.62$ & $5.57$ & $8.98$ \\
		$-20$ & $-23.39$ & $-7.65$ & $-19.93$ \\
		10 & $17.74$ & $-4.50$ & $9.96$ \\
		0 & $-9.19$ & $4.13$ & $-0.26$ \\
		0 & $5.25$ & $2.99$ & $0.72$ \\
		0 & $0.18$ & $-1.21$ & $-0.93$ \\
		0 & $-2.48$ & $-0.41$ & $0.73$ \\
		0 & $1.81$ & $-0.013$ & $-0.36$ \\
		0 & $-0.66$ & $0.08$ & $0.11$ \\
		0 & $0.12$ & $0.04$ & $-0.02$ \\
		0 & $-0.001$ & $-0.02$ & $0.002$ \\
		\hline
	\end{tabular}
	\bigbreak
	\noindent The exact value of $c_{j}, j=0,1, \ldots, 10$, appears in the first column while the other columns correspond to results obtained by three different methods:
	\begin{itemize}
		\item Column 2: The matrix and vector are converted to the data structure sympy .mpmath.fp.matrix and the sympy.mpmath.fp.lu\textunderscore solve function is used to solve the system.
		\item Column 3: The matrix and vector are converted to numpy arrays with data type numpy.float32 (single precision floating-point number) and solved by the numpy.linalg.solve function.
		\item Column 4: As column 3, but the data type is numpy.float64 (double precision floating-point number).
	\end{itemize}
	
	\noindent We see from the numbers in the table that double precision performs much better than single precision. Nevertheless, when plotting all these solutions the curves cannot be visually distinguished (!). This means that the approximations look perfect, despite the partially very wrong values of the coefficients.
	
	Increasing $N$ to 12 makes the numerical solver in numpy abort with the message: "matrix is numerically singular". A matrix has to be non-singular to be invertible, which is a requirement when solving a linear system. Already when the matrix is close to singular, it is \textit{ill-conditioned}, which here implies that the numerical solution algorithms are sensitive to round-off errors and may produce (very) inaccurate results.
	
	The reason why the coefficient matrix is nearly singular and ill-conditioned is that our basis functions $\psi_{i}(x)=x^{i}$ are nearly linearly dependent for large $i$. That is, $x^{i}$ and $x^{i+1}$ are very close for $i$ not very small. This phenomenon is illustrated in Figure 4. There are 15 lines in this figure, but only half of them are visually distinguishable. Almost linearly dependent basis functions give rise to an ill-conditioned and almost singular matrix. This fact can be illustrated by computing the determinant, which is indeed very close to zero (recall that a zero determinant implies a singular and non-invertible matrix): $10^{-65}$ for $N=10$ and $10^{-92}$ for $N=12$. Already for $N=28$ the numerical determinant computation returns a plain zero.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_4}
		\caption{The 15 first basis functions $x^{i}, i=0, \ldots, 14$.}
		\label{fig:img_4}
	\end{figure}
	On the other hand, the double precision numpy solver do run for $N=100$, resulting in answers that are not significantly worse than those in the table above, and large powers are associated with small coefficients (e.g., $c_{j}<10^{-2}$ for $10 \leq j \leq 20$ and $c<10^{-5}$ for $\left.j>20\right)$. Even for $N=100$ the approximation still lies on top of the exact curve in a plot (!).
	
	The conclusion is that visual inspection of the quality of the approximation may not uncover fundamental numerical problems with the computations. However, numerical analysts have studied approximations and ill-conditioning for decades, and it is well known that the basis \{1, $x^{2}$, $x^{3}$, \ldots, \} is a bad basis. The best basis from a matrix conditioning point of view is to have orthogonal functions such that $\left(\psi_{i}, \psi_{j}\right)=0$ for $i \neq j$. There are many known sets of orthogonal polynomials and other functions. The functions used in the finite element methods are almost orthogonal, and this property helps to avoid problems with solving matrix systems. Almost orthogonal is helpful, but not enough when it comes to partial differential equations, and ill-conditioning of the coefficient matrix is a theme when solving large-scale matrix systems arising from finite element discretizations.
	\bigbreak
	\section[Fourier series]{Fourier series}
	\label{sec:sec_2_7}
	\noindent A set of sine functions is widely used for approximating functions (the sines are
	also orthogonal as explained more in Section \hyperref[sec:sec_2_6]{2.6}). Let us take
	
	$$
	V=\operatorname{span}\{\sin \pi x, \sin 2 \pi x, \ldots, \sin (N+1) \pi x\}.
	$$
	That is,
	$$
	\psi_{i}(x)=\sin ((i+1) \pi x), \quad i \in \mathcal{I}_{s} .
	$$
	An approximation to the $f(x)$ function from Section \hyperref[sec:sec_2_3]{2.3} can then be computed by the least\textunderscore squares function from Section \hyperref[sec:sec_2_4]{2.4}:
	\begin{lstlisting}[numbers=none]
		N = 3
		from sympy import sin, pi
		x = sp.Symbol('x')
		psi = [sin(pi*(i+1)*x) for i in range(N+1)]
		f = 10*(x-1)**2 - 1
		Omega = [0, 1]
		u, c = least_squares(f, psi, Omega)
		comparison_plot(f, u, Omega)
	\end{lstlisting}
	Figure \hyperref[fig:img_5]{5} (left) shows the oscillatory approximation of $\sum_{j-0}^{N} c_{j} \sin ((j+1) \pi x)$ when $N=3$. Changing $N$ to 11 improves the approximation considerably, see Figure \hyperref[fig:img_5]{5} (right).
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_5}
		\caption{Best approximation of a parabola by a sum of 3 (left) and 11 (right) sine functions.}
		\label{fig:img_5}
	\end{figure}
	
	There is an error $f(0)-u(0)=9$ at $x=0$ in Figure \hyperref[fig:img_5]{5} regardless of how large $N$ is, because all $\psi_{i}(0)=0$ and hence $u(0)=0$. We may help the approximation to be correct at $x=0$ by seeking
	\begin{equation}\label{eqa39}
		u(x)=f(0)+\sum_{j \in \mathcal{I}_{s}} c_{j} \psi_{j}(x).
	\end{equation}
	However, this adjustment introduces a new problem at $x=1$ since we now get an error $f(1)-u(1)=f(1)-0=-1$ at this point. A more clever adjustment is to replace the $f(0)$ term by a term that is $f(0)$ at $x=0$ and $f(1)$ at $x=1$. A simple linear combination $f(0)(1-x)+x f(1)$ does the job:
	\begin{equation}\label{eqa40}
		u(x)=f(0)(1-x)+x f(1)+\sum_{j \in \mathcal{I}_{s}} c_{j} \psi_{j}(x).
	\end{equation}
	\noindent This adjustment of $u$ alters the linear system slightly as we get an extra term $-\left(f(0)(1-x)+x f(1), \psi_{i}\right)$ on the right-hand side. Figure \hyperref[fig:img_6]{6} shows the result of this technique for ensuring right boundary values: even 3 sines can now adjust the $f(0)(1-x)+x f(1)$ term such that $u$ approximates the parabola really well, at least visually.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_6}
		\caption{Best approximation of a parabola by a sum of 3 (left) and 11 (right)
			sine functions with a boundary term.}
		\label{fig:img_6}
	\end{figure}
	\section[Orthogonal basis functions]{Orthogonal basis functions}
	\label{sec:sec_2_8}
	The choice of sine functions $\psi_{i}(x)=\sin ((i+1) \pi x)$ has a great computational advantage: on $\Omega=[0,1]$ these basis functions are \textit{orthogonal}, implying that $A_{i, j}=0$ if $i \neq j$. This result is realized by trying
	\begin{lstlisting}[numbers=none]
		integrate(sin(j*pi*x)*sin(k*pi*x), x, 0, 1)
	\end{lstlisting}
	in \href{https://www.wolframalpha.com/}{WolframAlpha} (avoid i in the integrand as this symbol means the imaginary unit $\sqrt{-1})$. Also by asking WolframAlpha about $\int_{0}^{1} \sin ^{2}(j \pi x) \mathrm{d} x$, we find it to equal $1 / 2$. With a diagonal matrix we can easily solve for the coefficients by hand:
	\begin{equation}\label{eqa41}
		c_{i}=2 \int_{0}^{1} f(x) \sin ((i+1) \pi x) \mathrm{d} x, \quad i \in \mathcal{I}_{s},
	\end{equation}
	which is nothing but the classical formula for the coefficients of the Fourier sine series of $f(x)$ on $[0,1]$. In fact, when $V$ contains the basic functions used in a Fourier series expansion, the approximation method derived in Section \hyperref[chap:chap_2]{2} results in the classical Fourier series for $f(x)$ (see Exercise \hyperref[sec:sec_10_8]{8} for details).
	
	With orthogonal basis functions we can make the least\textunderscore squares function (much) more efficient since we know that the matrix is diagonal and only the diagonal elements need to be computed:
	\begin{lstlisting}[numbers=none]
		def least_squares_orth(f, psi, Omega):
		N = len(psi) - 1
		A = [0]*(N+1)
		b = [0]*(N+1)
		x = sp.Symbol('x')
		for i in range(N+1):
		A[i] = sp.integrate(psi[i]**2, (x, Omega[0], Omega[1]))
		b[i] = sp.integrate(psi[i]*f, (x, Omega[0], Omega[1]))
		c = [b[i]/A[i] for i in range(len(b))]
		u = 0
		for i in range(len(psi)):
		u += c[i]*psi[i]
		return u, c
	\end{lstlisting}
	This function is found in the file approx1D.py.
	\section[Numerical computations]{Numerical computations}
	\label{sec:sec_2_9}
	\noindent Sometimes the basis functions $\psi_{i}$ and/or the function $f$ have a nature that makes symbolic integration CPU-time consuming or impossible. Even though we implemented a fallback on numerical integration of $\int f \varphi_{i} d x$ considerable time might be required by sympy in the attempt to integrate symbolically. Therefore, it will be handy to have function for fast \textit{numerical} integration and \textit{numerical} solution of the linear system. Below is such a method. It requires Python functions $\mathrm{f}(\mathrm{x})$ and psi $(\mathrm{x}, i)$ for $f(x)$ and $\psi_{i}(x)$ as input. The output is a mesh function with values $u$ on the mesh with points in the array $x$. Three numerical integration methods are offered: scipy. integrate. quad (precision set to $10^{-8}$ ), sympy -mpmath . quad (high precision), and a Trapezoidal rule based on the points in $x$.
	\begin{lstlisting}[numbers=none]
		def least_squares_numerical(f, psi, N, x,
		integration_method='scipy',
		orthogonal_basis=False):
		import scipy.integrate
		A = np.zeros((N+1, N+1))
		b = np.zeros(N+1)
		Omega = [x[0], x[-1]]
		dx = x[1] - x[0]
		
		for i in range(N+1):
		j_limit = i+1 if orthogonal_basis else N+1
		for j in range(i, j_limit):
		print '(%d,%d)' % (i, j)
		if integration_method == 'scipy':
		A_ij = scipy.integrate.quad(
		lambda x: psi(x,i)*psi(x,j),
		Omega[0], Omega[1], epsabs=1E-9, epsrel=1E-9)[0]
		elif integration_method == 'sympy':
		A_ij = sp.mpmath.quad(
		lambda x: psi(x,i)*psi(x,j),
		[Omega[0], Omega[1]])
		else:
		values = psi(x,i)*psi(x,j)
		A_ij = trapezoidal(values, dx)
		A[i,j] = A[j,i] = A_ij
		
		if integration_method == 'scipy':
		b_i = scipy.integrate.quad(			
		lambda x: f(x)*psi(x,i), Omega[0], Omega[1],
		epsabs=1E-9, epsrel=1E-9)[0]
		elif integration_method == 'sympy':
		b_i = sp.mpmath.quad(
		lambda x: f(x)*psi(x,i), [Omega[0], Omega[1]])
		else:
		values = f(x)*psi(x,i)
		b_i = trapezoidal(values, dx)
		b[i] = b_i
		
		c = b/np.diag(A) if orthogonal_basis else np.linalg.solve(A, b)
		u = sum(c[i]*psi(x, i) for i in range(N+1))
		return u, c
		
		def trapezoidal(values, dx):
		"""Integrate values by the Trapezoidal rule (mesh size dx)."""
		return dx*(np.sum(values) - 0.5*values[0] - 0.5*values[-1])
	\end{lstlisting}
	Here is an example on calling the function:
	\begin{lstlisting}[numbers=none]
		from numpy import linspace, tanh, pi
		def psi(x, i):
		return sin((i+1)*x)
		x = linspace(0, 2*pi, 501)
		N = 20
		u, c = least_squares_numerical(lambda x: tanh(x-pi), psi, N, x,
		orthogonal_basis=True)
	\end{lstlisting}
	\section[The interpolation (or collocation) method]{The interpolation (or collocation) method}
	\label{sec:sec_2_10}
	The principle of minimizing the distance between $u$ and $f$ is an intuitive way of computing a best approximation $u \in V$ to $f$. However, there are other approaches as well. One is to demand that $u\left(x_{i}\right)=f\left(x_{i}\right)$ at some selected points $x_{i}, i \in \mathcal{I}_{s}$ :
	\begin{equation}\label{eqa42}
		u\left(x_{i}\right)=\sum_{j \in \mathcal{I}_{s}} c_{j} \psi_{j}\left(x_{i}\right)=f\left(x_{i}\right), \quad i \in \mathcal{I}_{s}.
	\end{equation}
	This criterion also gives a linear system with $N+1$ unknown coefficients
	$\left\{c_{i}\right\}_{i \in \mathcal{I}_{s}}$ :
	\begin{equation}\label{eqa43}
		\sum_{j \in \mathcal{I}_{s}} A_{i, j} c_{j}=b_{i}, \quad i \in \mathcal{I}_{s},
	\end{equation}
	with
	\begin{equation}\label{eqa44}
		A_{i, j} = \psi_{j}\left(x_{i}\right),
	\end{equation}
	\begin{equation}\label{eqa45}
		b_{i} = f\left(x_{i}\right).
	\end{equation}
	This time the coefficient matrix is not symmetric because $\psi_{j}\left(x_{i}\right) \neq \psi_{i}\left(x_{j}\right)$ in general. The method is often referred to as an \textit{interpolation method} since some point values of $f$ are given $\left(f\left(x_{i}\right)\right)$ and we fit a continuous function $u$ that goes through the $f\left(x_{i}\right)$ points. In this case the $x_{i}$ points are called \textit{interpolation points}. When the same approach is used to approximate differential equations, one usually applies the name \textit{collocation method} and $x_{i}$ are known as \textit{collocation points}.
	
	Given $f$ as a sympy symbolic expression $\mathrm{f},\left\{\psi_{i}\right\}_{i \in \mathcal{I}_{s}}$ as a list psi, and a set of points $\left\{x_{i}\right\}_{i \in \mathcal{I}_{s}}$ as a list or array points, the following Python function sets up and solves the matrix system for the coefficients $\left\{c_{i}\right\}_{i \in \mathcal{I}_{s}}$ :
	\begin{lstlisting}[numbers=none]
		def interpolation(f, psi, points):
		N = len(psi) - 1
		A = sp.zeros((N+1, N+1))
		b = sp.zeros((N+1, 1))
		x = sp.Symbol('x')
		# Turn psi and f into Python functions
		psi = [sp.lambdify([x], psi[i]) for i in range(N+1)]
		f = sp.lambdify([x], f)
		for i in range(N+1):
		for j in range(N+1):
		A[i,j] = psi[j](points[i])
		b[i,0] = f(points[i])
		c = A.LUsolve(b)
		u = 0
		for i in range(len(psi)):
		u += c[i,0]*psi[i](x)
		return u	
	\end{lstlisting}	
	The interpolation function is a part of the approx1D module.
	
	We found it convenient in the above function to turn the expressions f
	and psi into ordinary Python functions of x, which can be called with float
	values in the list points when building the matrix and the right-hand side.
	The alternative is to use the subs method to substitute the x variable in an
	expression by an element from the points list. The following session illustrates
	both approaches in a simple setting:\\
	\begin{lstlisting}[numbers=none]
		>>> from sympy import *
		>>> x = Symbol('x')
		>>> e = x**2 # symbolic expression involving x
		>>> p = 0.5 # a value of x
		>>> v = e.subs(x, p) # evaluate e for x=p
		>>> v
		0.250000000000000
		>>> type(v)
		sympy.core.numbers.Float
		>>> e = lambdify([x], e) # make Python function of e
		>>> type(e)
		>>> function
		>>> v = e(p) # evaluate e(x) for x=p
		>>> v
		0.25
		>>> type(v)
		float	
	\end{lstlisting}
	A nice feature of the interpolation or collocation method is that it avoids computing integrals. However, one has to decide on the location of the $x_{i}$ points. A simple, yet common choice, is to distribute them uniformly throughout $\Omega$.
	\bigbreak
	\noindent \textbf{Example.} Let us illustrate the interpolation or collocation method by approximating our parabola $f(x)=10(x-1)^{2}-1$ by a linear function on $\Omega=[1,2]$, using two collocation points $x_{0}=1+1 / 3$ and $x_{1}=1+2 / 3$ :
	\begin{lstlisting}[numbers=none]
		f = 10*(x-1)**2 - 1
		psi = [1, x]
		Omega = [1, 2]
		points = [1 + sp.Rational(1,3), 1 + sp.Rational(2,3)]
		u = interpolation(f, psi, points)
		comparison_plot(f, u, Omega)		
	\end{lstlisting}
	The resulting linear system becomes
	$$
	\left(\begin{array}{ll}
		1 & 4 / 3 \\
		1 & 5 / 3
	\end{array}\right)\left(\begin{array}{l}
		c_{0} \\
		c_{1}
	\end{array}\right)=\left(\begin{array}{l}
		1 / 9 \\
		31 / 9
	\end{array}\right)
	$$
	with solution $c_{0}=-119 / 9$ and $c_{1}=10$. Figure \hyperref[fig:img_7]{7} (left) shows the resulting approximation $u=-119 / 9+10 x$. We can easily test other interpolation points, say $x_{0}=1$ and $x_{1}=2$. This changes the line quite significantly, see Figure \hyperref[fig:img_7]{7} (right).
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_7}
		\caption{Approximation of a parabola by linear functions computed by two
			interpolation points: 4/3 and 5/3 (left) versus 1 and 2 (right).}
		\label{fig:img_7}
	\end{figure}
	\section[Lagrange polynomials]{Lagrange polynomials}
	\label{sec:sec_2_11}
	\noindent In Section \hyperref[sec:sec_2_7]{2.7} we explain the advantage with having a diagonal matrix: formulas for the coefficients $\left\{c_{i}\right\}_{i \in \mathcal{I}_{s}}$ can then be derived by hand. For an interpolation/collocation method a diagonal matrix implies that $\psi_{j}\left(x_{i}\right)=0$ if $i \neq j$. One set of basis functions $\psi_{i}(x)$ with this property is the Lagrange interpolating polynomials, or just \textit{Lagrange polynomials}. (Although the functions are named after Lagrange, they were first discovered by Waring in 1779 , rediscovered by
	Euler in 1783 , and published by Lagrange in 1795.) The Lagrange polynomials have the form
	\begin{equation}\label{eqa46}
		\psi_{i}(x)=\prod_{j=0, j \neq i}^{N} \frac{x-x_{j}}{x_{i}-x_{j}}=\frac{x-x_{0}}{x_{i}-x_{0}} \cdots \frac{x-x_{i-1}}{x_{i}-x_{i-1}} \frac{x-x_{i+1}}{x_{i}-x_{i+1}} \cdots \frac{x-x_{N}}{x_{i}-x_{N}},
	\end{equation}
	for $i \in \mathcal{I}_{s}$. We see from (\hyperref[eqa46]{46}) that all the $\psi_{i}$ functions are polynomials of degree $N$ which have the property
	\begin{equation}\label{eqa47}
		\psi_{i}\left(x_{s}\right)=\delta_{i s}, \quad \delta_{i s}= \begin{cases}1, & i=s, \\ 0, & i \neq s,\end{cases}
	\end{equation}
	when $x_{s}$ is an interpolation/collocation point. Here we have used the \textit{Kronecker delta} symbol $\delta_{i s}$. This property implies that $A_{i, j}=0$ for $i \neq j$ and $A_{i, j}=1$ when $i=j$. The solution of the linear system is them simply
	\begin{equation}\label{eqa48}
		c_{i}=f\left(x_{i}\right), \quad i \in \mathcal{I}_{s},
	\end{equation}
	and
	\begin{equation}\label{eqa49}
		u(x)=\sum_{j \in \mathcal{I}_{s}} f\left(x_{i}\right) \psi_{i}(x).
	\end{equation}
	The following function computes the Lagrange interpolating polynomial $\psi_{i}(x)$, given the interpolation points $x_{0}, \ldots, x_{N}$ in the list or array points:
	\begin{lstlisting}[numbers=none]
		def Lagrange_polynomial(x, i, points):
		p = 1
		for k in range(len(points)):
		if k != i:
		p *= (x - points[k])/(points[i] - points[k])
		return p
	\end{lstlisting}
	The next function computes a complete basis using equidistant points throughout
	$\Omega$:
	\begin{lstlisting}[numbers=none]
		def Lagrange_polynomials_01(x, N):
		if isinstance(x, sp.Symbol):
		h = sp.Rational(1, N-1)
		else:
		h = 1.0/(N-1)
		points = [i*h for i in range(N)]
		psi = [Lagrange_polynomial(x, i, points) for i in range(N)]
		return psi, points	
	\end{lstlisting}
	When $x$ is an sp. Symbol object, we let the spacing between the interpolation points, $\mathrm{h}$, be a sympy rational number for nice end results in the formulas for $\psi_{i}$. The other case, when $\mathrm{x}$ is a plain Python $\mathrm{float}$, signifies numerical computing, and then we let $\mathrm{h}$ be a floating-point number. Observe that the Lagrange\textunderscore polynomial function works equally well in the symbolic and numerical case - just think of x being an sp.Symbol object or a Python float. A little
	interactive session illustrates the difference between symbolic and numerical
	computing of the basis functions and points:
	\begin{lstlisting}[numbers=none]
		>>> import sympy as sp
		>>> x = sp.Symbol('x')
		>>> psi, points = Lagrange_polynomials_01(x, N=3)
		>>> points
		[0, 1/2, 1]
		>>> psi
		[(1 - x)*(1 - 2*x), 2*x*(2 - 2*x), -x*(1 - 2*x)]
		>>> x = 0.5 # numerical computing
		>>> psi, points = Lagrange_polynomials_01(x, N=3)
		>>> points
		[0.0, 0.5, 1.0]
		>>> psi
		[-0.0, 1.0, 0.0]	
	\end{lstlisting}
	The Lagrange polynomials are very much used in finite element methods because
	of their property (\hyperref[eqa47]{47}).
	
	\noindent \textbf{Approximation of a polynomial.} The Galerkin or least squares method lead
	to an exact approximation if f lies in the space spanned by the basis functions. It
	could be interest to see how the interpolation method with Lagrange polynomials
	as basis is able to approximate a polynomial, e.g., a parabola. Running
	\begin{lstlisting}[numbers=none]
		for N in 2, 4, 5, 6, 8, 10, 12:
		f = x**2
		psi, points = Lagrange_polynomials_01(x, N)
		u = interpolation(f, psi, points)	
	\end{lstlisting}
	shows the result that up to $\mathrm{N}=4$ we achieve an exact approximation, and then round-off errors start to grow, such that $\mathrm{N}=15$ leads to a 15 -degree polynomial for $u$ where the coefficients in front of $x^{r}$ for $r>2$ are of size $10^{-5}$ and smaller.
	Successful example. Trying out the Lagrange polynomial basis for approximating $f(x)=\sin 2 \pi x$ on $\Omega=[0,1]$ with the least squares and the interpolation techniques can be done by
	\begin{lstlisting}[numbers=none]
		x = sp.Symbol('x')
		f = sp.sin(2*sp.pi*x)
		psi, points = Lagrange_polynomials_01(x, N)
		Omega=[0, 1]
		u = least_squares(f, psi, Omega)
		comparison_plot(f, u, Omega)
		u = interpolation(f, psi, points)
		comparison_plot(f, u, Omega)	
	\end{lstlisting}
	Figure \hyperref[fig:img_8]{8} shows the results. There is little difference between the least squares and
	the interpolation technique. Increasing \textit{N} gives visually better approximations.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_8}
		\caption{Approximation via least squares (left) and interpolation (right) of a
			sine function by Lagrange interpolating polynomials of degree 3.}
		\label{fig:img_8}
	\end{figure}	
	
	\noindent \textbf{Less successful example.} The next example concerns interpolating $f(x)=$ $|1-2 x|$ on $\Omega=[0,1]$ using Lagrange polynomials. Figure \hyperref[fig:img_9]{9} shows a peculiar effect: the approximation starts to oscillate more and more as $N$ grows. This numerical artifact is not surprising when looking at the individual Lagrange polynomials. Figure \hyperref[fig:img_10]{10} shows two such polynomials, $\psi_{2}(x)$ and $\psi_{7}(x)$, both of degree 11 and computed from uniformly spaced points $x_{x_{i}}=i / 11, i=0, \ldots, 11$, marked with circles. We clearly see the property of Lagrange polynomials: $\psi_{2}\left(x_{i}\right)=0$ and $\psi_{7}\left(x_{i}\right)=0$ for all $i$, except $\psi_{2}\left(x_{2}\right)=1$ and $\psi_{7}\left(x_{7}\right)=1$. The most striking feature, however, is the significant oscillation near the boundary. The reason is easy to understand: since we force the functions to zero at so many points, a polynomial of high degree is forced to oscillate between the points. The points, a polynomial of high degree is forced to oscillate between the points. The phenomenon is named Runge's phenomenon and you can read a more detailed explanation on \href{https://en.wikipedia.org/wiki/Runge%27s_phenomenon}{Wikipedia}.
	\bigbreak
	\noindent \textbf{Remedy for strong oscillations.} The oscillations can be reduced by a more clever choice of interpolation points, called the \textit{Chebyshev nodes}:
	\begin{equation}\label{eqa50}
		x_{i}=\frac{1}{2}(a+b)+\frac{1}{2}(b-a) \cos \left(\frac{2 i+1}{2(N+1)} p i\right), \quad i=0 \ldots, N,
	\end{equation}
	on the interval $\Omega=[a, b]$. Here is a flexible version of the Lagrange\textunderscore polynomials\textunderscore 01 function above, valid for any interval $\Omega=[a, b]$ and with the possibility to generate both uniformly distributed points and Chebyshev nodes:
	\begin{lstlisting}[numbers=none]
		def Lagrange_polynomials(x, N, Omega, point_distribution='uniform'):
		if point_distribution == 'uniform':
		if isinstance(x, sp.Symbol):
		h = sp.Rational(Omega[1] - Omega[0], N)
		else:
		h = (Omega[1] - Omega[0])/float(N)
		points = [Omega[0] + i*h for i in range(N+1)]
		elif point_distribution == 'Chebyshev':
		points = Chebyshev_nodes(Omega[0], Omega[1], N)
		psi = [Lagrange_polynomial(x, i, points) for i in range(N+1)]
		return psi, points
		def Chebyshev_nodes(a, b, N):
		from math import cos, pi
		return [0.5*(a+b) + 0.5*(b-a)*cos(float(2*i+1)/(2*N+1))*pi) \
		for i in range(N+1)]
	\end{lstlisting}
	All the functions computing Lagrange polynomials listed above are found in the module file Lagrange.py. Figure 11 shows the improvement of using Chebyshev nodes (compared with Figure \hyperref[fig:img_9]{9}). The reason is that the corresponding Lagrange polynomials have much smaller oscillations as seen in Figure \hyperref[fig:img_12]{12} (compare with Figure \hyperref[fig:img_10]{10}).
	
	Another cure for undesired oscillation of higher-degree interpolating polynomials is to use lower-degree Lagrange polynomials on many small patches of the domain, which is the idea pursued in the finite element method. For instance, linear Lagrange polynomials on $[0,1 / 2]$ and $[1 / 2,1]$ would yield a perfect approximation to $f(x)=|1-2 x|$ on $\Omega=[0,1]$ since $f$ is piecewise linear.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_9}
		\caption{Interpolation of an absolute value function by Lagrange polynomials and uniformly distributed interpolation points: degree 7 (left) and 14 (right).}
		\label{fig:img_9}
	\end{figure}
	How does the least squares or projection methods work with Lagrange polynomials? Unfortunately, sympy has problems integrating the $f(x)=|1-2 x|$ function times a polynomial. Other choices of $f(x)$ can also make the symbolic integration fail. Therefore, we should extend the least\textunderscore squares function such that it falls back on numerical integration if the symbolic integration is unsuccessful. In the latter case, the returned value from sympy's integrate function is an object of type Integral. We can test on this type and utilize the mpmath module in sympy to perform numerical integration of high precision. Here is the code:
	\begin{lstlisting}[numbers=none]
		def least_squares(f, psi, Omega):
		N = len(psi) - 1
		A = sp.zeros((N+1, N+1))
		b = sp.zeros((N+1, 1))
		x = sp.Symbol('x')
		for i in range(N+1):
		for j in range(i, N+1):
		integrand = psi[i]*psi[j]
		I = sp.integrate(integrand, (x, Omega[0], Omega[1]))	
	\end{lstlisting}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_10}
		\caption{Illustration of the oscillatory behavior of two Lagrange polynomials
			based on 12 uniformly spaced points (marked by circles).}
		\label{fig:img_10}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_11}
		\caption{ Interpolation of an absolute value function by Lagrange polynomials
			and Chebyshev nodes as interpolation points: degree 7 (left) and 14 (right).}
		\label{fig:img_11}
	\end{figure}
	\begin{lstlisting}[numbers=none]
		if isinstance(I, sp.Integral):
		# Could not integrate symbolically, fallback
		# on numerical integration with mpmath.quad
		integrand = sp.lambdify([x], integrand)
		I = sp.mpmath.quad(integrand, [Omega[0], Omega[1]])
		A[i,j] = A[j,i] = I
		integrand = psi[i]*f
		I = sp.integrate(integrand, (x, Omega[0], Omega[1]))
		if isinstance(I, sp.Integral):
		integrand = sp.lambdify([x], integrand)
	\end{lstlisting}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_12}
		\caption{Illustration of the less oscillatory behavior of two Lagrange polynomials based on 12 Chebyshev points (marked by circles).}
		\label{fig:img_12}
	\end{figure}
	\begin{lstlisting}[numbers=none]
		I = sp.mpmath.quad(integrand, [Omega[0], Omega[1]])
		b[i,0] = I
		c = A.LUsolve(b)
		u = 0
		for i in range(len(psi)):
		u += c[i,0]*psi[i]
		return u
	\end{lstlisting}
	
	\chapter{Finite element basis functions}
	\label{chap:chap_3}
	\noindent The specific basis functions exemplified in Section \hyperref[chap:chap_2]{2} are in general nonzero on the entire domain $\Omega$, see Figure \hyperref[fig:img_13]{13} for an example where we plot $\psi_{0}(x)=\sin \frac{1}{2} \pi x$ and $\psi_{1}(x)=\sin 2 \pi x$ together with a possible sum $u(x)=4 \psi_{0}(x)-\frac{1}{2} \psi_{1}(x)$. We shall now turn the attention to basis functions that have compact support, meaning that they are nonzero on only a small portion of $\Omega$. Moreover, we shall restrict the functions to be \textit{piecewise polynomials}. This means that the domain is split into subdomains and the function is a polynomial on one or more subdomains, see Figure \hyperref[fig:img_14]{14} for a sketch involving locally defined hat functions that make $u=\sum_{j} c_{j} \psi_{j}$ piecewise linear. At the boundaries between subdomains one normally forces continuity of the function only so that when connecting two polynomials from two subdomains, the derivative becomes discontinuous. These type of basis functions are fundamental in the finite element method.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_13}
		\caption{A function resulting from adding two sine basis functions.}
		\label{fig:img_13}
	\end{figure}
	We first introduce the concepts of elements and nodes in a simplistic fashion
	as often met in the literature. Later, we shall generalize the concept of an
	element, which is a necessary step to treat a wider class of approximations within
	the family of finite element methods. The generalization is also compatible with
	the concepts used in the \href{https://fenicsproject.org/}{FEniCS} finite element software
	\section[Elements and nodes]{Elements and nodes}
	\label{sec:sec_3_1}
	Let us divide the interval $\Omega$ on which $f$ and $u$ are defined into non-overlapping subintervals $\Omega^{(e)}, e=0, \ldots, N_{e}$ :
	\begin{equation}\label{eqa51}
		\Omega=\Omega^{(0)} \cup \cdots \cup \Omega^{\left(N_{c}\right)}.
	\end{equation}
	We shall for now refer to $\Omega^{(e)}$ as an element, having number $e$. On each element we introduce a set of points called nodes. For now we assume that the nodes are uniformly spaced throughout the element and that the boundary points of the elements are also nodes. The nodes are given numbers both within an element and in the global domain. These are referred to as \textit{local} and \textit{global} node numbers, respectively. Figure \hyperref[fig:img_15]{15} shows element boundaries with small vertical lines, nodes as small disks, element numbers in circles, and global node numbers under the nodes.
	
	Nodes and elements uniquely define a \textit{finite element mesh}, which is our discrete representation of the domain in the computations. A common special
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_14}
		\caption{A function resulting from adding three local piecewise linear (hat)
			functions.}
		\label{fig:img_14}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_15}
		\caption{Finite element mesh with 5 elements and 6 nodes.}
		\label{fig:img_15}
	\end{figure}
	
	\noindent case is that of a \textit{uniformly partitioned mesh} where each element has the same
	length and the distance between nodes is constant.
	
	\noindent \textbf{Example.} $\quad$ On $\Omega=[0,1]$ we may introduce two elements, $\Omega^{(0)}=[0,0.4]$ and $\Omega^{(1)}=[0.4,1]$. Furthermore, let us introduce three nodes per element, equally spaced within each element. Figure \hyperref[fig:img_16]{16} shows the mesh. The three nodes in element number 0 are $x_{0}=0, x_{1}=0.2$, and $x_{2}=0.4$. The local and global node numbers are here equal. In element number 1 , we have the local nodes $x_{0}=0.4$, $x_{1}=0.7$, and $x_{2}=1$ and the corresponding global nodes $x_{2}=0.4, x_{3}=0.7$, and $x_{4}=1$. Note that the global node $x_{2}=0.4$ is shared by the two elements.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_16}
		\caption{Finite element mesh with 2 elements and 5 nodes.}
		\label{fig:img_16}
	\end{figure}
	
	For the purpose of implementation, we introduce two lists or arrays: nodes
	for storing the coordinates of the nodes, with the global node numbers as indices,
	and elements for holding the global node numbers in each element, with the
	local node numbers as indices. The nodes and elements lists for the sample
	mesh above take the form
	\begin{lstlisting}[numbers=none]
		nodes = [0, 0.2, 0.4, 0.7, 1]
		elements = [[0, 1, 2], [2, 3, 4]]	
	\end{lstlisting}
	Looking up the coordinate of local node number 2 in element 1 is here done by
	nodes[elements[1][2]] (recall that nodes and elements start their numbering
	at 0).
	
	The numbering of elements and nodes does not need to be regular. Figure \hyperref[fig:img_17]{17}
	shows and example corresponding to
	\begin{lstlisting}[numbers=none]
		nodes = [1.5, 5.5, 4.2, 0.3, 2.2, 3.1]
		elements = [[2, 1], [4, 5], [0, 4], [3, 0], [5, 2]]	
	\end{lstlisting}
	\section[The basis functions]{The basis functions}
	\label{sec:sec_3_2}
	\noindent \textbf{Construction principles}. Finite element basis functions are in this text recognized by the notation $\varphi_{i}(x)$, where the index now in the beginning corresponds to a global node number. In the current approximation problem we shall simply take $\psi_{i}=\varphi_{i}$.
	
	Let $i$ be the global node number corresponding to local node $r$ in element number $e$. The finite element basis functions $\varphi_{i}$ are now defined as follows.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_17}
		\caption{Example on irregular numbering of elements and nodes.}
		\label{fig:img_17}
	\end{figure}
	
	\begin{itemize}
		\item If local node number $r$ is not on the boundary of the element, take $\varphi_{i}(x)$ to be the Lagrange polynomial that is 1 at the local node number $r$ and zero at all other nodes in the element. On all other elements, $\varphi_{i}=0$.
		\item If local node number $r$ is on the boundary of the element, let $\varphi_{i}$ be made up of the Lagrange polynomial over element $e$ that is 1 at node $i$, combined with the Lagrange polynomial over element $e+1$ that is also 1 at node $i$. On all other elements, $\varphi_{i}=0$.
	\end{itemize}
	A visual impression of three such basis functions are given in Figure $18 .$
	\bigbreak
	\noindent \textbf{Properties of} $\varphi_{i}$. The construction of basis functions according to the principles above lead to two important properties of $\varphi_{i}(x)$. First,
	\begin{equation}\label{eqa52}
		\varphi_{i}\left(x_{j}\right)=\delta_{i j}, \quad \delta_{i j}= \begin{cases}1, & i=j, \\ 0, & i \neq j,\end{cases}
	\end{equation}
	when $x_{j}$ is a node in the mesh with global node number $j$. The result $\varphi_{i}\left(x_{j}\right)=\delta_{i j}$ arises because the Lagrange polynomials are constructed to have exactly this property. The property also implies a convenient interpretation of $c_{i}$ as the value of $u$ at node $i$. To show this, we expand $u$ in the usual way as $\sum_{j} c_{j} \psi_{j}$ and choose $\psi_{i}=\varphi_{i}$ :
	$$
	u\left(x_{i}\right)=\sum_{j \in \mathcal{I}_{n}} c_{j} \psi_{j}\left(x_{i}\right)=\sum_{j \in \mathcal{I}_{n}} c_{j} \varphi_{j}\left(x_{i}\right)=c_{i} \varphi_{i}\left(x_{i}\right)=c_{i}.
	$$
	Because of this interpretation, the coefficient $c_{i}$ is by many named $u_{i}$ or $U_{i}$.
	
	Second, $\varphi_{i}(x)$ is mostly zero throughout the domain:
	\begin{itemize}
		\item $\varphi_{i}(x) \neq 0$ only on those elements that contain global node $i$,
		\item $\varphi_{i}(x) \varphi_{j}(x) \neq 0$ if and only if $i$ and $j$ are global node numbers in the same element.
	\end{itemize}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_18}
		\caption{Illustration of the piecewise quadratic basis functions associated
			with nodes in element 1.}
		\label{fig:img_18}
	\end{figure}
	
	\noindent Since $A_{i, j}$ is the integral of $\varphi_{i} \varphi_{j}$ it means that \textit{most of the elements in the coefficient matrix will be zero}. We will come back to these properties and use them actively in computations to save memory and CPU time.
	
	We let each element have $d+1$ nodes, resulting in local Lagrange polynomials of degree $d$. It is not a requirement to have the same $d$ value in each element, but for now we will assume so.
	\section[Example on piecewise quadratic finite element functions]{Example on piecewise quadratic finite element functions}
	\label{sec:sec_3_3}
	\noindent Figure 18 illustrates how piecewise quadratic basis functions can look like $(d=2)$. We work with the domain $\Omega=[0,1]$ divided into four equal-sized elements, each having three nodes. The nodes and elements lists in this particular example become
	\begin{lstlisting}[numbers=none]
		nodes = [0, 0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1.0]
		elements = [[0, 1, 2], [2, 3, 4], [4, 5, 6], [6, 7, 8]]	
	\end{lstlisting}
	Figure \hyperref[fig:img_19]{19} sketches the mesh and the numbering. Nodes are marked with circles on the $x$ axis and element boundaries are marked with vertical dashed lines in Figure \hyperref[fig:img_18]{18}.
	
	Let us explain in detail how the basis functions are constructed according to the principles. Consider element number 1 in Figure \hyperref[fig:img_18]{18} $, \Omega^{(1)}=[0.25,0.5]$,
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_19}
		\caption{Sketch of mesh with 4 elements and 3 nodes per element.}
		\label{fig:img_19}
	\end{figure}
	\noindent with local nodes 0,1 , and 2 corresponding to global nodes 2 , 3 , and 4 . The coordinates of these nodes are $0.25,0.375$, and $0.5$, respectively. We define three Lagrange polynomials on this element:
	\begin{enumerate}
		\item The polynomial that is 1 at local node $1(x=0.375$, global node 3 ) makes up the basis function $\varphi_{3}(x)$ over this element, with $\varphi_{3}(x)=0$ outside the element.
		\item The Lagrange polynomial that is 1 at local node 0 is the "right part" of the global basis function $\varphi_{2}(x)$. The "left part" of $\varphi_{2}(x)$ consists of a Lagrange polynomial associated with local node 2 in the neighboring element $\Omega^{(0)}=[0,0.25]$.
		\item Finally, the polynomial that is 1 at local node 2 (global node 4 ) is the "left part" of the global basis function $\varphi_{4}(x)$. The "right part" comes from the Lagrange polynomial that is 1 at local node 0 in the neighboring element $\Omega^{(2)}=[0.5,0.75]$.
	\end{enumerate}	
	
	\noindent As mentioned earlier, any global basis function $\varphi_{i}(x)$ is zero on elements that do not contain the node with global node number $i$.
	
	The other global functions associated with internal nodes, $\varphi_{1}, \varphi_{5}$, and $\varphi_{7}$, are all of the same shape as the drawn $\varphi_{3}$, while the global basis functions associated with shared nodes also have the same shape, provided the elements are of the same length.
	
	\section[Example on piecewise linear finite element functions]{Example on piecewise linear finite element functions}
	\label{sec:sec_3_4}
	Figure \hyperref[fig:img_20]{20} shows piecewise linear basis functions $(d=1)$. Also here we have four elements on $\Omega=[0,1]$. Consider the element $\Omega^{(1)}=[0.25,0.5]$. Now there are no internal nodes in the elements so that all basis functions are associated with nodes at the element boundaries and hence made up of two Lagrange
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_20}
		\caption{Illustration of the piecewise linear basis functions associated with
			nodes in element 1.}
		\label{fig:img_20}
	\end{figure}
	
	\noindent polynomials from neighboring elements. For example, $\varphi_{1}(x)$ results from the Lagrange polynomial in element 0 that is 1 at local node 1 and 0 at local node 0 , combined with the Lagrange polynomial in element 1 that is 1 at local node 0 and 0 at local node 1 . The other basis functions are constructed similarly.
	
	Explicit mathematical formulas are needed for $\varphi_{i}(x)$ in computations. In the piecewise linear case, one can show that
	\begin{equation}\label{eqa53}
		\varphi_{i}(x)= \begin{cases}0, & x<x_{i-1}, \\ \left(x-x_{i-1}\right) /\left(x_{i}-x_{i-1}\right), & x_{i-1} \leq x<x_{i}, \\ 1-\left(x-x_{i}\right) /\left(x_{i+1}-x_{i}\right), & x_{i} \leq x<x_{i+1}, \\ 0, & x \geq x_{i+1}.\end{cases}
	\end{equation}
	Here, $x_{j}, j=i-1, i, i+1$, denotes the coordinate of node $j$. For elements of equal length $h$ the formulas can be simplified to
	\begin{equation}\label{eqa54}
		\varphi_{i}(x)= \begin{cases}0, & x<x_{i-1}, \\ \left(x-x_{i-1}\right) / h, & x_{i-1} \leq x<x_{i}, \\ 1-\left(x-x_{i}\right) / h, & x_{i} \leq x<x_{i+1}, \\ 0, & x \geq x_{i+1}\end{cases}
	\end{equation}
	\section[Example on piecewise cubic finite element basis functions]{Example on piecewise cubic finite element basis functions}
	\label{sec:sec_3_5}
	Piecewise cubic basis functions can be defined by introducing four nodes per element. Figure \hyperref[fig:img_21]{21} shows examples on $\varphi_{Wi}(x)$, $i=3,4,5,6$, associated with element number 1. Note that $\varphi_{4}$ and $\varphi_{5}$ are nonzero on element number 1 , while $\varphi_{3}$ and $\varphi_{6}$ are made up of Lagrange polynomials on two neighboring elements.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_21}
		\caption{Illustration of the piecewise cubic basis functions associated with
			nodes in element 1.}
		\label{fig:img_21}
	\end{figure}
	
	We see that all the piecewise linear basis functions have the same "hat" shape.
	They are naturally referred to as \textit{hat functions}, also called \textit{chapeau functions.}
	The piecewise quadratic functions in Figure \hyperref[fig:img_18]{18} are seen to be of two types.
	"Rounded hats" associated with internal nodes in the elements and some more
	"sombrero" shaped hats associated with element boundary nodes. Higher-order
	basis functions also have hat-like shapes, but the functions have pronounced
	oscillations in addition, as illustrated in Figure \hyperref[fig:img_21]{21}.
	
	A common terminology is to speak about \textit{linear elements} as elements with two
	local nodes associated with piecewise linear basis functions. Similarly, \textit{quadratic
		elements} and \textit{cubic elements} refer to piecewise quadratic or cubic functions
	over elements with three or four local nodes, respectively. Alternative names,
	frequently used later, are P1 elements for linear elements, P2 for quadratic
	elements, and so forth: Pd signifies degree d of the polynomial basis functions.
	
	\section[Calculating the linear system]{Calculating the linear system}
	\label{sec:sec_3_6}
	The elements in the coefficient matrix and right-hand side are given by the formulas (\hyperref[eqa27]{27}) and (\hyperref[eqa28]{28}), but now the choice of $\psi_{i}$ is $\varphi_{i}$. Consider P1 elements where $\varphi_{i}(x)$ piecewise linear. Nodes and elements numbered consecutively from left to right in a uniformly partitioned mesh imply the nodes
	$$
	x_{i}=i h, \quad i=0, \ldots, N,
	$$
	and the elements
	\begin{equation}\label{eqa55}
		\Omega^{(i)}=\left[x_{i}, x_{i+1}\right]=[i h,(i+1) h], \quad i=0, \ldots, N_{e}=N-1.
	\end{equation}
	We have in this case $N$ elements and $N+1$ nodes, and $\Omega=\left[x_{0}, x_{N}\right]$. The formula for $\varphi_{i}(x)$ is given by (\hyperref[eqa54]{54}) and a graphical illustration is provided in Figures \hyperref[fig:img_20]{20} and \hyperref[fig:img_23]{23} . First we clearly see from the figures the very important property $\varphi_{i}(x) \varphi_{j}(x) \neq 0$ if and only if $j=i-1, j=i$, or $j=i+1$, or alternatively expressed, if and only if $i$ and $j$ are nodes in the same element. Otherwise, $\varphi_{i}$ and $\varphi_{j}$ are too distant to have an overlap and consequently their product vanishes.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_22}
		\caption{Illustration of the piecewise linear basis functions corresponding to
			global node 2 and 3.}
		\label{fig:img_22}
	\end{figure}
	
	\noindent \textbf{Calculating a specific matrix entry.} Let us calculate the specific matrix entry $A_{2,3}=\int_{\Omega} \varphi_{2} \varphi_{3} \mathrm{~d} x$. Figure \hyperref[fig:img_22]{22} shows how $\varphi_{2}$ and $\varphi_{3}$ look like. We realize from this figure that the product $\varphi_{2} \varphi_{3} \neq 0$ only over element 2 , which contains node 2 and 3. The particular formulas for $\varphi_{2}(x)$ and $\varphi_{3}(x)$ on $\left[x_{2}, x_{3}\right]$ are found from (\hyperref[eqa54]{54}). The function $\varphi_{3}$ has positive slope over $\left[x_{2}, x_{3}\right]$ and corresponds to the interval $\left[x_{i-1}, x_{i}\right]$ in (\hyperref[eqa54]{54}). With $i=3$ we get
	$$
	\varphi_{3}(x)=\left(x-x_{2}\right) / h,
	$$
	while $\varphi_{2}(x)$ has negative slope over $\left[x_{2}, x_{3}\right]$ and corresponds to setting $i=2$ in (\hyperref[eqa54]{54}).
	
	$$
	\varphi_{2}(x)=1-\left(x-x_{2}\right) / h .
	$$
	We can now easily integrate,
	$$
	A_{2,3}=\int_{\Omega} \varphi_{2} \varphi_{3} \mathrm{~d} x=\int_{x_{2}}^{x_{3}}\left(1-\frac{x-x_{2}}{h}\right) \frac{x-x_{2}}{h} \mathrm{~d} x=\frac{h}{6}.
	$$
	The diagonal entry in the coefficient matrix becomes
	$$
	A_{2,2}=\int_{x_{1}}^{x_{2}}\left(\frac{x-x_{1}}{h}\right)^{2} \mathrm{~d} x+\int_{x_{2}}^{x_{3}}\left(1-\frac{x-x_{2}}{h}\right)^{2} \mathrm{~d} x=\frac{h}{3}.
	$$
	The entry $A_{2,1}$ has an the integral that is geometrically similar to the situation in Figure \hyperref[fig:img_22]{22}, so we get $A_{2,1}=h / 6$.
	\bigbreak
	\noindent \textbf{Calculating a general row in the matrix.} We can now generalize the calculation of matrix entries to a general row number $i$. The entry $A_{i, i-1}=$ $\int_{\Omega} \varphi_{i} \varphi_{i-1} \mathrm{~d} x$ involves hat functions as depicted in Figure \hyperref[fig:img_23]{23} . Since the integral is geometrically identical to the situation with specific nodes 2 and 3 , we realize that $A_{i, i-1}=A_{i, i+1}=h / 6$ and $A_{i, i}=2 h / 3$. However, we can compute the integral directly too:
	$$
	\begin{aligned}
		A_{i, i-1} &=\int_{\Omega} \varphi_{i} \varphi_{i-1} \mathrm{~d} x \\
		&=\underbrace{\int_{x_{i-2}}^{x_{i-1}} \varphi_{i} \varphi_{i-1} \mathrm{~d} x}_{\varphi_{i}=0}+\int_{x_{i-1}}^{x_{i}} \varphi_{i} \varphi_{i-1} \mathrm{~d} x+\underbrace{\int_{x_{i}}^{x_{i+1}} \varphi_{i} \varphi_{i-1} \mathrm{~d} x}_{\varphi_{i-1}=0} \\
		&=\int_{x_{i-1}}^{x_{i}} \underbrace{\left(\frac{x-x_{i}}{h}\right)}_{\varphi_{i}(x)} \underbrace{\left(1-\frac{x-x_{i-1}}{h}\right)}_{\varphi_{i-1}(x)} \mathrm{d} x=\frac{h}{6}.
	\end{aligned}
	$$
	The particular formulas for $\varphi_{i-1}(x)$ and $\varphi_{i}(x)$ on $\left[x_{i-1}, x_{i}\right]$ are found from (\hyperref[eqa54]{54}): $\varphi_{i}$ is the linear function with positive slope, corresponding to the interval $\left[x_{i-1}, x_{i}\right]$ in (\hyperref[eqa54]{54}), while $\phi_{i-1}$ has a negative slope so the definition in interval $\left[x_{i}, x_{i+1}\right]$ in (\hyperref[eqa54]{54}) must be used. (The appearance of $i$ in (\hyperref[eqa54]{54}) and the integral might be confusing, as we speak about two different $i$ indices.)
	
	The first and last row of the coefficient matrix lead to slightly different integrals:
	$$
	A_{0,0}=\int_{\Omega} \varphi_{0}^{2} \mathrm{~d} x=\int_{x_{0}}^{x_{1}}\left(1-\frac{x-x_{0}}{h}\right)^{2} \mathrm{~d} x=\frac{h}{3}.
	$$
	Similarly, $A_{N, N}$ involves an integral over only one element and equals hence $h / 3$. The general formula for $b_{i}$, see Figure \hyperref[fig:img_24]{24}, is now easy to set up
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_23}
		\caption{Illustration of two neighboring linear (hat) functions with general
			node numbers.}
		\label{fig:img_23}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_24}
		\caption{Right-hand side integral with the product of a basis function and
			the given function to approximate.}
		\label{fig:img_24}
	\end{figure}
	\begin{equation}\label{eqa56}
		b_{i}=\int_{\Omega} \varphi_{i}(x) f(x) \mathrm{d} x=\int_{x_{i-1}}^{x_{i}} \frac{x-x_{i-1}}{h} f(x) \mathrm{d} x+\int_{x_{i}}^{x_{i+1}}\left(1-\frac{x-x_{i}}{h}\right) f(x) \mathrm{d} x.
	\end{equation}
	We need a specific $f(x)$ function to compute these integrals. With two equal-sized elements in $\Omega=[0,1]$ and $f(x)=x(1-x)$, one gets
	$$
	A=\frac{h}{6}\left(\begin{array}{ccc}
		2 & 1 & 0 \\
		1 & 4 & 1 \\
		0 & 1 & 2
	\end{array}\right), \quad b=\frac{h^{2}}{12}\left(\begin{array}{c}
		2-3 h \\
		12-14 h \\
		10-17 h
	\end{array}\right)
	$$
	The solution becomes
	$$
	c_{0}=\frac{h^{2}}{6}, \quad c_{1}=h-\frac{5}{6} h^{2}, \quad c_{2}=2 h-\frac{23}{6} h^{2}.
	$$
	The resulting function
	$$
	u(x)=c_{0} \varphi_{0}(x)+c_{1} \varphi_{1}(x)+c_{2} \varphi_{2}(x)
	$$
	is displayed in Figure \hyperref[fig:img_25]{25} (left). Doubling the number of elements to four leads to the improved approximation in the right part of Figure \hyperref[fig:img_25]{25} .
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_25}
		\caption{Least squares approximation of a parabola using 2 (left) and 4
			(right) P1 elements.}
		\label{fig:img_25}
	\end{figure}
	
	\section[Assembly of elementwise computations]{Assembly of elementwise computations}
	\label{sec:sec_3_7}
	The integrals above are naturally split into integrals over individual elements since the formulas change with the elements. This idea of splitting the integral is fundamental in all practical implementations of the finite element method.
	Let us split the integral over $\Omega$ into a sum of contributions from each element:
	\begin{equation}\label{eqa57}
		A_{i, j}=\int_{\Omega} \rho_{i} \rho_{j} \mathrm{~d} x=\sum_{e} A_{i, j}^{(e)}, \quad A_{i, j}^{(e)}=\int_{\Omega^{(e)}} \rho_{i} \rho_{j} \mathrm{~d} x.
	\end{equation}
	Now, $A_{i, j}^{(e)} \neq 0$ if and only if $i$ and $j$ are nodes in element $e$. Introduce $i=q(e, r)$ as the mapping of local node number $r$ in element $e$ to the global node number $i$. This is just a short mathematical notation for the expression $i=e l$ ements [e] [r] in a program. Let $r$ and $s$ be the local node numbers corresponding to the global node numbers $i=q(e, r)$ and $j=q(e, s)$. With $d$ nodes per element, all the nonzero elements in $A_{i, j}^{(e)}$ arise from the integrals involving basis functions with indices corresponding to the global node numbers in element number $e$ :
	$$
	\int_{\Omega^{(e)}} \varphi_{q(e, r)} \varphi_{q(e, s)} \mathrm{d} x, \quad r, s=0, \ldots, d .
	$$
	These contributions can be collected in a $(d+1) \times(d+1)$ matrix known as the \textit{element matrix}. Let $I_{d}=\{0, \ldots, d\}$ be the valid indices of $r$ and $s$. We introduce the notation
	$$
	\tilde{A}^{(e)}=\left\{\tilde{A}_{r, s}^{(e)}\right\}, \quad r, s \in I_{d},
	$$
	for the element matrix. For the case $d=2$ we have
	$$
	\tilde{A}^{(e)}=\left[\begin{array}{ccc}
		\tilde{A}_{0,0}^{(e)} & \tilde{A}_{0,1}^{(e)} & \tilde{A}_{0,2}^{(e)} \\
		\tilde{A}_{1,0}^{(e)} & \tilde{A}_{1,1}^{(e)} & \tilde{A}_{1,2}^{(e)} \\
		\tilde{A}_{2,0}^{(e)} & \tilde{A}_{2,1}^{(e)} & \tilde{A}_{2,2}^{(e)}
	\end{array}\right] .
	$$
	Given the numbers $\tilde{A}_{r, s}^{(e)}$, we should according to (\hyperref[eqa57]{57}) add the contributions to the global coefficient matrix by
	\begin{equation}\label{eqa58}
		A_{q(e, r), q(e, s)}:=A_{q(e, r), q(e, s)}+\tilde{A}_{r, s}^{(e)}, \quad r, s \in I_{d} .
	\end{equation}
	
	This process of adding in elementwise contributions to the global matrix is called \textit{finite element assembly} or simply \textit{assembly}. Figure \hyperref[fig:img_26]{26} illustrates how element matrices for elements with two nodes are added into the global matrix. More specifically, the figure shows how the element matrix associated with elements 1 and 2 assembled, assuming that global nodes are numbered from left to right in the domain. With regularly numbered P3 elements, where the element matrices have size $4 \times 4$, the assembly of elements 1 and 2 are sketched in Figure \hyperref[fig:img_27]{27}.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_26}
		\caption{Illustration of matrix assembly: regularly numbered P1 elements.}
		\label{fig:img_26}
	\end{figure}
	
	After assembly of element matrices corresponding to regularly numbered
	elements and nodes are understood, it is wise to study the assembly process for
	irregularly numbered elements and nodes. Figure \hyperref[fig:img_17]{17} shows a mesh where the
	elements array, or q(e, r) mapping in mathematical notation, is given as
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_27}
		\caption{Illustration of matrix assembly: regularly numbered P3 elements.}
		\label{fig:img_27}
	\end{figure}
	
	\begin{lstlisting}[numbers=none]
		elements = [[2, 1], [4, 5], [0, 4], [3, 0], [5, 2]
	\end{lstlisting}
	The associated assembly of element matrices 1 and 2 is sketched in Figure \hyperref[fig:img_28]{28}. 
	
	These three assembly processes can also be \href{http://hplgit.github.io/INF5620/doc/pub/mov-fem/fe_assembly.html}{animated.}
	
	
	The right-hand side of the linear system is also computed elementwise:
	\begin{equation}\label{eqa59}
		b_{i}=\int_{\Omega} f(x) \varphi_{i}(x) \mathrm{d} x=\sum_{e} b_{i}^{(e)}, \quad b_{i}^{(e)}=\int_{\Omega^{(e)}} f(x) \varphi_{i}(x) \mathrm{d} x.
	\end{equation}
	We observe that $b_{i}^{(e)} \neq 0$ if and only if global node $i$ is a node in element $e$. With $d$ nodes per element we can collect the $d+1$ nonzero contributions $b_{i}^{(e)}$, for $i=q(e, r), r \in I_{d}$, in an \textit{element vector}
	$$
	\tilde{b}_{r}^{(e)}=\left\{\tilde{b}_{r}^{(e)}\right\}, \quad r \in I_{d} .
	$$
	These contributions are added to the global right-hand side by an assembly process similar to that for the element matrices:
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_28}
		\caption{Illustration of matrix assembly: irregularly numbered P1 elements.}
		\label{fig:img_28}
	\end{figure}
	\begin{equation}\label{eqa60}
		b_{q(e, r)}:=b_{q(e, r)}+\tilde{b}_{r}^{(e)}, \quad r \in I_{d}.
	\end{equation}
	\section[Mapping to a reference element]{Mapping to a reference element}
	\label{sec:sec_3_8}
	Instead of computing the integrals
	$$
	\tilde{A}_{r, s}^{(e)}=\int_{\Omega^{(e)}} \varphi_{q(e, r)}(x) \varphi_{q(e, s)}(x) \mathrm{d} x
	$$
	over some element $\Omega^{(e)}=\left[x_{L}, x_{R}\right]$, it is convenient to map the element domain $\left[x_{L}, x_{R}\right]$ to a standardized reference element domain $[-1,1]$. (We have now introduced $x_{L}$ and $x_{R}$ as the left and right boundary points of an arbitrary element. With a natural, regular numbering of nodes and elements from left to right through the domain, we have $x_{L}=x_{e}$ and $x_{R}=x_{e+1}$ for $\mathrm{P} 1$ elements.)
	
	Let $X \in[-1,1]$ be the coordinate in the reference element. A linear or \textit{affine mapping} from $X$ to $x$ reads
	\begin{equation}\label{eqa61}
		x=\frac{1}{2}\left(x_{L}+x_{R}\right)+\frac{1}{2}\left(x_{R}-x_{L}\right) X.
	\end{equation}
	This relation can alternatively be expressed by
	\begin{equation}\label{eqa62}
		x=x_{m}+\frac{1}{2} h X,
	\end{equation}
	where we have introduced the element midpoint $x_{m}=\left(x_{L}+x_{R}\right) / 2$ and the element length $h=x_{R}-x_{L}$.
	
	Integrating on the reference element is a matter of just changing the integration variable from $x$ to $X$. Let
	\begin{equation}\label{eqa63}
		\tilde{\varphi}_{r}(X)=\varphi_{q(e, r)}(x(X))
	\end{equation}
	be the basis function associated with local node number $r$ in the reference element. The integral transformation reads
	\begin{equation}\label{eqa64}
		\tilde{A}_{r, s}^{(e)}=\int_{\Omega^{(e)}} \varphi_{q(e, r)}(x) \varphi_{q(e, s)}(x) \mathrm{d} x=\int_{-1}^{1} \tilde{\varphi}_{r}(X) \tilde{\varphi}_{s}(X) \frac{d x}{d X} \mathrm{~d} X.
	\end{equation}
	The stretch factor $d x / d X$ between the $x$ and $X$ coordinates becomes the determinant of the Jacobian matrix of the mapping between the coordinate systems in $2 \mathrm{D}$ and 3D. To obtain a uniform notation for $1 \mathrm{D}, 2 \mathrm{D}$, and 3D problems we therefore replace $d x / d X$ by det $J$ already now. In $1 \mathrm{D}$, $\operatorname{det} J=d x / d X=h / 2$. The integration over the reference element is then written as
	\begin{equation}\label{eqa65}
		\tilde{A}_{r, s}^{(e)}=\int_{-1}^{1} \tilde{\varphi}_{r}(X) \tilde{\varphi}_{s}(X) \operatorname{det} J d X.
	\end{equation}
	The corresponding formula for the element vector entries becomes
	\begin{equation}\label{eqa66}
		\tilde{b}_{r}^{(e)}=\int_{\Omega^{(e)}} f(x) \varphi_{q(e, r)}(x) d x=\int_{-1}^{1} f(x(X)) \tilde{\varphi}_{r}(X) \operatorname{det} J d X.
	\end{equation}
	
	Since we from now on will work in the reference element, we need explicit mathematical formulas for the basis functions $\varphi_{i}(x)$ in the reference element only, i.e., we only need to specify formulas for $\tilde{\varphi}_{r}(X)$. This is a very convenient simplification compared to specifying piecewise polynomials in the physical domain.
	
	The $\tilde{\varphi}_{r}(x)$ functions are simply the Lagrange polynomials defined through the local nodes in the reference element. For $d=1$ and two nodes per element, we have the linear Lagrange polynomials
	\begin{equation}\label{eqa67}
		\tilde{\varphi}_{0}(X) =\frac{1}{2}(1-X)
	\end{equation}
	\begin{equation}\label{eqa68}
		\tilde{\varphi}_{1}(X) =\frac{1}{2}(1+X)
	\end{equation}
	Quadratic polynomials, $d=2$, have the formulas
	\begin{equation}\label{eqa69}
		\tilde{\varphi}_{0}(X) =\frac{1}{2}(X-1) X
	\end{equation}
	\begin{equation}\label{eqa70}
		\tilde{\varphi}_{1}(X) =1-X^{2}
	\end{equation}
	\begin{equation}\label{eqa71}
		\tilde{\varphi}_{2}(X) =\frac{1}{2}(X+1)X
	\end{equation}
	In general,
	\begin{equation}\label{eqa72}
		\tilde{\varphi}_{r}(X)=\prod_{s=0, s \neq r}^{d} \frac{X-X_{(s)}}{X_{(r)}-X_{(s)}},
	\end{equation}
	where $X_{(0)}, \ldots, X_{(d)}$ are the coordinates of the local nodes in the reference element. These are normally uniformly spaced: $X_{(r)}=-1+2 r / d, r \in I_{d}$.
	\begin{mybox}
		\textbf{Why reference elements?}
		
		\noindent The great advantage of using reference elements is that the formulas for the basis functions, $\tilde{\varphi}_{r}(X)$, are the same for all elements and independent of the element geometry (length and location in the mesh). The geometric information is "factored out" in the simple mapping formula and the associated det $J$ quantity, but this information is (here taken as) the same for element types. Also, the integration domain is the same for all elements.
	\end{mybox}
	
	
	\section[Example: Integration over a reference element]{Example: Integration over a reference element}
	\label{sec:sec_3_9}
	\noindent To illustrate the concepts from the previous section in a specific example, we now consider calculation of the element matrix and vector for a specific choice of $d$ and $f(x)$. A simple choice is $d=1$ (P1 elements) and $f(x)=x(1-x)$ on $\Omega=[0,1]$. We have the general expressions (\hyperref[eqa65]{65}) and (\hyperref[eqa66]{66}) for $\tilde{A}_{r, s}^{(e)}$ and $\tilde{b}_{r}^{(e)}$. Writing these out for the choices (\hyperref[eqa67]{67}) and (\hyperref[eqa68]{68}), and using that $\operatorname{det} J=h / 2$, we can do the following calculations of the element matrix entries:
	\begin{equation}\label{eqa73}
		\begin{aligned}
			\tilde{A}_{0,0}^{(e)} &=\int_{-1}^{1} \tilde{\varphi}_{0}(X) \tilde{\varphi}_{0}(X) \frac{h}{2} d X \\
			&=\int_{-1}^{1} \frac{1}{2}(1-X) \frac{1}{2}(1-X) \frac{h}{2} d X=\frac{h}{8} \int_{-1}^{1}(1-X)^{2} d X=\frac{h}{3},
		\end{aligned}
	\end{equation}
	\begin{equation}\label{eqa74}
		\begin{aligned}
			\tilde{A}_{1,0}^{(e)} &=\int_{-1}^{1} \tilde{\varphi}_{1}(X) \tilde{\varphi}_{0}(X) \frac{h}{2} d X \\
			&=\int_{-1}^{1} \frac{1}{2}(1+X) \frac{1}{2}(1-X) \frac{h}{2} d X=\frac{h}{8} \int_{-1}^{1}\left(1-X^{2}\right) d X=\frac{h}{6},
		\end{aligned}
	\end{equation}
	\begin{equation}\label{eqa75}
		\tilde{A}_{0,1}^{(e)} =\tilde{A}_{1,0}^{(e)},
	\end{equation}
	\begin{equation}\label{eqa76}
		\begin{aligned}
			\tilde{A}_{1,1}^{(e)} &=\int_{-1}^{1} \tilde{\varphi}_{1}(X) \tilde{\varphi}_{1}(X) \frac{h}{2} d X \\
			&=\int_{-1}^{1} \frac{1}{2}(1+X) \frac{1}{2}(1+X) \frac{h}{2} d X=\frac{h}{8} \int_{-1}^{1}(1+X)^{2} d X=\frac{h}{3}.
		\end{aligned}
	\end{equation}
	The corresponding entries in the element vector becomes
	\begin{equation}\label{eqa77}
		\begin{aligned}
			\tilde{b}_{0}^{(e)} &=\int_{-1}^{1} f(x(X)) \tilde{\varphi}_{0}(X) \frac{h}{2} d X \\
			&=\int_{-1}^{1}\left(x_{m}+\frac{1}{2} h X\right)\left(1-\left(x_{m}+\frac{1}{2} h X\right)\right) \frac{1}{2}(1-X) \frac{h}{2} d X \\
			&=-\frac{1}{24} h^{3}+\frac{1}{6} h^{2} x_{m}-\frac{1}{12} h^{2}-\frac{1}{2} h x_{m}^{2}+\frac{1}{2} h x_{m}.
		\end{aligned}
	\end{equation}
	\begin{equation}\label{eqa78}
		\begin{aligned}
			\tilde{b}_{1}^{(e)} &=\int_{-1}^{1} f(x(X)) \tilde{\varphi}_{1}(X) \frac{h}{2} d X \\
			&=\int_{-1}^{1}\left(x_{m}+\frac{1}{2} h X\right)\left(1-\left(x_{m}+\frac{1}{2} h X\right)\right) \frac{1}{2}(1+X) \frac{h}{2} d X \\
			&=-\frac{1}{24} h^{3}-\frac{1}{6} h^{2} x_{m}+\frac{1}{12} h^{2}-\frac{1}{2} h x_{m}^{2}+\frac{1}{2} h x_{m}.
		\end{aligned}
	\end{equation}
	In the last two expressions we have used the element midpoint $x_{m}$.
	
	Integration of lower-degree polynomials above is tedious, and higher-degree polynomials involve very much more algebra, but sympy may help. For example, we can easily calculate (\hyperref[eqa73]{73}), (\hyperref[eqa73]{73}) and (\hyperref[eqa77]{77}) by
	\begin{lstlisting}[numbers=none]
		>>> import sympy as sp
		>>> x, x_m, h, X = sp.symbols('x x_m h X')
		>>> sp.integrate(h/8*(1-X)**2, (X, -1, 1))
		h/3
		>>> sp.integrate(h/8*(1+X)*(1-X), (X, -1, 1))
		h/6
		>>> x = x_m + h/2*X
		>>> b_0 = sp.integrate(h/4*x*(1-x)*(1-X), (X, -1, 1))
		>>> print b_0
		-h**3/24 + h**2*x_m/6 - h**2/12 - h*x_m**2/2 + h*x_m/2	
	\end{lstlisting}
	For inclusion of formulas in documents (like the present one), sympy can print
	expressions in LATEX format:
	\begin{lstlisting}[numbers=none]
	>>> print sp.latex(b_0, mode='plain')
	- \frac{1}{24} h^{3} + \frac{1}{6} h^{2} x_{m}
	- \frac{1}{12} h^{2} - \half h x_{m}^{2}
	+ \half h x_{m
	\end{lstlisting}
	\chapter{Implementation}
	\label{chap:chap_4}
	\noindent Based on the experience from the previous example, it makes sense to write
	some code to automate the analytical integration process for any choice of finite
	element basis functions. In addition, we can automate the assembly process
	and linear system solution. Appropriate functions for this purpose document
	all details of all steps in the finite element computations and can found in the
	module file \href{https://github.com/hplgit/INF5620/blob/master/src/fem/fe_approx1D.py}{fe\textunderscore approx1D.py.} The key steps in the computational machinery are
	now explained in detail in terms of code and text.
	\section[Integration]{Integration}
	\label{sec:sec_4_1}
	First we need a Python function for defining $\tilde{\varphi}_{r}(X)$ in terms of a Lagrange polynomial of degree $d$ :
	\begin{lstlisting}[numbers=none]
		import sympy as sp
		import numpy as np
		def phi_r(r, X, d):
		if isinstance(X, sp.Symbol):
		h = sp.Rational(1, d) # node spacing
		nodes = [2*i*h - 1 for i in range(d+1)]
		else:
		# assume X is numeric: use floats for nodes
		nodes = np.linspace(-1, 1, d+1)
		return Lagrange_polynomial(X, r, nodes)
		
		def Lagrange_polynomial(x, i, points):
		p = 1
		for k in range(len(points)):
		if k != i:
		p *= (x - points[k])/(points[i] - points[k])
		return p	
	\end{lstlisting}
	Observe how we construct the phi\textunderscore r function to be a symbolic expression for $\tilde{\varphi}_{r}(X)$ if $\mathrm{X}$ is a Symbol object from sympy. Otherwise, we assume that $\mathrm{X}$ is a float object and compute the corresponding floating-point value of $\ddot{\varphi}_{r}(X)$. Recall that the Lagrange\textunderscore polynomial function, here simply copied from Section \hyperref[sec:sec_2_7]{2.7}, works with both symbolic and numeric variables.
	
	The complete basis $\tilde{\varphi}_{0}(X), \ldots, \tilde{\varphi}_{d}(X)$ on the reference element, represented as a list of symbolic expressions, is constructed by
	\begin{lstlisting}[numbers=none]
		def basis(d=1):
		X = sp.Symbol('X')
		phi = [phi_r(r, X, d) for r in range(d+1)]
		return phi	
	\end{lstlisting}
	Now we are in a position to write the function for computing the element matrix:
	\begin{lstlisting}[numbers=none]
		def element_matrix(phi, Omega_e, symbolic=True):
		n = len(phi)
		A_e = sp.zeros((n, n))
		X = sp.Symbol('X')
		if symbolic:
		h = sp.Symbol('h')
		else:
		h = Omega_e[1] - Omega_e[0]
		detJ = h/2 # dx/dX
		for r in range(n):
		for s in range(r, n):
		A_e[r,s] = sp.integrate(phi[r]*phi[s]*detJ, (X, -1, 1))
		A_e[s,r] = A_e[r,s]
		return A_e	
	\end{lstlisting}
	In the symbolic case (symbolic is True), we introduce the element length as
	a symbol h in the computations. Otherwise, the real numerical value of the element interval Omega\textunderscore e is used and the final matrix elements are numbers, not
	symbols. This functionality can be demonstrated:
	\begin{lstlisting}[numbers=none]
		>>> from fe_approx1D import *
		>>> phi = basis(d=1)
		>>> phi
		[1/2 - X/2, 1/2 + X/2]
		>>> element_matrix(phi, Omega_e=[0.1, 0.2], symbolic=True)
		[h/3, h/6]
		[h/6, h/3]
		>>> element_matrix(phi, Omega_e=[0.1, 0.2], symbolic=False)
		[0.0333333333333333, 0.0166666666666667]
		[0.0166666666666667, 0.0333333333333333]	
	\end{lstlisting}
	The computation of the element vector is done by a similar procedure:
	\begin{lstlisting}[numbers=none]
		def element_vector(f, phi, Omega_e, symbolic=True):
		n = len(phi)
		b_e = sp.zeros((n, 1))
		# Make f a function of X
		X = sp.Symbol('X')
		if symbolic:
		h = sp.Symbol('h')
		else:
		h = Omega_e[1] - Omega_e[0]
		x = (Omega_e[0] + Omega_e[1])/2 + h/2*X # mapping
		f = f.subs('x', x) # substitute mapping formula for x
		detJ = h/2 # dx/dX
		for r in range(n):
		b_e[r] = sp.integrate(f*phi[r]*detJ, (X, -1, 1))
		return b_e	
	\end{lstlisting}
	Here we need to replace the symbol $x$ in the expression for $f$ by the mapping formula such that $f$ can be integrated in terms of $X$, cf. the formula $\tilde{b}_{r}^{(e)}=$ $\int_{-1}^{1} f(x(X)) \tilde{\varphi}_{r}(X) \frac{h}{2} d X$.
	
	The integration in the element matrix function involves only products of polynomials, which sympy can easily deal with, but for the right-hand side sympy may face difficulties with certain types of expressions $f$. The result of the integral is then an Integral object and not a number or expression as when symbolic integration is successful. It may therefore be wise to introduce a fallback on numerical integration. The symbolic integration can also take much time before an unsuccessful conclusion so we may also introduce a parameter symbolic and set it to False to avoid symbolic integration:
	\begin{lstlisting}[numbers=none]
		def element_vector(f, phi, Omega_e, symbolic=True):
		...
		if symbolic:
		I = sp.integrate(f*phi[r]*detJ, (X, -1, 1))
		if not symbolic or isinstance(I, sp.Integral):
		h = Omega_e[1] - Omega_e[0] # Ensure h is numerical
		detJ = h/2
		integrand = sp.lambdify([X], f*phi[r]*detJ)
		I = sp.mpmath.quad(integrand, [-1, 1])
		b_e[r] = I
		...
	\end{lstlisting}
	Numerical integration requires that the symbolic integrand is converted to a plain
	Python function (integrand) and that the element length h is a real number.
	\section[Linear system assembly and solution]{Linear system assembly and solution}
	\label{sec:sec_4_2}
	The complete algorithm for computing and assembling the elementwise contributions takes the following form
	\begin{lstlisting}[numbers=none]
		def assemble(nodes, elements, phi, f, symbolic=True):
		N_n, N_e = len(nodes), len(elements)
		if symbolic:
		A = sp.zeros((N_n, N_n))
		b = sp.zeros((N_n, 1)) # note: (N_n, 1) matrix
		else:
		A = np.zeros((N_n, N_n))
		b = np.zeros(N_n)
		for e in range(N_e):
		Omega_e = [nodes[elements[e][0]], nodes[elements[e][-1]]]
		
		A_e = element_matrix(phi, Omega_e, symbolic)
		b_e = element_vector(f, phi, Omega_e, symbolic)
		
		for r in range(len(elements[e])):
		for s in range(len(elements[e])):
		A[elements[e][r],elements[e][s]] += A_e[r,s]
		b[elements[e][r]] += b_e[r]
		return A, b	
	\end{lstlisting}
	The nodes and elements variables represent the finite element mesh as explained earlier.
	
	Given the coefficient matrix $\mathrm{A}$ and the right-hand side $\mathrm{b}$, we can compute the coefficients $\left\{c_{i}\right\}_{i \in \mathcal{I}_{s}}$ in the expansion $u(x)=\sum_{j} c_{j} \varphi_{j}$ as the solution vector $c$ of the linear system:
	\begin{lstlisting}[numbers=none]
		if symbolic:
		c = A.LUsolve(b)
		else:
		c = np.linalg.solve(A, b)
	\end{lstlisting}
	When A and b are sympy arrays, the solution procedure implied by A.LUsolve is
	symbolic. Otherwise, A and b are numpy arrays and a standard numerical solver
	is called. The symbolic version is suited for small problems only (small N values)
	since the calculation time becomes prohibitively large otherwise. Normally, the
	symbolic integration will be more time consuming in small problems than the
	symbolic solution of the linear system.
	\section[Example on computing symbolic approximations]{Example on computing symbolic approximations}
	\label{sec:sec_4_3}
	We can exemplify the use of assemble on the computational case from Section \hyperref[sec:sec_3_6]{3.6} with two P1 clcments (lincar basis functions) on the domain $\Omega=[0,1]$. Let us first work with a symbolic element length:
	\begin{lstlisting}[numbers=none]
		>>> h, x = sp.symbols('h x')
		>>> nodes = [0, h, 2*h]
		>>> elements = [[0, 1], [1, 2]]
		>>> phi = basis(d=1)
		>>> f = x*(1-x)
		>>> A, b = assemble(nodes, elements, phi, f, symbolic=True)
		>>> A
		[h/3, h/6, 0]
		[h/6, 2*h/3, h/6]
		[ 0, h/6, h/3]
		>>> b
		[ h**2/6 - h**3/12]
		[ h**2 - 7*h**3/6]
		[5*h**2/6 - 17*h**3/12]
		>>> c = A.LUsolve(b)
		>>> c
		[ h**2/6]
		[12*(7*h**2/12 - 35*h**3/72)/(7*h)]
		[ 7*(4*h**2/7 - 23*h**3/21)/(2*h)]	
	\end{lstlisting}
	\section[Comparison with finite elements and interpolation/- collocation]{Comparison with finite elements and interpolation/- collocation}
	\label{sec:sec_4_4}
	We may, for comparison, compute the c vector corresponding to an interpolation/collocation method with finite element basis functions. Choosing the nodes as points, the principle is
	$$
	u\left(x_{i}\right)=\sum_{j \in \mathcal{I}_{s}} c_{j} \varphi_{j}\left(x_{i}\right)=f\left(x_{i}\right), \quad i \in \mathcal{I}_{s}.
	$$
	The coefficient matrix $A_{i, j}=\varphi_{j}\left(x_{i}\right)$ becomes the identity matrix because basis function number $j$ vanishes at all nodes, except node $j: \varphi_{j}\left(x_{i}=\delta_{i j}\right.$. Therefore, $c_{i}=f\left(x_{i}\right.$.
	
	The associated sympy calculations are
	\begin{lstlisting}[numbers=none]
		>>> fn = sp.lambdify([x], f)
		>>> c = [fn(xc) for xc in nodes]
		>>> c
		[0, h*(1 - h), 2*h*(1 - 2*h)]
	\end{lstlisting}
	These expressions are much simpler than those based on least squares or projection in combination with finite element basis functions.
	\section[Example on computing numerical approximations]{Example on computing numerical approximations}
	\label{sec:sec_4_5}
	The numerical computations corresponding to the symbolic ones in Section \hyperref[sec:sec_4_3]{4.3},
	and still done by sympy and the assemble function, go as follows:
	\begin{lstlisting}[numbers=none]
		>>> nodes = [0, 0.5, 1]
		>>> elements = [[0, 1], [1, 2]]
		>>> phi = basis(d=1)
		>>> x = sp.Symbol('x')
		>>> f = x*(1-x)
		>>> A, b = assemble(nodes, elements, phi, f, symbolic=False)
		>>> A
		[ 0.166666666666667, 0.0833333333333333, 0]
		[0.0833333333333333, 0.333333333333333, 0.0833333333333333]
		[ 0, 0.0833333333333333, 0.166666666666667]
		>>> b
		[ 0.03125]
		[0.104166666666667]
		[ 0.03125]
		>>> c = A.LUsolve(b)
		>>> c
		[0.0416666666666666]
		[ 0.291666666666667]
		[0.0416666666666666]	
	\end{lstlisting}
	The fe\textunderscore approx1D module contains functions for generating the nodes and
	elements lists for equal-sized elements with any number of nodes per element.
	The coordinates in nodes can be expressed either through the element length
	symbol h (symbolic=True) or by real numbers (symbolic=False):
	\begin{lstlisting}[numbers=none]
		nodes, elements = mesh_uniform(N_e=10, d=3, Omega=[0,1],
		symbolic=True)	
	\end{lstlisting}
	There is also a function
	\begin{lstlisting}[numbers=none]
		def approximate(f, symbolic=False, d=1, N_e=4, filename='tmp.pdf'):
	\end{lstlisting}
	which computes a mesh with $\mathrm{N}_{-}$e elements, basis functions of degree $\mathrm{d}$, and approximates a given symbolic expression $\mathrm{f}$ by a finite element expansion $u(x)=$ $\sum_{j} c_{j} \varphi_{j}(x)$. When symbolic is False, $u(x)=\sum_{j} c_{j} \varphi_{j}(x)$ can be computed at a (large) number of points and plotted together with $f(x)$. The construction of $u$ points from the solution vector $\mathrm{c}$ is done elementwise by evaluating $\sum_{r} c_{r} \tilde{\varphi}_{r}(X)$ at a (large) number of points in each element in the local coordinate system, and the discrete $(x, u)$ values on each element are stored in separate arrays that are finally concatenated to form a global array for $x$ and for $u$. The details are found in the $u_{-} g$ lob function in fe\textunderscore approx1D.py.
	\section[The structure of the coefficient matrix]{The structure of the coefficient matrix}
	\label{sec:sec_4_6}
	\noindent Let us first see how the global matrix looks like if we assemble symbolic element
	matrices, expressed in terms of h, from several elements:
	\begin{lstlisting}[numbers=none]
		>>> d=1; N_e=8; Omega=[0,1] # 8 linear elements on [0,1]
		>>> phi = basis(d)
		>>> f = x*(1-x)
		>>> nodes, elements = mesh_symbolic(N_e, d, Omega)
		>>> A, b = assemble(nodes, elements, phi, f, symbolic=True)
		>>> A
		[h/3, h/6, 0, 0, 0, 0, 0, 0, 0]
		[h/6, 2*h/3, h/6, 0, 0, 0, 0, 0, 0]
		[ 0, h/6, 2*h/3, h/6, 0, 0, 0, 0, 0]
		[ 0, 0, h/6, 2*h/3, h/6, 0, 0, 0, 0]
		[ 0, 0, 0, h/6, 2*h/3, h/6, 0, 0, 0]
		[ 0, 0, 0, 0, h/6, 2*h/3, h/6, 0, 0]
		[ 0, 0, 0, 0, 0, h/6, 2*h/3, h/6, 0]
		[ 0, 0, 0, 0, 0, 0, h/6, 2*h/3, h/6]
		[ 0, 0, 0, 0, 0, 0, 0, h/6, h/3]	
	\end{lstlisting}
	The reader is encouraged to assemble the element matrices by hand and verify
	this result, as this exercise will give a hands-on understanding of what the
	assembly is about. In general we have a coefficient matrix that is tridiagonal:
	\begin{equation}\label{eqa79}
		A=\frac{h}{6}\left(\begin{array}{ccccccccc}
			2 & 1 & 0 & \cdots & \cdots & \cdots & \cdots & \cdots & 0 \\
			1 & 4 & 1 & \ddots & & & & & \vdots \\
			0 & 1 & 4 & 1 & \ddots & & & & \vdots \\
			\vdots & \ddots & & \ddots & \ddots & 0 & & & \vdots \\
			\vdots & & \ddots & \ddots & \ddots & \ddots & \ddots & & \vdots \\
			\vdots & & & 0 & 1 & 4 & 1 & \ddots & \vdots \\
			\vdots & & & & \ddots & \ddots & \ddots & \ddots & 0 \\
			\vdots & & & & & \ddots & 1 & 4 & 1 \\
			0 & \cdots & \cdots & \cdots & \cdots & \cdots & 0 & 1 & 2
		\end{array}\right)
	\end{equation}
	\bigbreak
	The structure of the right-hand side is more difficult to reveal since it involves an assembly of elementwise integrals of $f(x(X)) \tilde{\varphi}_{r}(X) h / 2$, which obviously depend on the particular choice of $f(x)$. Numerical integration can give some insight into the nature of the right-hand side. For this purpose it is easier to look at the integration in $x$ coordinates, which gives the general formula (\hyperref[eqa56]{56}). For equal-sized elements of length $h$, we can apply the Trapezoidal rule at the global node points to arrive at
	\begin{equation}\label{eqa80}
		b_{i} =h\left(\frac{1}{2} \varphi_{i}\left(x_{0}\right) f\left(x_{0}\right)+\frac{1}{2} \varphi_{i}\left(x_{N}\right) f\left(x_{N}\right)+\sum_{j=1}^{N-1} \varphi_{i}\left(x_{j}\right) f\left(x_{j}\right)\right)
	\end{equation}
	\begin{equation}\label{eqa81}
		= \begin{cases}\frac{1}{2} h f\left(x_{i}\right), & i=0 \text { or } i=N, \\
			h f\left(x_{i}\right), & 1 \leq i \leq N-1\end{cases}
	\end{equation}
	The reason for this simple formula is simply that $\varphi_{i}$ is either 0 or 1 at the nodes and 0 at all but one of them.
	Going to P2 elements $(d=2)$ leads to the element matrix
	\begin{equation}\label{eqa82}
		A^{(e)}=\frac{h}{30}\left(\begin{array}{ccc}
			4 & 2 & -1 \\
			2 & 16 & 2 \\
			-1 & 2 & 4
		\end{array}\right)
	\end{equation}
	and the following global assembled matrix from four elements:
	\begin{equation}\label{eqa83}
		A=\frac{h}{30}\left(\begin{array}{ccccccccc}
			4 & 2 & -1 & 0 & 0 & 0 & 0 & 0 & 0 \\
			2 & 16 & 2 & 0 & 0 & 0 & 0 & 0 & 0 \\
			-1 & 2 & 8 & 2 & -1 & 0 & 0 & 0 & 0 \\
			0 & 0 & 2 & 16 & 2 & 0 & 0 & 0 & 0 \\
			0 & 0 & -1 & 2 & 8 & 2 & -1 & 0 & 0 \\
			0 & 0 & 0 & 0 & 2 & 16 & 2 & 0 & 0 \\
			0 & 0 & 0 & 0 & -1 & 2 & 8 & 2 & -1 \\
			0 & 0 & 0 & 0 & 0 & 0 & 2 & 16 & 2 \\
			0 & 0 & 0 & 0 & 0 & 0 & -1 & 2 & 4
		\end{array}\right)
	\end{equation}
	In general, for $i$ odd we have the nonzeroes
	$$
	A_{i, i-2}=-1, \quad A_{i-1, i}=2, \quad A_{i, i}=8, \quad A_{i+1, i}=2, \quad A_{i+2, i}=-1,
	$$
	multiplied by $h / 30$, and for $i$ even we have the nonzeros
	$$
	A_{i-1, i}=2, \quad A_{i, i}=16, \quad A_{i+1, i}=2,
	$$
	multiplied by $h / 30$. The rows with odd numbers correspond to nodes at the element boundaries and get contributions from two neighboring elements in the assembly process, while the even numbered rows correspond to internal nodes in the elements where the only one element contributes to the values in the global matrix.
	\section[Applications]{Applications}
	\label{sec:sec_4_7}
	\noindent With the aid of the approximate function in the fe\textunderscore approx1D module we can easily investigate the quality of various finite element approximations to some given functions. Figure 29 shows how linear and quadratic elements approximates the polynomial $f(x)=x(1-x)^{8}$ on $\Omega=[0,1]$, using equal-sized elements. The results arise from the program
	\begin{lstlisting}[numbers=none]
		import sympy as sp
		from fe_approx1D import approximate
		x = sp.Symbol('x')
		approximate(f=x*(1-x)**8, symbolic=False, d=1, N_e=4)
		approximate(f=x*(1-x)**8, symbolic=False, d=2, N_e=2)
		approximate(f=x*(1-x)**8, symbolic=False, d=1, N_e=8)
		approximate(f=x*(1-x)**8, symbolic=False, d=2, N_e=4)	
	\end{lstlisting}
	The quadratic functions are seen to be better than the linear ones for the same
	value of N, as we increase N. This observation has some generality: higher
	degree is not necessarily better on a coarse mesh, but it is as we refined the
	mesh.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_29}
		\caption{Comparison of the finite element approximations: 4 P1 elements with
			5 nodes (upper left), 2 P2 elements with 5 nodes (upper right), 8 P1 elements
			with 9 nodes (lower left), and 4 P2 elements with 9 nodes (lower right).}
		\label{fig:img_29}
	\end{figure}
	\section[Sparse matrix storage and solution]{Sparse matrix storage and solution}
	\label{sec:sec_4_8}
	\noindent Some of the examples in the preceding section took several minutes to compute, even on small meshes consisting of up to eight elements. The main explanation for slow computations is unsuccessful symbolic integration: sympy may use a lot of energy on integrals like $\int f(x(X)) \tilde{\varphi}_{r}(X) h / 2 d x$ before giving up, and the program then resorts to numerical integration. Codes that can deal with a large number of basis functions and accept flexible choices of $f(x)$ should compute all integrals numerically and replace the matrix objects from sympy by the far more efficient array objects from numpy.
	
	Another reason for slow code is related to the fact that most of the matrix entries $A_{i, j}$ are zero, because $\left(\varphi_{i}, \varphi_{j}\right)=0$ unless $i$ and $j$ are nodes in the same element. A matrix whose majority of entries are zeros, is known as a sparse matrix. The sparsity should be utilized in software as it dramatically decreases the storage demands and the CPU-time needed to compute the solution of the linear system. This optimization is not critical in 1D problems where modern computers can afford computing with all the zeros in the complete square matrix, but in $2 \mathrm{D}$ and especially in 3D, sparse matrices are fundamental for feasible finite element computations.
	
	In 1D problems, using a numbering of nodes and elements from left to right
	over the domain, the assembled coefficient matrix has only a few diagonals
	different from zero. More precisely, 2d + 1 diagonals are different from zero.
	With a different numbering of global nodes, say a random ordering, the diagonal
	structure is lost, but the number of nonzero elements is unaltered. Figures \hyperref[fig:img_30]{30}
	and \hyperref[fig:img_31]{31} exemplify sparsity patterns.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_30}
		\caption{Matrix sparsity pattern for left-to-right numbering (left) and random
			numbering (right) of nodes in P1 elements.}
		\label{fig:img_30}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_31}
		\caption{Matrix sparsity pattern for left-to-right numbering (left) and random
			numbering (right) of nodes in P3 elements.}
		\label{fig:img_31}
	\end{figure}
	
	The scipy.sparse library supports creation of sparse matrices and linear system solution.
	\begin{itemize}
		\item scipy.sparse.diags for matrix defined via diagonals
		\item scipy.sparse.lil\textunderscore matrix for creation via setting matrix entries
		\item scipy.sparse.dok\textunderscore matrix for creation via setting matrix entries
	\end{itemize}
	\chapter{Comparison of finite element and finite difference approximation}
	\label{chap:chap_5}
	\noindent The previous sections on approximating f by a finite element function u utilize
	the projection/Galerkin or least squares approaches to minimize the approximation error. We may, alternatively, use the collocation/interpolation method
	as described in Section \hyperref[sec:sec_4_4]{4.4}. Here we shall compare these three approaches with
	what one does in the finite difference method when representing a given function
	on a mesh.
	\section[Finite difference approximation of given functions]{Finite difference approximation of given functions}
	\label{sec:sec_5_1}
	Approximating a given function $f(x)$ on a mesh in a finite difference context will typically just sample $f$ at the mesh points. If $u_{\imath}$ is the value of the approximate $u$ at the mesh point $x_{i}$, we have $u_{i}=f\left(x_{i}\right)$. The collocation/interpolation method using finite element basis functions gives exactly the same representation, as shown Section \hyperref[sec:sec_4_4]{4.4},
	$$
	u\left(x_{i}\right)=c_{i}=f\left(x_{i}\right) .
	$$
	How does a finite element Galerkin or least squares approximation differ from this straightforward interpolation of $f$ ? This is the question to be addressed next. We now limit the scope to $\mathrm{P} 1$ elements since this is the element type that gives formulas closest to those arising in the finite difference method.
	\section[Finite difference interpretation of a finite element approximation]{Finite difference interpretation of a finite element approximation}
	\label{sec:sec_5_2}
	The linear system arising from a Galerkin or least squares approximation reads in general
	$$
	\sum_{j \in \mathcal{I}_{s}} c_{j}\left(\psi_{i}, \psi_{j}\right)=\left(f, \psi_{i}\right), \quad i \in \mathcal{I}_{s}.
	$$
	In the finite element approximation we choose $\psi_{i}=\varphi_{i}$. With $\varphi_{i}$ corresponding to $\mathrm{P} 1$ elements and a uniform mesh of element length $h$ we have in Section \hyperref[sec:sec_3_6]{3.6} calculated the matrix with entries $\left(\varphi_{i}, \varphi_{j}\right)$. Equation number $i$ reads
	\begin{equation}\label{eqa84}
		\frac{h}{6}\left(u_{i-1}+4 u_{i}+u_{i+1}\right)=\left(f, \varphi_{i}\right).
	\end{equation}
	The first and last equation, corresponding to $i=0$ and $i=N$ are slightly different, see Section \hyperref[sec:sec_4_6]{4.6}.
	
	The finite difference counterpart to (\hyperref[eqa84]{84}) is just $u_{i}=f_{i}$ as explained in Section \hyperref[sec:sec_5_1]{5.1}. To easier compare this result to the finite element approach to approximating functions, we can rewrite the left-hand side of (\hyperref[eqa84]{84}) as
	\begin{equation}\label{eqa85}
		h\left(u_{i}+\frac{1}{6}\left(u_{i-1}-2 u_{i}+u_{i+1}\right)\right).
	\end{equation}
	Thinking in terms of finite differences, we can write this expression using finite difference operator notation:
	$$
	\left[h\left(u+\frac{h^{2}}{6} D_{x} D_{x} u\right)\right]_{i},
	$$
	which is nothing but the standard discretization of
	$$
	h\left(u+\frac{h^{2}}{6} u^{\prime \prime}\right) .
	$$
	Before interpreting the approximation procedure as solving a differential equation, we need to work out what the right-hand side is in the context of P1 elements. Since $\varphi_{i}$ is the linear function that is 1 at $x_{i}$ and zero at all other nodes, only the interval $\left[x_{i-1}, x_{i+1}\right]$ contribute to the integral on the right-hand side. This integral is naturally split into two parts according to (\hyperref[eqa54]{54}):
	$$
	\left(f, \varphi_{i}\right)=\int_{x_{i-1}}^{x_{i}} f(x) \frac{1}{h}\left(x-x_{i-1}\right) d x+\int_{x_{i}}^{x_{i+1}} f(x) \frac{1}{h}\left(1-\left(x-x_{i}\right)\right) d x.
	$$
	However, if $f$ is not known we cannot do much else with this expression. It is clear that many values of $f$ around $x_{i}$ contributes to the right-hand side, not just the single point value $f\left(x_{i}\right)$ as in the finite difference method.
	
	To proceed with the right-hand side, we can turn to numerical integration schemes. The Trapezoidal method for $\left(f, \varphi_{i}\right)$, based on sampling the integrand $f \varphi_{i}$ at the node points $x_{i}=i h$ gives
	$$
	\left(f, \varphi_{i}\right)=\int_{\Omega} f \varphi_{i} d x \approx h \frac{1}{2}\left(f\left(x_{0}\right) \varphi_{i}\left(x_{0}\right)+f\left(x_{N}\right) \varphi_{i}\left(x_{N}\right)\right)+h \sum_{j=1}^{N-1} f\left(x_{j}\right) \varphi_{i}\left(x_{j}\right) .
	$$
	Since $\varphi_{i}$ is zero at all these points, except at $x_{i}$, the Trapezoidal rule collapses to one term:
	\begin{equation}\label{eqa86}
		\left(f, \varphi_{i}\right) \approx h f\left(x_{i}\right),
	\end{equation}
	for $i=1, \ldots, N-1$, which is the same result as with collocation/interpolation, and of course the same result as in the finite difference method. For $i=0$ and $i=N$ we get contribution from only one element so
	\begin{equation}\label{eqa87}
		\left(f, \varphi_{i}\right) \approx \frac{1}{2} h f\left(x_{i}\right), \quad i=0, i=N .
	\end{equation}
	Simpson's rule with sample points also in the middle of the elements, at $x_{i+\frac{1}{2}}=\left(x_{i}+x_{i+1}\right) / 2$, can be written as
	$$
	\int_{\Omega} g(x) d x \approx \frac{\tilde{h}}{3}\left(g\left(x_{0}\right)+2 \sum_{j=1}^{N-1} g\left(x_{j}\right)+4 \sum_{j=0}^{N-1} g\left(x_{j+\frac{1}{2}}\right)+f\left(x_{2 N}\right)\right),
	$$
	where $\tilde{h}=h / 2$ is the spacing between the sample points. Our integrand is $g=$ $f \varphi_{i}$. For all the node points, $\varphi_{i}\left(x_{j}\right)=\delta_{i j}$, and therefore $\sum_{j=1}^{N-1} f\left(x_{j}\right) \varphi_{i}\left(x_{j}\right)=$ $f\left(x_{i}\right)$. At the midpoints, $\varphi_{i}\left(x_{i \pm \frac{1}{2}}\right)=1 / 2$ and $\varphi_{i}\left(x_{j+\frac{1}{2}}\right)=0$ for $j>1$ and $j<i-1$. Consequently,
	$$
	\sum_{j=0}^{N-1} f\left(x_{j+\frac{1}{2}}\right) \varphi_{i}\left(x_{j+\frac{1}{2}}\right)=\frac{1}{2}\left(f x_{j-\frac{1}{2}}+x_{j+\frac{1}{2}}\right) \text {. }
	$$
	When $1 \leq i \leq N-1$ we then get
	\begin{equation}\label{eqa88}
		\left(f, \varphi_{i}\right) \approx \frac{h}{3}\left(f_{i-\frac{1}{2}}+f_{i}+f_{i+\frac{1}{2}}\right) .
	\end{equation}
	This result shows that, with Simpson's rule, the finite element method operates with the average of $f$ over three points, while the finite difference method just applies $f$ at one point. We may interpret this as a "smearing" or smoothing of $f$ by the finite element method.
	
	We can now summarize our findings. With the approximation of $\left(f, \varphi_{i}\right)$ by the Trapezoidal rule, P1 elements give rise to equations that can be expressed as a finite difference discretization of
	\begin{equation}\label{eqa89}
		u+\frac{h^{2}}{6} u^{\prime \prime}=f, \quad u^{\prime}(0)=u^{\prime}(L)=0,
	\end{equation}
	expressed with operator notation as
	\begin{equation}\label{eqa90}
		\left[u+\frac{h^{2}}{6} D_{x} D_{x} u=f\right]_{i}.
	\end{equation}
	As $h \rightarrow 0$, the extra term proportional to $u^{\prime \prime}$ goes to zero, and the two methods are then equal.
	With the Simpson's rule, we may say that we solve
	\begin{equation}\label{eqa91}
		\left[u+\frac{h^{2}}{6} D_{x} D_{x} u=\bar{f}\right]_{i},
	\end{equation}
	where $\bar{f}_{i}$ means the average $\frac{1}{3}\left(f_{i-1 / 2}+f_{i}+f_{i+1 / 2}\right)$.
	
	The extra term $\frac{h^{2}}{6} u^{\prime \prime}$ represents a smoothing effect: with just this term, we would find $u$ by integrating $f$ twice and thereby smooth $f$ considerably. In addition, the finite element representation of $f$ involves an average, or a smoothing, of $f$ on the right-hand side of the equation system. If $f$ is a noisy function, direct interpolation $u_{i}=f_{i}$ may result in a noisy $u$ too, but with a Galerkin or least squares formulation and P1 elements, we should expect that $u$ is smoother than $f$ unless $h$ is very small.
	
	The interpretation that finite elements tend to smooth the solution is valid in applications far beyond approximation of $1 \mathrm{D}$ functions.
	
	\section[Making finite elements behave as finite differences]{Making finite elements behave as finite differences}
	\label{sec:sec_5_3}
	\noindent With a simple trick, using numerical integration, we can easily produce the result $u_{i}=f_{i}$ with the Galerkin or least square formulation with P1 elements. This is useful in many occasions when we deal with more difficult differential equations and want the finite element method to have properties like the finite difference method (solving standard linear wave equations is one primary example).
	
	\noindent \textbf{Computations in physical space.} We have already seen that applying the Trapezoidal rule to the right-hand side $\left(f, \varphi_{i}\right)$ simply gives $f$ sampled at $x_{i}$. Using the Trapezoidal rule on the matrix entries $A_{i, j}=\left(\varphi_{i}, \varphi_{j}\right)$ involves a sum
	$$
	\sum_{k} \varphi_{i}\left(x_{k}\right) \varphi_{j}\left(x_{k}\right),
	$$
	but $\varphi_{i}\left(x_{k}\right)=\delta_{i k}$ and $\varphi_{j}\left(x_{k}\right)=\delta_{j k}$. The product $\varphi_{i} \varphi_{j}$ is then different from zero only when sampled at $x_{i}$ and $i=j$. The Trapezoidal approximation to the integral is then
	$$
	\left(\varphi_{i}, \varphi_{j}\right) \approx h, \quad i=j,
	$$
	and zero if $i \neq j$. This means that we have obtained a diagonal matrix! The first and last diagonal elements, $\left(\varphi_{0}, \varphi_{0}\right)$ and $\left(\varphi_{N}, \varphi_{N}\right)$ get contribution only from the first and last element, respectively, resulting in the approximate integral value $h / 2$. The corresponding right-hand side also has a factor $1 / 2$ for $i=0$ and $i=N$. Therefore, the least squares or Galerkin approach with $\mathrm{P} 1$ elements and Trapezoidal integration results in
	$$
	c_{i}=f_{i}, \quad i \in \mathcal{I}_{s} .
	$$
	Simpsons's rule can be used to achieve a similar result for $\mathrm{P} 2$ elements, i.e, a diagonal coefficient matrix, but with the previously derived average of $f$ on the right-hand side.
	\bigbreak
	\noindent \textbf{Elementwise computations.} Identical results to those above will arise if we perform elementwise computations. The idea is to use the Trapezoidal rule on the reference element for computing the element matrix and vector. When assembled, the same equations $c_{i}=f\left(x_{i}\right)$ arise. Exercise \hyperref[sec:sec_10_19]{19} encourages you to carry out the details.
	\bigbreak
	\noindent \textbf{Terminology}. The matrix with entries $\left(\varphi_{i}, \varphi_{j}\right)$ typically arises from terms proportional to $u$ in a differential equation where $u$ is the unknown function. This matrix is often called the mass matrix, hecause in the early days of the finite element method, the matrix arose from the mass times acceleration term in Newton's second law of motion. Making the mass matrix diagonal by, e.g., numerical integration, as demonstrated above, is a widely used technique and is called mass lumping. In time-dependent problems it can sometimes enhance the numerical accuracy and computational efficiency of the finite element method. However, there are also examples where mass lumping destroys accuracy.
	
	\chapter{A generalized element concept}
	\label{chap:chap_6}
	\noindent So far, finite element computing has employed the nodes and element lists
	together with the definition of the basis functions in the reference element.
	
	Suppose we want to introduce a piecewise constant approximation with one basis function $\tilde{\varphi}_{0}(x)=1$ in the reference element, corresponding to a $\varphi_{i}(x)$ function that is 1 on element number $i$ and zero on all other elements. Although we could associate the function value with a node in the middle of the elements, there are no nodes at the ends, and the previous code snippets will not work because we cannot find the element boundaries from the nodes list.
	\section[Cells, vertices, and degrees of freedom]{Cells, vertices, and degrees of freedom}
	\label{sec:sec_6_1}
	We now introduce \textit{cells} as the subdomains $\Omega^{(e)}$ previously referred as elements. The cell boundaries are denoted as \textit{vertices}. The reason for this name is that cells are recognized by their vertices in $2 \mathrm{D}$ and 3D. We also define a \textit{set of degrees of freedom}, which are the quantities we aim to compute. The most common type of degree of freedom is the value of the unknown function $u$ at some point. (For example, we can introduce nodes as before and say the degrees of freedom are the values of $u$ at the nodes.) The basis functions are constructed so that they equal unity for one particular degree of freedom and zero for the rest. This property ensures that when we evaluate $u=\sum_{j} c_{j} \varphi_{j}$ for degree of freedom number $i$, we get $u=c_{i}$. Integrals are performed over cells, usually by mapping the cell of interest to a \textit{reference cell}.
	
	With the concepts of cells, vertices, and degrees of freedom we increase the decoupling of the geometry (cell, vertices) from the space of basis functions. We will associate different sets of basis functions with a cell. In 1D, all cells are intervals, while in 2D we can have cells that are triangles with straight sides, or any polygon, or in fact any two-dimensional geometry. Triangles and quadrilaterals are most common, though. The popular cell types in $3 \mathrm{D}$ are tetrahedra and hexahedra.
	\section[Extended finite element concept]{Extended finite element concept}
	\label{sec:sec_6_2}
	The concept of a finite element is now
	\begin{itemize}
		\item a \textit{reference cell} in a local reference coordinate system;
		\item a set of \textit{basis functions} $\tilde{\varphi}_{i}$ defined on the cell;
		\item a set of \textit{degrees of freedom} that uniquely determines the basis functions such that $\tilde{\rho}_{i}=1$ for degree of freedom number $i$ and $\tilde{\rho}_{i}=0$ for all other degrees of freedom;
		\item a mapping between local and global degree of freedom numbers, here called the \textit{dof map};
		\item a geometric \textit{mapping} of the reference cell onto to cell in the physical domain.
	\end{itemize}
	There must be a geometric description of a cell. This is trivial in 1D since the
	cell is an interval and is described by the interval limits, here called vertices. If the cell is $\Omega^{(e)}=\left[x_{L}, x_{R}\right]$, vertex 0 is $x_{L}$ and vertex 1 is $x_{R}$. The reference cell in $1 \mathrm{D}$ is $[-1,1]$ in the reference coordinate system $X$.
	The expansion of $u$ over one cell is often used:
	\begin{equation}\label{eqa92}
		u(x)=\tilde{u}(X)=\sum_{r} c_{r} \tilde{\rho}_{r}(X), \quad x \in \Omega^{(e)}, X \in[-1,1],
	\end{equation}
	where the sum is taken over the numbers of the degrees of freedom and $c_{r}$ is the value of $u$ for degree of freedom number $r$.
	
	Our previous $\mathrm{P} 1, \mathrm{P} 2$, etc., elements are defined by introducing $d+1$ equally spaced nodes in the reference cell and saying that the degrees of freedom are the $d+1$ function values at these nodes. The basis functions must be 1 at one node and 0 at the others, and the Lagrange polynomials have exactly this property. The nodes can be numbered from left to right with associated degrees of freedom that are numbered in the same way. The degree of freedom mapping becomes what was previously represented by the elements lists. The cell mapping is the same affine mapping (\hyperref[eqa61]{61}) as before.
	\section[Implementation]{Implementation}
	\label{sec:sec_6_3}
	\noindent Implementationwise,
	\begin{itemize}
		\item we replace nodes by vertices;
		\item we introduce cells such that cell [e] [r] gives the mapping from local vertex $r$ in cell e to the global vertex number in vertices;
		\item we replace elements by dof\textunderscore map (the contents are the same for $\mathrm{P} d$ elements).
	\end{itemize}
	
	\noindent Consider the example from Section \hyperref[sec:sec_3_1]{3.1} where $\Omega=[0,1]$ is divided into two cells, $\Omega^{(0)}=[0,0.4]$ and $\Omega^{(1)}=[0.4,1]$, as depicted in Figure \hyperref[fig:img_16]{16} . The vertices are $[0,0.4,1]$. Local vertex 0 and 1 are 0 and $0.4$ in cell 0 and $0.4$ and 1 in cell 1 . A P2 element means that the degrees of freedom are the value of $u$ at three equally spacd points (nodes) in each cll. The data structuress become
	\begin{lstlisting}[numbers=none]
		vertices = [0, 0.4, 1]
		cells = [[0, 1], [1, 2]]
		dof_map = [[0, 1, 2], [2, 3, 4]]	
	\end{lstlisting}
	If we would approximate $f$ by piecewise constants, known as $\mathrm{P} 0$ elements, we simply introduce one point or node in an element, preferably $X=0$, and define one degree of freedom, which is the function value at this node. Moreover, we set $\tilde{\varphi}_{0}(X)=1$. The cells and vertices arrays remain the same, but dof\textunderscore map is altered:
	\begin{lstlisting}[numbers=none]
		dof_map = [[0], [1]]	
	\end{lstlisting}
	We use the cells and vertices lists to retrieve information on the geometry of a cell, while dof\textunderscore map is the $q(e, r)$ mapping introduced earlier in the assembly of element matrices and vectors. For example, the Omega\textunderscore e variable (representing the cell interval) in previous code snippets must now be computed as
	\begin{lstlisting}[numbers=none]
		Omega_e = [vertices[cells[e][0], vertices[cells[e][1]]	
	\end{lstlisting}
	The assembly is done by
	\begin{lstlisting}[numbers=none]
		A[dof_map[e][r], dof_map[e][s]] += A_e[r,s]
		b[dof_map[e][r]] += b_e[r]	
	\end{lstlisting}
	We will hereafter drop the nodes and elements arrays and work exculsively
	with cells, vertices, and dof\textunderscore map. The module fe\textunderscore approx1D\textunderscore numint.py
	now replaces the module fe\textunderscore approx1D and offers similar functions that work
	with the new concepts:
	\begin{lstlisting}[numbers=none]
		from fe_approx1D_numint import *
		x = sp.Symbol('x')
		f = x*(1 - x)
		N_e = 10
		vertices, cells, dof_map = mesh_uniform(N_e, d=3, Omega=[0,1])
		phi = [basis(len(dof_map[e])-1) for e in range(N_e)]
		A, b = assemble(vertices, cells, dof_map, phi, f)
		c = np.linalg.solve(A, b)
		# Make very fine mesh and sample u(x) on this mesh for plotting
		x_u, u = u_glob(c, vertices, cells, dof_map,
		resolution_per_element=51)
		plot(x_u, u)	
	\end{lstlisting}
	These steps are offered in the approximate function, which we here apply to see
	how well four P0 elements (piecewise constants) can approximate a parabola:
	\begin{lstlisting}[numbers=none]
		from fe_approx1D_numint import *
		x=sp.Symbol("x")
		for N_e in 4, 8:
		approximate(x*(1-x), d=0, N_e=N_e, Omega=[0,1])	
	\end{lstlisting}
	Figure \hyperref[fig:img_32]{32} shows the result.
	
	\section[Computing the error of the approximation]{Computing the error of the approximation}
	\label{sec:sec_6_4}
	So far we have focused on computing the coefficients $c_{j}$ in the approximation $u(x)=\sum_{j} c_{j} \varphi_{j}$ as well as on plotting $u$ and $f$ for visual comparison. A more quantitative comparison needs to investigate the error $e(x)=f(x)-u(x)$. We mostly want a single number to reflect the error and use a norm for this purpose, usually the $L^{2}$ norm
	$$
	\|e\|_{L^{2}}=\left(\int_{\Omega} e^{2} d x\right)^{1 / 2}.
	$$
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_32}
		\caption{Approximation of a parabola by 4 (left) and 8 (right) P0 elements.}
		\label{fig:img_32}
	\end{figure}
	\noindent Since the finite element approximation is defined for all $x \in \Omega$, and we are interested in how $u(x)$ deviates from $f(x)$ through all the elements, we can either integrate analytically or use an accurate numerical approximation. The latter is more convenient as it is a generally feasible and simple approach. The idea is to sample $e(x)$ at a large number of points in each element. The function u\textunderscore glob in the fe\textunderscore approx1D\textunderscore numint module does this for $u(x)$ and returns an array $\mathbf{x}$ with coordinates and an array $u$ with the $u$ values:
	\begin{lstlisting}[numbers=none]
		x, u = u_glob(c, vertices, cells, dof_map,
		resolution_per_element=101)
		e = f(x) - u
	\end{lstlisting}
	Let us use the Trapezoidal method to approximate the integral. Because different
	elements may have different lengths, the x array has a non-uniformly distributed
	set of coordinates. Also, the u\textunderscore glob function works in an element by element
	fashion such that coordinates at the boundaries between elements appear twice.
	We therefore need to use a raw version of the Trapezoidal rule where we just
	add up all the trapezoids:
	$$
	\int_{\Omega} g(x) d x \approx \sum_{j=0}^{n-1} \frac{1}{2}\left(g\left(x_{j}\right)+g\left(x_{j+1}\right)\right)\left(x_{j+1}-x_{j}\right),
	$$
	if $x_{0}, \ldots, x_{n}$ are all the coordinates in $\mathrm{x}$. In vectorized Python code,
	\begin{lstlisting}[numbers=none]
		g_x = g(x)
		integral = 0.5*np.sum((g_x[:-1] + g_x[1:])*(x[1:] - x[:-1]))	
	\end{lstlisting}
	Computing the $L^{2}$ norm of the error, here named E, is now achieved by
	\begin{lstlisting}[numbers=none]
		e2 = e**2
		E = np.sqrt(0.5*np.sum((e2[:-1] + e2[1:])*(x[1:] - x[:-1]))	
	\end{lstlisting}
	\begin{mybox}
		\textbf{How does the error depend on $h$ and $d$ ?}
		
		\noindent Theory and experiments show that the least squares or projection/Galerkin method in combination with $\mathrm{P} d$ elements of equal length $h$ has an error
		\begin{equation}\label{eqa93}
			\|e\|_{L^{2}}=C h^{d+1},
		\end{equation}
		
		\noindent where $C$ is a constant depending on $f$, but not on $h$ or $d$.
	\end{mybox}
	
	\section[Example: Cubic Hermite polynomials]{Example: Cubic Hermite polynomials}
	\label{sec:sec_6_5}
	The finite elements considered so far represent u as piecewise polynomials with
	discontinuous derivatives at the cell boundaries. Sometimes it is desirable to
	have continuous derivatives. A primary examples is the solution of differential
	equations with fourth-order derivatives where standard finite element formulations lead to a need for basis functions with continuous first-order derivatives.
	The most common type of such basis functions in 1D is the so-called cubic
	Hermite polynomials. The construction of such polynomials, as explained next,
	will further exemplify the concepts of a cell, vertex, degree of freedom, and dof
	map.
	
	Given a reference cell $[-1,1]$, we seek cubic polynomials with the values of the \textit{function} and its \textit{first-order derivative} at $X=-1$ and $X=1$ as the four degrees of freedom. Let us number the degrees of freedom as
	\begin{itemize}
		\item 0 : value of function at $X=-1$
		\item 1: value of first derivative at $X=-1$
		\item 2: value of function at $X=1$
		\item 3: value of first derivative at $X=1$
	\end{itemize}
	By having the derivatives as unknowns, we ensure that the derivative of a basis function in two neighboring elements is the same at the node points.
	The four basis functions can be written in a general form
	$$
	\tilde{\varphi}_{i}(X)=\sum_{j=0}^{3} C_{i, j} X^{j},
	$$
	with four coefficients $C_{i, j}, j=0,1,2,3$, to be determined for each $i$. The constraints that basis function number $i$ must be 1 for degree of freedom number $i$ and zero for the other three degrees of freedom, gives four equations to determine $C_{i, j}$ for each $i$. In mathematical detail,
	$$
	\begin{aligned}
		\tilde{\varphi}_{0}(-1)=1, & \tilde{\varphi}_{0}(1)=\tilde{\varphi}_{0}^{\prime}(-1)=\tilde{\varphi}_{i}^{\prime}(1)=0, \\
		\tilde{\varphi}_{1}^{\prime}(-1)=1, & \tilde{\varphi}_{1}(-1)=\tilde{\varphi}_{1}(1)=\tilde{\varphi}_{1}^{\prime}(1)=0, \\
		\tilde{\varphi}_{2}(1)=1, & \tilde{\varphi}_{2}(-1)=\tilde{\varphi}_{2}^{\prime}(-1)=\tilde{\varphi}_{2}^{\prime}(1)=0, \\
		\tilde{\varphi}_{3}^{\prime}(1)=1, & \tilde{\varphi}_{3}(-1)=\tilde{\varphi}_{3}^{\prime}(-1)=\tilde{\varphi}_{3}(1)=0 .
	\end{aligned}
	$$
	These four $4 \times 4$ linear equations can be solved, yielding the following formulas for the cubic basis functions:
	\begin{equation}\label{eqa94}
		\tilde{\varphi}_{0}(X) =1-\frac{3}{4}(X+1)^{2}+\frac{1}{4}(X+1)^{3}
	\end{equation}
	\begin{equation}\label{eqa95}
		\tilde{\varphi}_{1}(X) =-(X+1)\left(1-\frac{1}{2}(X+1)\right)^{2}
	\end{equation}
	\begin{equation}\label{eqa96}
		\tilde{\varphi}_{2}(X) =\frac{3}{4}(X+1)^{2}-\frac{1}{2}(X+1)^{3}
	\end{equation}
	\begin{equation}\label{eqa97}
		\tilde{\varphi}_{3}(X) =-\frac{1}{2}(X+1)\left(\frac{1}{2}(X+1)^{2}-(X+1)\right)
	\end{equation}
	\begin{equation}\label{eqa98}
	\end{equation}	
	The construction of the dof map needs a scheme for numbering the global degrees of freedom. A natural left-to-right numbering has the function value at vertex $x_{i}$ as degree of freedom number $2 i$ and the value of the derivative at $x_{i}$ as degree of freedom number $2 i+1, i=0, \ldots, N_{e}+1$.
	\chapter{Numerical integration}
	\label{chap:chap_7}
	\noindent Finite element codes usually apply numerical approximations to integrals. Since the integrands in the coefficient matrix often are (lower-order) polynomials, integration rules that can integrate polynomials cxactly are popular.
	The numerical integration rules can be expressed in a common form,
	\begin{equation}\label{eqa99}
		\int_{-1}^{1} g(X) d X \approx \sum_{j=0}^{M} w_{j} g\left(\bar{X}_{j}\right),
	\end{equation}	
	where $\bar{X}_{j}$ are integration points and $w_{j}$ are integration weights, $j=0, \ldots, M$. Different rules correspond to different choices of points and weights.
	The very simplest method is the Midpoint rule,
	\begin{equation}\label{eqa100}
		\int_{-1}^{1} g(X) d X \approx 2 g(0), \quad \bar{X}_{0}=0, w_{0}=2,
	\end{equation}	
	which integrates linear functions exactly.
	\section[Newton-Cotes rules]{Newton-Cotes rules}
	\label{sec:sec_7_1}
	\noindent The \href{https://en.wikipedia.org/wiki/Newton%E2%80%93Cotes_formulas}{Newton-Cotes} rules are based on a fixed uniform distribution of the integration points. The first two formulas in this family are the well-known \textit{Trapezoidal rule},
	\begin{equation}\label{eqa101}
		\int_{-1}^{1} g(X) d X \approx g(-1)+g(1), \quad \bar{X}_{0}=-1, \quad \bar{X}_{1}=1, w_{0}=w_{1}=1,
	\end{equation}
	and \textit{Simpson's rule},
	\begin{equation}\label{eqa102}
		\int_{-1}^{1} g(X) d X \approx \frac{1}{3}(g(-1)+4 g(0)+g(1)),
	\end{equation}
	where
	\begin{equation}\label{eqa103}
		\bar{X}_{0}=-1, \bar{X}_{1}=0, \bar{X}_{2}=1, w_{0}=w_{2}=\frac{1}{3}, w_{1}=\frac{4}{3}.
	\end{equation}
	Newton-Cotes rules up to five points is supported in the module file \href{https://github.com/hplgit/INF5620/blob/master/src/fem/numint.py}{numint.py.}
	
	For higher accuracy one can divide the reference cell into a set of subintervals and use the rules above on each subinterval. This approach results in composite rules, well-known from basic introductions to numerical integration of $\int_{a}^{b} f(x) d x$.
	\bigbreak
	\section[Gauss-Legendre rules with optimized points]{Gauss-Legendre rules with optimized points}
	\label{sec:sec_7_2}
	More accurate rules, for a given $M$, arise if the location of the integration points are optimized for polynomial integrands. The\href{https://en.wikipedia.org/wiki/Gaussian_quadrature}{Gauss-Legendre rules} (also known as Gauss-Legendre quadrature or Gaussian quadrature) constitute one such class of integration methods. Two widely applied Gauss-Legendre rules in this family have the choice
	\begin{equation}\label{eqa104}
		M=1: \bar{X}_{0}=-\frac{1}{\sqrt{3}}, \bar{X}_{1}=\frac{1}{\sqrt{3}}, w_{0}=w_{1}=1
	\end{equation}
	\begin{equation}\label{eqa105}
		M=2: \bar{X}_{0}=-\sqrt{\frac{3}{5}}, \bar{X}_{0}=0, \bar{X}_{2}=\sqrt{\frac{3}{5}}, w_{0}=w_{2}=\frac{5}{9}, w_{1}=\frac{8}{9}.
	\end{equation}
	These rules integrate 3 rd and 5 th degree polynomials exactly. In general, an $M$-point Gauss-Legendre rule integrates a polynomial of degree $2 M+1$ exactly. The code numint.py contains a large collection of Gauss-Legendre rules.
	\chapter{Approximation of functions in 2D}
	\label{chap:chap_8}
	\noindent All the concepts and algorithms developed for approximation of $1 \mathrm{D}$ functions $f(x)$ can readily be extended to $2 \mathrm{D}$ functions $f(x, y)$ and $3 \mathrm{D}$ functions $f(x, y, z)$. Basically, the extensions consists of defining basis functions $\psi_{i}(x, y)$ or $\psi_{i}(x, y, z)$ over some domain $\Omega$, and for the least squares and Galerkin methods, the integration is done over $\Omega$.
	
	As in 1D, the least squares and projection/Galerkin methods two lead to linear systems
	$$
	\begin{aligned}
		\sum_{j \in \mathcal{I}_{s}} A_{i, j} c_{j} &=b_{i}, \quad i \in \mathcal{I}_{s}, \\
		A_{i, j} &=\left(\psi_{i}, \psi_{j}\right), \\
		b_{i} &=\left(f, \psi_{i}\right),
	\end{aligned}
	$$
	where the inner product of two functions $f(x, y)$ and $g(x, y)$ is defined completely analogously to the $1 \mathrm{D}$ case (\hyperref[eqa24]{24}) :
	\begin{equation}\label{eqa106}
		(f, g)=\int_{\Omega} f(x, y) g(x, y) d x d y
	\end{equation}
	\section[2D basis functions as tensor products of 1D functions]{2D basis functions as tensor products of 1D functions}
	\label{sec:sec_8_1}
	
	\noindent One straightforward way to construct a basis in $2 \mathrm{D}$ is to combine $1 \mathrm{D}$ basis functions. Say we have the $1 \mathrm{D}$ vector space
	\begin{equation}\label{eqa107}
		V_{x}=\operatorname{span}\left\{\hat{\psi}_{0}(x), \ldots, \hat{\psi}_{N_{x}}(x)\right\}.
	\end{equation}
	A similar space for variation in $y$ can be defined,
	\begin{equation}\label{eqa108}
		V_{y}=\operatorname{span}\left\{\hat{\psi}_{0}(y), \ldots, \hat{\psi}_{N_{y}}(y)\right\}.
	\end{equation}
	We can then form 2D basis functions as tensor products of 1D basis functions.
	\begin{mybox}
		\textbf{Tensor products.}
		
		Given two vectors $a=\left(a_{0}, \ldots, a_{M}\right)$ and $b=\left(b_{0}, \ldots, b_{N}\right)$, their outer tensor product, also called the dyadic product, is $p=a \otimes b$, defined through
		$$
		p_{i, j}=a_{i} b_{j}, \quad i=0, \ldots, M, j=0, \ldots, N .
		$$
		In the tensor terminology, $a$ and $b$ are first-order tensors (vectors with one
		index, also termed rank-1 tensors). The corresponding \textit{inner tensor product} is the well-known scalar or dot product of two vectors: $p=a \cdot b=\sum_{j=0}^{N} a_{j} b_{j}$. Now, $p$ is a rank-0 tensor.
		
		Tensors are typically represented by arrays in computer code. In the above example, $a$ and $b$ are represented by one-dimensional arrays of length $M$ and $N$, respectively, while $p=a \otimes b$ must be represented by a twodimensional array of size $M \times N$.
		
		\href{https://en.wikipedia.org/wiki/Tensor_product}{Tensor products} can be used in a variety of context.
	\end{mybox}
	
	Given the vector spaces $V_{x}$ and $V_{y}$ as defined in (107) and (108), the tensor product space $V=V_{x} \otimes V_{y}$ has a basis formed as the tensor product of the basis for $V_{x}$ and $V_{y}$. That is, if $\left\{\varphi_{i}(x)\right\}_{i \in \mathcal{I}_{x}}$ and $\left\{\varphi_{i}(y)\right\}_{i \in \mathcal{I}_{y}}$ are basis for $V_{x}$ and $V_{y}$, respectively, the elements in the basis for $V$ arise from the tensor product: $\left\{\varphi_{i}(x) \varphi_{j}(y)\right\}_{i \in \mathcal{I}_{x}, j \in \mathcal{I}_{y}}$. The index sets are $I_{x}=\left\{0, \ldots, N_{x}\right\}$ and $I_{y}=\left\{0, \ldots, N_{y}\right\}$.
	
	The notation for a basis function in 2D can employ a double index as in
	$$
	\psi_{p, q}(x, y)=\hat{\psi}_{p}(x) \hat{\psi}_{q}(y), \quad p \in \mathcal{I}_{x}, q \in \mathcal{I}_{y}.
	$$
	
	The expansion for u is then written as a double sum
	
	$$
	u=\sum_{p \in \mathcal{I}_{x}} \sum_{q \in \mathcal{I}_{y}} c_{p, q} \psi_{p, q}(x, y).
	$$
	Alternatively, we may employ a single index,
	$$
	\psi_{i}(x, y)=\hat{\psi}_{p}(x) \hat{\psi}_{q}(y),
	$$
	and use the standard form for $u$,
	$$
	u=\sum_{j \in \mathcal{I}_{s}} c_{j} \psi_{j}(x, y).
	$$
	The single index is related to the double index through $i=p N_{y}+q$ or $i=q N_{x}+p$.
	\bigbreak
	
	\section[Example: Polynomial basis in 2D]{Example: Polynomial basis in 2D}
	\label{sec:sec_8_2}
	\noindent Suppose we choose $\hat{\psi}_{p}(x)=x^{p}$, and try an approximation with $N_{x}=N_{y}=1$ :
	$$
	\psi_{0,0}=1, \quad \psi_{1,0}=x, \quad \psi_{0,1}=y, \quad \psi_{1,1}=x y.
	$$
	Using a mapping to one index like $i=q N_{x}+p$, we get
	$$
	\psi_{0}=1, \quad \psi_{1}=x, \quad \psi_{2}=y, \quad \psi_{3}=x y.
	$$
	With the specific choice $f(x, y)=\left(1+x^{2}\right)\left(1+2 y^{2}\right)$ on $\Omega=\left[0, L_{x}\right] \times\left[0, L_{y}\right]$, we can perform actual calculations:
	$$
	\begin{aligned}
		&A_{u, 0}=\left(\psi_{0}, \psi_{0}\right)=\int_{0}^{L_{y}} \int_{0}^{L_{x}} \psi_{0}(x, y)^{2} d x d y=\int_{0}^{L_{y}} \int_{0}^{L_{x}} d x d y=L_{x} L_{y}, \\
		&A_{1,0}=\left(\psi_{1}, \psi_{0}\right)=\int_{0}^{L_{y}} \int_{0}^{L_{x}} x d x d y=\frac{1}{2} L_{x}^{2} L_{y}, \\
		&A_{0,1}=\left(\psi_{0}, \psi_{1}\right)=\int_{0}^{L_{y}} \int_{0}^{L_{x}} y d x d y=\frac{1}{2} L_{y}^{2} L_{x}, \\
		&A_{0,1}=\left(\psi_{0}, \psi_{1}\right)=\int_{0}^{L_{y}} \int_{0}^{L_{x}} x y d x d y=\int_{0}^{L_{y}} y d y \int_{0}^{L_{x}} x d x=\frac{1}{4} L_{y}^{2} L_{x}^{2}.
	\end{aligned}
	$$
	The right-hand side vector has the entries
	
	$$
	\begin{aligned}
		b_{0} &=\left(\psi_{0}, f\right)=\int_{0}^{L_{y}} \int_{0}^{L_{x}} 1 \cdot\left(1+x^{2}\right)\left(1+2 y^{2}\right) d x d y \\
		&=\int_{0}^{L_{y}}\left(1+2 y^{2}\right) d y \int_{0}^{L_{x}}\left(1+x^{2}\right) d x=\left(L_{y}+\frac{2}{3} L_{y}^{3}\right)\left(L_{x}+\frac{1}{3} L_{x}^{3}\right) \\
		b_{1} &=\left(\psi_{1}, f\right)=\int_{0}^{L_{y}} \int_{0}^{L_{x}} x\left(1+x^{2}\right)\left(1+2 y^{2}\right) d x d y \\
		&=\int_{0}^{L_{y}}\left(1+2 y^{2}\right) d y \int_{0}^{L_{x}} x\left(1+x^{2}\right) d x=\left(L_{y}+\frac{2}{3} L_{y}^{3}\right)\left(\frac{1}{2} L_{x}^{2}+\frac{1}{4} L_{x}^{4}\right) \\
		b_{2} &=\left(\psi_{2}, f\right)=\int_{0}^{L_{y}} \int_{0}^{L_{x}} y\left(1+x^{2}\right)\left(1+2 y^{2}\right) d x d y \\
		&=\int_{0}^{L_{y}} y\left(1+2 y^{2}\right) d y \int_{0}^{L_{x}}\left(1+x^{2}\right) d x=\left(\frac{1}{2} L_{y}+\frac{1}{2} L_{y}^{4}\right)\left(L_{x}+\frac{1}{3} L_{x}^{3}\right) \\
		b_{3} &=\left(\psi_{2}, f\right)=\int_{0}^{L_{y}} \int_{0}^{L_{x}} x y\left(1+x^{2}\right)\left(1+2 y^{2}\right) d x d y \\
		&=\int_{0}^{L_{y}} y\left(1+2 y^{2}\right) d y \int_{0}^{L_{x}} x\left(1+x^{2}\right) d x=\left(\frac{1}{2} L_{y}^{2}+\frac{1}{2} L_{y}^{4}\right)\left(\frac{1}{2} L_{x}^{2}+\frac{1}{4} L_{x}^{4}\right).
	\end{aligned}
	$$
	There is a general pattern in these calculations that we can explore. An arbitrary matrix entry has the formula
	$$
	\begin{aligned}
		A_{i, j} &=\left(\psi_{i}, \psi_{j}\right)=\int_{0}^{L_{y}} \int_{0}^{L_{x}} \psi_{i} \psi_{j} d x d y \\
		&=\int_{0}^{L_{y}} \int_{0}^{L_{x}} \psi_{p, q} \psi_{r, s} d x d y=\int_{0}^{L_{y}} \int_{0}^{L_{x}} \hat{\psi}_{p}(x) \hat{\psi}_{q}(y) \hat{\psi}_{r}(x) \hat{\psi}_{s}(y) d x d y \\
		&=\int_{0}^{L_{y}} \hat{\psi}_{q}(y) \hat{\psi}_{s}(y) d y \int_{0}^{L_{x}} \hat{\psi}_{p}(x) \hat{\psi}_{r}(x) d x \\
		&=\hat{A}_{p, r}^{(x)} \hat{A}_{q, s}^{(y)},
	\end{aligned}
	$$
	where
	
	$$
	\hat{A}_{p, r}^{(x)}=\int_{0}^{L_{x}} \hat{\psi}_{p}(x) \hat{\psi}_{r}(x) d x, \quad \hat{A}_{q, s}^{(y)}=\int_{0}^{L_{y}} \hat{\psi}_{q}(y) \hat{\psi}_{s}(y) d y,
	$$
	are matrix entries for one-dimensional approximations. Moreover, $i=q N_{y}+q$ and $j=s N_{y}+r$.
	With $\hat{\psi}_{p}(x)=x^{p}$ we have
	$$
	\hat{A}_{p, r}^{(x)}=\frac{1}{p+r+1} L_{x}^{p+r+1}, \quad \hat{A}_{q, s}^{(y)}=\frac{1}{q+s+1} L_{y}^{q+s+1},
	$$
	and
	$$
	A_{i, j}=\hat{A}_{p, r}^{(x)} \hat{A}_{q, s}^{(y)}=\frac{1}{p+r+1} L_{x}^{p+r+1} \frac{1}{q+s+1} L_{y}^{q+s+1},
	$$
	for $p, r \in \mathcal{I}_{x}$ and $q, s \in \mathcal{I}_{y}$.
	Corresponding reasoning for the right-hand side leads to
	$$
	\begin{aligned}
		b_{i} &=\left(\psi_{i}, f\right)=\int_{0}^{L_{y}} \int_{0}^{L_{x}} \psi_{i} f d x d x \\
		&=\int_{0}^{L_{y}} \int_{0}^{L_{x}} \hat{\psi}_{p}\left(x^{2} \hat{\psi}_{q}(y) f d x d x\right.\\
		&=\int_{0}^{L_{y}} \hat{\psi}_{q}(y)\left(1+2 y^{2}\right) d y \int_{0}^{L_{y}} \hat{\psi}_{p}(x) x^{p}\left(1+x^{2}\right) d x \\
		&=\int_{0}^{L_{y}} y^{q}\left(1+2 y^{2}\right) d y \int_{0}^{L_{y}} x^{p}\left(1+x^{2}\right) d x \\
		&=\left(\frac{1}{q+1} L_{y}^{q+1}+\frac{2}{q+3} L_{y}^{q+3}\right)\left(\frac{1}{p+1} L_{x}^{p+1}+\frac{2}{q+3} L_{x}^{p+3}\right)
	\end{aligned}
	$$
	Choosing $L_{x}=L_{y}=2$, we have
	$$
	A=\left[\begin{array}{cccc}
		4 & 4 & 4 & 4 \\
		4 & \frac{16}{3} & 4 & \frac{16}{3} \\
		4 & 4 & \frac{16}{3} & \frac{16}{3} \\
		4 & \frac{16}{3} & \frac{16}{3} & \frac{64}{9}
	\end{array}\right], \quad b=\left[\begin{array}{c}
		\frac{308}{9} \\
		\frac{140}{3} \\
		44 \\
		60
	\end{array}\right], \quad c=\left[\begin{array}{r}
		-\frac{1}{9} \\
		\frac{4}{3} \\
		-\frac{2}{3} \\
		8
	\end{array}\right] .
	$$
	Figure 33 illustrates the result.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_33}
		\caption{Approximation of a 2D quadratic function (left) by a 2D bilinear
			function (right) using the Galerkin or least squares method.}
		\label{fig:img_33}
	\end{figure}
	
	\section[Implementation]{Implementation}
	\label{sec:sec_8_3}
	\noindent The least\textunderscore squares function from Section \hyperref[sec:sec_2_8]{2.8} and/or the file \href{https://github.com/hplgit/INF5620/blob/master/src/fem/fe_approx1D.py}{approx1D.py} can with very small modifications solve $2 \mathrm{D}$ approximation problems. First, let Omega now be a list of the intervals in $x$ and $y$ direction. For example, $\Omega=\left[0, L_{x}\right] \times\left[0, L_{y}\right]$ can be represented by Omega $=\left[\left[0, L_{-} \mathrm{x}\right],\left[0, \mathrm{~L}_{-} \mathrm{y}\right]\right]$. Second, the symbolic integration must be extended to $2 \mathrm{D}$ :
	\begin{lstlisting}[numbers=none]
		import sympy as sp
		
		integrand = psi[i]*psi[j]
		I = sp.integrate(integrand,
		
		(x, Omega[0][0], Omega[0][1]),
		(y, Omega[1][0], Omega[1][1]))	
	\end{lstlisting}
	provided integrand is an expression involving the sympy symbols x and y. The
	2D version of numerical integration becomes
	\begin{lstlisting}[numbers=none]
		if isinstance(I, sp.Integral):
		integrand = sp.lambdify([x,y], integrand)
		I = sp.mpmath.quad(integrand,
		[Omega[0][0], Omega[0][1]],
		[Omega[1][0], Omega[1][1]])	
	\end{lstlisting}
	The right-hand side integrals are modified in a similar way.
	
	Third, we must construct a list of $2 \mathrm{D}$ basis functions. Here are two examples based on tensor products of $1 \mathrm{D}$ "Taylor-style" polynomials $x^{i}$ and $1 \mathrm{D}$ sine functions $\sin ((i+1) \pi x)$ :
	\begin{lstlisting}[numbers=none]
		def taylor(x, y, Nx, Ny):
		return [x**i*y**j for i in range(Nx+1) for j in range(Ny+1)]
		
		def sines(x, y, Nx, Ny):
		return [sp.sin(sp.pi*(i+1)*x)*sp.sin(sp.pi*(j+1)*y)
		for i in range(Nx+1) for j in range(Ny+1)]	
	\end{lstlisting}
	The complete code appears in \href{http://tinyurl.com/jvzzcfn/fem/fe_approx2D.py}{approx2D.py.}
	
	The previous hand calculation where a quadratic f was approximated by a
	bilinear function can be computed symbolically by
	\begin{lstlisting}[numbers=none]
		>>> from approx2D import *
		>>> f = (1+x**2)*(1+2*y**2)
		>>> psi = taylor(x, y, 1, 1)
		>>> Omega = [[0, 2], [0, 2]]
		>>> u = least_squares(f, psi, Omega)
		>>> print u
		8*x*y - 2*x/3 + 4*y/3 - 1/9
		>>> print sp.expand(f)
		2*x**2*y**2 + x**2 + 2*y**2 + 1	
	\end{lstlisting}
	We may continue with adding higher powers to the basis:
	\begin{lstlisting}[numbers=none]
		>>> psi = taylor(x, y, 2, 2)
		>>> u = least_squares(f, psi, Omega)
		>>> print u
		2*x**2*y**2 + x**2 + 2*y**2 + 1
		>>> print u-f
		0	
	\end{lstlisting}
	For $N_{x} \geq 2$ and $N_{y} \geq 2$ we recover the exact function $f$, as expected, since in that case $f \in V$ (see Section \hyperref[sec:sec_2_5]{2.5}).
	\bigbreak 
	
	\section[Extension to 3D]{Extension to 3D}
	\label{sec:sec_8_4}
	\noindent Extension to 3D is in principle straightforward once the $2 \mathrm{D}$ extension is understood. The only major difference is that we need the repeated outer tensor product,
	$$
	V=V_{x} \otimes V_{y} \otimes V_{z}.
	$$
	In general, given vectors (first-order tensors) $a^{(q)}=\left(a_{0}^{(q)}, \ldots, a_{N_{q}}^{(q)}, q=0, \ldots, m\right.$, the tensor product $p=a^{(0)} \otimes \cdots \otimes a^{m}$ has elements
	$$
	p_{i_{0}, i_{1}, \ldots, i_{m}}=a_{i_{1}}^{(0)} a_{i_{1}}^{(1)} \cdots a_{i_{m}}^{(m)} .
	$$
	The basis functions in $3 \mathrm{D}$ are then
	$$
	\psi_{p, q, r}(x, y, z)=\hat{\psi}_{p}(x) \hat{\psi}_{q}(y) \hat{\psi}_{r}(z),
	$$
	with $p \in \mathcal{I}_{x}, q \in \mathcal{I}_{y}, r \in \mathcal{I}_{z}$. The expansion of $u$ becomes
	$$
	u(x, y, z)=\sum_{p \in \mathcal{I}_{x}} \sum_{q \in \mathcal{I}_{y}} \sum_{r \in \mathcal{I}_{z}} c_{p, q, r} \psi_{p, q, r}(x, y, z) .
	$$
	A single index can be introduced also here, e.g., $i=N_{x} N_{y} r+q_{N} x+p, u=$ $\sum_{i} c_{i} \psi_{i}(x, y, z)$.
	\begin{mybox}
		\textbf{Use of tensor product spaces.}
		
		\noindent Constructing a multi-dimensional space and basis from tensor products of 1D spaces is a standard technique when working with global basis functions. In the world of finite elements, constructing basis functions by tensor products is much used on quadrilateral and hexahedra cell shapes, but not on triangles and tetrahedra. Also, the global finite element basis functions are almost exclusively denoted by a single index and not by the natural tuple of indices that arises from tensor products.
	\end{mybox}
	
	\chapter{Finite elements in 2D and 3D}
	\label{chap:chap_9}
	\noindent Finite element approximation is particularly powerful in $2 \mathrm{D}$ and 3D because the method can handle a geometrically complex domain $\Omega$ with ease. The principal idea is, as in $1 \mathrm{D}$, to divide the domain into cells and use polynomials for approximating a function over a cell. Two popular cell shapes are triangles and the quadrilaterals. Figures \hyperref[fig:img_34]{34}, \hyperref[fig:img_35]{35} , and \hyperref[fig:img_36]{36} provide examples. $\mathrm{P} 1$ elements means linear functions $\left(a_{0}+a_{1} x+a_{2} y\right)$ over triangles, while Q1 elements have bilinear functions $\left(a_{0}+a_{1} x+a_{2} y+a_{3} x y\right)$ over rectangular cells. Higher-order elements can easily be defined.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_34}
		\caption{Examples on 2D P1 elements.}
		\label{fig:img_34}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_35}
		\caption{Examples on 2D P1 elements in a deformed geometry.}
		\label{fig:img_35}
	\end{figure}
	\section[Basis functions over triangles in the physical domain]{Basis functions over triangles in the physical domain}
	\label{sec:sec_9_1}
	\noindent Cells with triangular shape will be in main focus here. With the $\mathrm{P} 1$ triangular element, $u$ is a linear function over each cell, as depicted in Figure \hyperref[fig:img_37]{37}, with discontinuous derivatives at the cell boundaries.
	
	We give the vertices of the cells global and local numbers as in 1D. The degrees of freedom in the $\mathrm{P} 1$ element are the function values at a set of nodes, which are the three vertices. The basis function $\varphi_{i}(x, y)$ is then 1 at the vertex with global vertex number $i$ and zero at all other vertices. On an element, the three degrees of freedom uniquely determine the linear basis functions in that element, as usual. The global $\varphi_{i}(x, y)$ function is then a combination of the linear functions (planar surfaces) over all the neighboring cells that have vertex number $i$ in common. Figure \hyperref[fig:img_38]{38} tries to illustrate the shape of such a "pyramid"-like function.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_36}
		\caption{Examples on 2D Q1 elements.}
		\label{fig:img_36}
	\end{figure}
	
	\noindent \textbf{Element matrices and vectors}. As in $1 \mathrm{D}$, we split the integral over $\Omega$ into a sum of integrals over cells. Also as in 1D, $\varphi_{i}$ overlaps $\varphi_{j}$ (i.e., $\varphi_{i} \varphi_{j} \neq 0$ ) if and only if $i$ and $j$ are vertices in the same cell. Therefore, the integral of $\varphi_{i} \varphi_{j}$ over an element is nonzero only when $i$ and $j$ run over the vertex numbers in the element. These nonzero contributions to the coefficient matrix are, as in 1D, collected in an element matrix. The size of the element matrix becomes $3 \times 3$ since there are three degrees of freedom that $i$ and $j$ run nver. Again, as in 1D, we number the local vertices in a cell, starting at 0 , and add the entries in the element matrix into the global system matrix, exactly as in 1D. All details and code appear below.
	\bigbreak
	\section[Basis functions over triangles in the reference cell]{Basis functions over triangles in the reference cell}
	\label{sec:sec_9_2}
	\noindent As in 1D, we can define the basis functions and the degrees of freedom in a reference cell and then use a mapping from the reference coordinate system to the physical coordinate system. We also have a mapping of local degrees of freedom numbers to global degrees of freedom numbers.
	
	The reference cell in an $(X, Y)$ coordinate system has vertices $(0,0),(1,0)$, and $(0,1)$, corresponding to local vertex numbers 0,1 , and 2 , respectively. The $\mathrm{P} 1$ element has linear functions $\tilde{\varphi}_{r}(X, Y)$ as basis functions, $r=0,1,2$. Since a linear function $\tilde{\varphi}_{r}(X, Y)$ in $2 \mathrm{D}$ is on the form $C_{r, 0}+C_{r, 1} X+C_{r, 2} Y$, and hence has three parameters $C_{r, 0}, C_{r, 1}$, and $C_{r, 2}$, we need three degrees of freedom. These are in general taken as the function values at a set of nodes. For the P1
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_37}
		\caption{Example on piecewise linear 2D functions defined on triangles.}
		\label{fig:img_37}
	\end{figure}
	\noindent element the set of nodes is the three vertices. Figure \hyperref[fig:img_39]{39} displays the geometry of the element and the location of the nodes.
	
	Requiring $\tilde{\varphi}_{r}=1$ at node number $r$ and $\tilde{\varphi}_{r}=0$ at the two other nodes, gives three linear equations to determine $C_{r, 0}, C_{r, 1}$, and $C_{r, 2}$. The result is
	\begin{equation}\label{eqa109}
		\tilde{\varphi}_{0}(X, Y)=1-X-Y,
	\end{equation}
	\begin{equation}\label{eqa110}
		\tilde{\varphi}_{1}(X, Y)=X,
	\end{equation}
	\begin{equation}\label{eqa111}
		\tilde{\varphi}_{2}(X, Y)=Y
	\end{equation}
	Higher-order approximations are obtained by increasing the polynomial order, adding additional nodes, and letting the degrees of freedom be function values at the nodes. Figure \hyperref[fig:img_40]{40} shows the location of the six nodes in the P2 element.
	A polynomial of degree $p$ in $X$ and $Y$ has $n_{p}=(p+1)(p+2) / 2$ terms and hence needs $n_{p}$ nodes. The values at the nodes constitute $n_{p}$ degrees of freedom. The location of the nodes for $\tilde{\varphi}_{r}$ up to degree 6 is displayed in Figure \hyperref[fig:img_41]{41}.
	
	The generalization to $3 \mathrm{D}$ is straightforward: the reference element is a \href{https://en.wikipedia.org/wiki/Tetrahedron}{tetrahedron} with vertices $(0,0,0),(1,0,0),(0,1,0)$, and $(0,0,1)$ in a $X, Y, Z$ reference coordinate system. The $\mathrm{P} 1$ element has its degrees of freedom as four nodes, which are the four vertices, see Figure \hyperref[fig:img_42]{42} . The $\mathrm{P} 2$ element adds additional nodes along the edges of the cell, yielding a total of 10 nodes and degrees of freedom, see Figure \hyperref[fig:img_43]{43} .
	
	The interval in 1D, the triangle in 2D, the tetrahedron in 3D, and its generalizations to higher space dimensions are known as \textit{simplex} cells (the
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_38}
		\caption{Example on a piecewise linear 2D basis function over a patch of
			triangles.}
		\label{fig:img_38}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_39}
		\caption{2D P1 element.}
		\label{fig:img_39}
	\end{figure}
	
	\noindent geometry) or \textit{simplex} elements (the geometry, basis functions, degrees of freedom,
	etc.). The plural forms \href{https://en.wikipedia.org/wiki/Simplex}{simplices} and simplexes are also a much used shorter
	terms when referring to this type of cells or elements. The side of a simplex is
	called a \textit{face}, while the tetrahedron also has \textit{edges}.
	
	Acknowledgment. Figures \hyperref[fig:img_39]{39} to \hyperref[fig:img_43]{43} are created by Anders Logg and taken
	from the \href{https://launchpad.net/fenics-book}{FEniCS book}: \textit{Automated Solution of Differential Equations by the
		Finite Element Method}, edited by A. Logg, K.-A. Mardal, and G. N. Wells,
	published by \href{https://link.springer.com/book/10.1007/978-3-642-23099-8}{Springer}, 2012.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_40}
		\caption{2D P2 element.}
		\label{fig:img_40}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_41}
		\caption{2D P1, P2, P3, P4, P5, and P6 elements.}
		\label{fig:img_41}
	\end{figure}
	\section[Affine mapping of the reference cell]{Affine mapping of the reference cell}
	\label{sec:sec_9_3}
	\noindent Let $\tilde{\varphi}_{r}^{(1)}$ denote the basis functions associated with the $\mathrm{P} 1$ element in $1 \mathrm{D}, 2 \mathrm{D}$, or 3D, and let $\boldsymbol{x}_{q(e, r)}$ be the physical coordinates of local vertex number $r$ in cell $e$. Furthermore, let $\boldsymbol{X}$ be a point in the reference coordinate system corresponding to the point $\boldsymbol{x}$ in the physical coordinate system. The affine mapping of any $\boldsymbol{X}$ onto $\boldsymbol{x}$ is then defined by
	\begin{equation}\label{eqa112}
		x=\sum_{r} \tilde{\varphi}_{r}^{(1)}(X) x_{q(e, r)},
	\end{equation}
	where $r$ runs over the local vertex numbers in the cell. The affine mapping essentially stretches, translates, and rotates the triangle. Straight or planar
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_42}
		\caption{P1 elements in 1D, 2D, and 3D.}
		\label{fig:img_42}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_43}
		\caption{P2 elements in 1D, 2D, and 3D.}
		\label{fig:img_43}
	\end{figure}
	
	\noindent faces of the reference cell are therefore mapped onto straight or planar faces
	in the physical coordinate system. The mapping can be used for both P1 and
	higher-order elements, but note that the mapping itself always applies the P1
	basis functions.
	\section[Isoparametric mapping of the reference cell]{Isoparametric mapping of the reference cell}
	\label{sec:sec_9_4}
	Instead of using the $\mathrm{P} 1$ basis functions in the mapping (\hyperref[eqa112]{112}), we may use the basis functions of the actual $\mathrm{Pd}$ element:
	\begin{equation}\label{eqa113}
		x=\sum_{r} \tilde{\varphi}_{r}(X) x_{q(e, r)},
	\end{equation}
	where $r$ runs over all nodes, i.e., all points associated with the degrees of freedom. This is called an \textit{isoparametric mapping}. For P1 elements it is identical to the affine mapping (\hyperref[eqa112]{112}), but for higher-order elements the mapping of the straight or planar faces of the reference cell will result in a \textit{curved} face in the physical coordinate system. For example, when we use the basis functions of the triangular P2 element in $2 \mathrm{D}$ in (\hyperref[eqa113]{113}), the straight faces of the reference triangle are mapped onto curved faces of parabolic shape in the physical coordinate system, see Figure \hyperref[fig:img_45]{45} .
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_44}
		\caption{Affine mapping of a P1 element.}
		\label{fig:img_44}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_45}
		\caption{Isoparametric mapping of a P2 element.}
		\label{fig:img_45}
	\end{figure}
	
	From (\hyperref[eqa112]{112}) or (\hyperref[eqa113]{113}) it is easy to realize that the vertices are correctly mapped. Consider a vertex with local number $s$. Then $\tilde{\varphi}_{s}=1$ at this vertex and zero at the others. This means that only one term in the sum is nonzero and $\boldsymbol{x}=\boldsymbol{x}_{q(e, s)}$, which is the coordinate of this vertex in the global coordinate system.
	\section[Computing integrals]{Computing integrals}
	\label{sec:sec_9_5}
	Let $\tilde{\Omega}^{r}$ denote the reference cell and $\Omega^{(e)}$ the cell in the physical coordinate system. The transformation of the integral from the physical to the reference coordinate system reads
	\begin{equation}\label{eqa114}
		\int_{\Omega^{(e)}} \varphi_{i}(\boldsymbol{x}) \varphi_{j}(\boldsymbol{x}) \mathrm{d} x =\int_{\tilde{\Omega}^{r}} \tilde{\varphi}_{i}(\boldsymbol{X}) \tilde{\varphi}_{j}(\boldsymbol{X}) \operatorname{det} J \mathrm{~d} X
	\end{equation}
	\begin{equation}\label{eqa115}
		\int_{\Omega^{(e)}} \varphi_{i}(\boldsymbol{x}) f(\boldsymbol{x}) \mathrm{d} x =\int_{\tilde{\Omega}^{r}} \tilde{\varphi}_{i}(\boldsymbol{X}) f(\boldsymbol{x}(\boldsymbol{X})) \operatorname{det} J \mathrm{~d} X
	\end{equation}
	
	\noindent where $\mathrm{d} x$ means the infinitesimal area element $d x d y$ in $2 \mathrm{D}$ and $d x d y d z$ in 3D, with a similar definition of $\mathrm{d} X$. The quantity det $J$ is the determinant of the Jacobian of the mapping $\boldsymbol{x}(\boldsymbol{X})$. In 2D,
	\begin{equation}\label{eqa116}
		J=\left[\begin{array}{ll}
			\frac{\partial x}{\partial X} & \frac{\partial x}{\partial Y} \\
			\frac{\partial y}{\partial X} & \frac{\partial y}{\partial Y}
		\end{array}\right], \quad \operatorname{det} J=\frac{\partial x}{\partial X} \frac{\partial y}{\partial Y}-\frac{\partial x}{\partial Y} \frac{\partial y}{\partial X} .
	\end{equation}
	With the affine mapping (\hyperref[eqa112]{112}), $\operatorname{det} J=2 \Delta$, where $\Delta$ is the area or volume of the cell in the physical coordinate system.
	\bigbreak
	\noindent \textbf{Remark.} Observe that finite elements in 2D and 3D builds on the same ideas and \textit{concepts} as in $1 \mathrm{D}$, but there is simply much more to compute because the specific mathematical formulas in 2D and $3 \mathrm{D}$ are more complicated and the book keeping with dof maps also gets more complicated. The manual work is tedious, lengthy, and error-prone so automation by the computer is a must.
	\chapter{Exercises}
	\label{chap:chap_10}
	
	\section*{Exercise 1: Linear algebra refresher I}
	\label{sec:sec_10_1}
	\noindent Look up the topic of \textit{vector} space in your favorite linear algebra book or search for the term at Wikipedia. Prove that vectors in the plane $(a, b)$ form a vector space by showing that all the axioms of a vector space are satisfied. Similarly, prove that all linear functions of the form $a x+b$ constitute a vector space, $a, b \in \mathbb{R}$.
	
	On the contrary, show that all quadratic functions of the form $1+a x^{2}+b x$ \textit{do not} constitute a vector space. Filename: linalg1.pdf.
	\bigbreak
	\section*{Exercise 2: Linear algebra refresher II}
	\label{sec:sec_10_2}
	\noindent As an extension of Exercise 1, check out the topic of \textit{inner product spaces}. Suggest a possible inner product for the space of all linear functions of the form $a x+b$, $a, b \in \mathbb{R}$. Show that this inner product satisfies the general requirements of an inner product in a vector space. Filename: linalg2.pdf.
	\bigbreak
	\section*{Exercise 3: Approximate a three-dimensional vector in a plane}
	\label{sec:sec_10_3}
	\noindent Given $\boldsymbol{f}=(1,1,1)$ in $\mathbb{R}^{3}$, find the best approximation vector $\boldsymbol{u}$ in the plane spanned by the unit vectors $(1,0)$ and $(0,1)$. Repeat the calculations using the vectors $(2,1)$ and $(1,2)$. Filename: vec111\textunderscore approx.pdf.
	\bigbreak
	\section*{Exercise 4: Approximate the exponential function by power functions}
	\label{sec:sec_10_4}
	\noindent Let $V$ be a function space with basis functions $x^{i}, i=0,1, \ldots, N$. Find the best approximation to $f(x)=\exp (-x)$ on $\Omega=[0,4]$ among all functions in $V$ for $N=2,4,6$. Illustrate the three approximations in three separate plots. Add the corresponding Taylor polynomial approximation of degree $N$ in each plot. Filename: exp\textunderscore powers.py.
	\bigbreak
	\section*{Exercise 5: Approximate the sine function by power functions}
	\label{sec:sec_10_5}
	\noindent Let $V$ be a function space with basis functions $x^{2 i+1}, i=0,1, \ldots, N$. Find the best approximation to $f(x)=\sin (x)$ among all functions in $V$, using $N=8$ for a domain that includes more and more half-periods of the sine function: $\Omega=[0, k \pi / 2], k=2,3, \ldots, 12$. How does a Taylor series of $\sin (x)$ around $x$ up to degree 9 behave for the largest domain?
	\bigbreak
	\noindent \textbf{Hint.} One can make a loop over $k$ and call the functions least\textunderscore squares and comparison\textunderscore plot from the approx1D module.
	
	Filename: sin\textunderscore powers.py.
	\bigbreak
	\section*{Exercise 6: Approximate a steep function by sines}
	\label{sec:sec_10_6}
	\noindent Find the best approximation of $f(x)=\tanh (s(x-\pi))$ on $[0,2 \pi]$ in the space $V$ with basis $\psi_{i}(x)=\sin ((2 i+1) x), i \in \mathcal{I}_{s}=\{0, \ldots, N\}$. Make a movie showing how $u=\sum_{j \in \mathcal{I}_{s}} c_{j} \psi_{j}(x)$ approximates $f(x)$ as $N$ grows. Choose $s$ such that $f$ is steep ( $s=20$ may be appropriate).
	\bigbreak
	\noindent \textbf{Hint.} One may naively call the least\textunderscore squares\textunderscore orth and comparison\textunderscore plot from the approx1D module in a loop and extend the basis with one new element in each pass. This approach implies a lot of recomputations. A more efficient strategy is to let least\textunderscore squares\textunderscore orth compute with only one basis function at a time and accumulate the corresponding $u$ in the total solution.
	Filename: tanh\textunderscore sines\textunderscore approx1.py.
	\bigbreak
	\section*{Exercise 7: Animate the approximation of a steep function by sines}
	\label{sec:sec_10_7}
	\noindent Make a movie where the steepness $(s)$ of the tanh function in Exercise \hyperref[sec:sec_10_6]{6} grows in "time", and for each value of the steepness, the movie shows how the approximation improves with increasing $N$. Filename: tanh\textunderscore sines\textunderscore approx2.py.
	\bigbreak
	\section*{Exercise 8: Fourier series as a least squares approximation}
	\label{sec:sec_10_8}
	\noindent Given a function $f(x)$ on an interval $[0, L]$, look up the formula for the coefficients $a_{j}$ and $b_{j}$ in the Fourier series of $f$ :
	$$
	f(x)=a_{0}+\sum_{j=1}^{\infty} a_{j} \cos \left(j \frac{\pi x}{L}\right)+\sum_{j=1}^{\infty} b_{j} \sin \left(j \frac{\pi x}{L}\right).
	$$
	Let an infinite-dimensional vector space $V$ have the basis functions $\cos j \frac{\pi x}{L}$ for $j=0,1, \ldots, \infty$ and $\sin j \frac{\pi x}{L}$ for $j=1, \ldots, \infty$. Show that the least squares approximation method from Section \hyperref[chap:chap_2]{2} leads to a linear system whose solution coincides with the standard formulas for the coefficients in a Fourier series of $\int(x)$ (see also Section \hyperref[sec:sec_2_7]{2.7}). You may choose
	$$
	\psi_{2 i}=\cos \left(i \frac{\pi}{L} x\right), \quad \psi_{2 i+1}=\sin \left(i \frac{\pi}{L} x\right),
	$$
	for $i=0,1, \ldots, N \rightarrow \infty$.
	Choose $f(x)=\tanh \left(s\left(x-\frac{1}{2}\right)\right)$ on $\Omega=[0,1]$, which is a smooth function, but with considerable steepness around $x=1 / 2$ as $s$ grows in size. Calculate the coefficients in the Fourier expansion by solving the linear system, arising from the least squares or Galerkin methods, by hand. Plot some truncated versions of the series together with $f(x)$ to show how the series expansion converges for $s=10$ and $s=100$. Filename: Fourier\textunderscore approx.py.
	\bigbreak
	\section*{Exercise 9: Approximate a steep function by Lagrange polynomials}
	\label{sec:sec_10_9}
	\noindent Use interpolation/collocation with uniformly distributed points and Chebychev nodes to approximate
	$$
	f(x)=-\tanh \left(s\left(x-\frac{1}{2}\right)\right), \quad x \in[0,1]
	$$
	by Lagrange polynomials for $s=10$ and $s=100$, and $N=3,6,9,11$. Make separate plots of the approximation for each combination of $s$, point type (Chebyshev or uniform), and $N$. Filename: tanh\textunderscore Lagrange.py.
	\bigbreak
	\section*{Exercise 10: Define nodes and elements}
	\label{sec:sec_10_10}
	\noindent Consider a domain $\Omega=[0,2]$ divided into the three $\mathrm{P} 2$ elements $[0,1],[1,1.2]$, and $[1.2,2]$.
	
	For P1 and P2 elements, set up the list of coordinates and nodes (nodes) and the numbers of the nodes that belong to each element (elements) in two cases: 1) nodes and elements numbered from left to right, and 2) nodes and elements numbered from right to left. Filename: fe\textunderscore numberings 1 .py ..
	\bigbreak
	\section*{Exercise 11: Define vertices, cells, and dof maps}
	\label{sec:sec_10_11}
	\noindent Repeat Exercise \hyperref[sec:sec_10_10]{10}, but define the data structures vertices, cells, and dof\textunderscore map instead of nodes and elements. Filename: fe\textunderscore numberings2.py.
	\bigbreak
	\section*{Exercise 12: Construct matrix sparsity patterns}
	\label{sec:sec_10_12}
	\noindent Exercise \hyperref[sec:sec_10_10]{10} describes a element mesh with a total of five elements, but with two different element and node orderings. For each of the two orderings, make a $5 \times 5$ matrix and fill in the entries that will be nonzero.
	\bigbreak
	\noindent \textbf{Hint.} A matrix entry $(i, j)$ is nonzero if $i$ and $j$ are nodes in the same element. Filename: \textunderscore sparsity\textunderscore pattern.pdf.
	
	\section*{Exercise 13: Perform symbolic finite element computations}
	\label{sec:sec_10_13}
	\noindent Perform hand calculations to find formulas for the coefficient matrix and righthand side when approximating $f(x)=\sin (x)$ on $\Omega=[0, \pi]$ by two P1 elements of size $\pi / 2$. Solve the system and compare $u(\pi / 2)$ with the exact value 1 .
	Filename: sin\textunderscore approx\textunderscore P1.py.
	\bigbreak
	\section*{Exercise 14: Approximate a steep function by $\mathrm{P} 1$ and $\mathrm{P} 2$ elements}
	\label{sec:sec_10_14}
	\noindent Given
	$$
	f(x)=\tanh \left(s\left(x-\frac{1}{2}\right)\right)
	$$
	use the Galerkin or least squares method with finite elements to find an approximate function $u(x)$. Choose $s=40$ and try $N_{e}=4,8,16 \mathrm{P} 1$ elements and $N_{e}=2,4,8 \mathrm{P} 2$ elements. Integrate $f \varphi_{i}$ numerically. Filename: tanh\textunderscore fe\textunderscore P1P2\textunderscore approx.py.
	\bigbreak
	\section*{Exercise 15: Approximate a steep function by $\mathrm{P} 3$ and $\mathrm{P} 4$ elements}
	\label{sec:sec_10_15}
	\noindent Solve Exercise \hyperref[sec:sec_10_14]{14} using $N_{e}=1,2,4 \mathrm{P} 3$ and P4 elements. How will a collocation/interpolation method work in this case with the same number of nodes? Filename: tanh\textunderscore fe\textunderscore P3P4\textunderscore approx.py.
	\bigbreak
	\section*{Exercise 16: Investigate the approximation error in finite elements}
	\label{sec:sec_10_16}
	\noindent The theory (\hyperref[eqa93]{93}) from Section ?? predicts that the error in the $\mathrm{P} d$ approximation of a function should behave as $h^{d+1}$. Use experiments to verify this asymptotic behavior (i.e., for small enough $h$ ). Choose two examples: $f(x)=A e^{-\omega x}$ on
	
	$[0,3 / \omega]$ and $f(x)=A \sin (\omega x)$ on $\Omega=[0,2 \pi / \omega]$ for constants $A$ and $\omega$. What happens if you try $f(x)=\sqrt{x}$ on $[0,1]$ ?
	\bigbreak
	\noindent \textbf{Hint}. Run a series of experiments: $\left(h_{i}, E\right), i=0, \ldots, m$, where $E_{i}$ is the $L^{2}$ norm of the error corresponding to element length $h_{i}$. Assume an error model $E=C h^{r}$ and compute $r$ from two successive experiments:
	$$
	r_{i}=\ln \left(E_{i+1} / E_{i}\right) / \ln \left(h_{i+1} / h_{i}\right), \quad i=0, \ldots, m-1 .
	$$
	Hopefully, the sequence $r_{0}, \ldots, r_{m-1}$ converges to the true $r$, and $r_{m-1}$ can be taken as an approximation to $r$.
	
	Filename: Asinwt\textunderscore interpolation\textunderscore error.py.
	\bigbreak
	\section*{Exercise 17: Approximate a step function by finite elements}
	\label{sec:sec_10_17}
	\noindent Approximate the step function
	$$
	f(x)= \begin{cases}1 & x<1 / 2 \\ 2 & x \geq 1 / 2\end{cases}
	$$
	by 2,4 , and $8 \mathrm{P} 1$ and P2 elements. Compare approximations visually.
	Hint. This $f$ can also be expressed in terms of the Heaviside function $H(x)$ : $f(x)=H(x-1 / 2)$. Therefore, $f$ can be defined by
	\begin{lstlisting}[numbers=none]
		f = sp.Heaviside(x - sp.Rational(1,2))	
	\end{lstlisting}
	making the approximate function in the fe\textunderscore approx1D.py module an obvious candidate to solve the problem. However, sympy does not handle symbolic integration with this particular integrand, and the approximate function faces a problem when converting $f$ to a Python function (for plotting) since Heaviside is not an available function in numpy. It is better to make special-purpose code for this case or perform all calculations by hand.
	
	Filename: Heaviside\textunderscore approx\textunderscore P1P2.py ..
	\bigbreak
	\section*{Exercise 18: 2D approximation with orthogonal functions}
	\label{sec:sec_10_18}
	\noindent Assume we have basis functions $\varphi_{i}(x, y)$ in $2 \mathrm{D}$ that are orthogonal such that $\left(\varphi_{i}, \varphi_{j}\right)=0$ when $i \neq j$. The function least\textunderscore squares in the file \href{http://tinyurl.com/jvzzcfn/fem/fe_approx2D.py}{approx2D.py} will then spend much time on computing off-diagonal terms in the coefficient matrix that we know are zero. To speed up the computations, make a version least\textunderscore squares\textunderscore orth that utilizes the orthogonality among the basis functions. Apply the function to approximate
	$$
	f(x, y)=x(1-x) y(1-y) e^{-x-y}
	$$
	on $\Omega=[0,1] \times[0,1]$ via basis functions
	$$
	\varphi_{i}(x, y)=\sin (p \pi x) \sin (q \pi y), \quad i=q N_{x}+p
	$$
	
	\noindent \textbf{Hint.} Get ideas from the function least\textunderscore squares\textunderscore orth in Section 2.8 and
	file \href{http://tinyurl.com/jvzzcfn/fem/fe_approx2D.py}{approx2D.py.}
	
	Filename: approx2D\textunderscore lsorth\textunderscore sin.py.
	\bigbreak
	\section*{Exercise 19: Use the Trapezoidal rule and $\mathrm{P} 1$ elements}
	\label{sec:sec_10_19}
	\noindent Consider approximation of some $f(x)$ on an interval $\Omega$ using the least squares or Galerkin methods with $\mathrm{P} 1$ elements. Derive the element matrix and vector using the Trapezoidal rule (\hyperref[eqa101]{101}) for calculating integrals on the reference element. Assemble the contributions, assuming a uniform cell partitioning, and show that the resulting linear system has the form $c_{i}=f\left(x_{i}\right)$ for $i \in \mathcal{I}_{s}$. Filename: fe\textunderscore P1\textunderscore trapez.pdf.
	\bigbreak
	\section*{Problem 20: Compare P1 elements and interpolation}
	\label{sec:sec_10_20}
	\noindent We shall approximate the function
	$$
	f(x)=1+\epsilon \sin (2 \pi n x), \quad x \in \Omega=[0,1]
	$$
	where $n \in \mathbb{Z}$ and $\epsilon \geq 0$.
	\begin{enumerate}[label=(\alph*)]
		\item Sketch $f(x)$ and find the wave length of the function.
		\item We want to use $N_{P}$ elements per wave length. Show that the number of elements is then $n N_{P}$.
		\item The critical quantity for accuracy is the number of elements per wave length, not the element size in itself. It therefore suffices to study an $f$ with just one wave length in $\Omega=[0,1]$. Set $\epsilon=0.5$.
		
		Run the least squares or projection/Galerkin method for $N_{P}=2,4,8,16,32$. Compute the error $E=\|u-f\|_{L^{2}}$.
		\bigbreak
		\textbf{Hint.} Use the fe\textunderscore approx1D\textunderscore numint module to compute $u$ and use the technique from Section \hyperref[sec:sec_6_4]{6.4} to compute the norm of the error.
		\item Repeat the set of experiments in the above point, but use interpolation/collocation based on the node points to compute $u(x)$ (recall that $c_{i}$ is now simply $\left.f\left(x_{i}\right)\right)$. Compute the error $E=\|u-f\|_{L^{2}}$. Which method seems to be most accurate?
		
		Filename: P1\textunderscore vs\textunderscore interp.py.
	\end{enumerate}
	\bigbreak
	\section*{Exercise 21: Implement 3D computations with global basis functions}
	\label{sec:sec_10_21}
	\noindent Extend the \href{http://tinyurl.com/jvzzcfn/fem/approx2D.py}{approx2D.py} code to 3D applying ideas from Section \hyperref[sec:sec_8_4]{8.4}. Use a
	3D generalization of the test problem in Section \hyperref[sec:sec_8_3]{8.3} to test the implementation.
	Filename: approx3D.py
	\bigbreak
	\section*{Exercise 22: Use Simpson's rule and P2 elements}
	\label{sec:sec_10_22}
	\noindent Redo Exercise \hyperref[sec:sec_10_19]{19}, but use P2 elements and Simpson's rule based on sampling
	the integrands at the nodes in the reference cell.
	Filename: fe\textunderscore P2\textunderscore simpson.pdf.





\chapter{Basic principles for approximating differential equations}
	\label{chap:chap_11}
	\pagenumbering{arabic}
	
	\noindent The finite element method is a very flexible approach for solving partial differential equations. Its two most attractive features are the ease of handling domains of complex shape in two and three dimensions and the ease of constructing higherorder discretization methods. The finite element method is usually applied for discretization in space, and therefore spatial problems will be our focus in the coming sections. Extensions to time-dependent problems may, for instance, use finite difference approximations in time.
	
	Before studying how finite element methods are used to tackle differential equation, we first look at how global basis functions and the least squares, Galerkin, and collocation principles can be used to solve differential equations.

\section[Differential equation models]{Differential equation models}
\label{sec:sec_11_1}
	Let us consider an abstract differential equation for a function $u(x)$ of one variable, written as
	
	\begin{equation}
		\label{eqa117}		
		\mathcal{L}(u)=0, \quad x \in \Omega .
	\end{equation}
	
	Here are a few examples on possible choices of $\mathcal{L}(u)$, of increasing complexity:
	
	\begin{equation}
		\label{eqa118}
		\mathcal{L}(u)=\frac{d^{2} u}{d x^{2}}-f(x) \\
	\end{equation}
	
	\begin{equation}
		\label{eqa119}
		\mathcal{L}(u)=\frac{d}{d x}\left(\alpha(x) \frac{d u}{d x}\right)+f(x) \\
	\end{equation}
	
	\begin{equation}
		\label{eqa120}
		\mathcal{L}(u)=\frac{d}{d x}\left(\alpha(u) \frac{d u}{d x}\right)-a u+f(x) \\
	\end{equation}
	
	\begin{equation}
		\label{eqa121}
		\mathcal{L}(u)=\frac{d}{d x}\left(\alpha(u) \frac{d u}{d x}\right)+f(u, x)
	\end{equation}
	
	\noindent Both $\alpha(x)$ and $f(x)$ are considered as specified functions, while $a$ is a prescribed parameter. Differential equations corresponding to (\ref{eqa118})-(\ref{eqa119}) arise in diffusion phenomena, such as steady transport of heat in solids and flow of viscous fluids between flat plates. The form (\ref{eqa120}) arises when transient diffusion or wave phenomenon are discretized in time by finite differences. The equation (\ref{eqa121}) appear in chemical models when diffusion of a substance is combined with chemical reactions. Also in biology, (\ref{eqa121}) plays an important role, both for spreading of species and in models involving generation and propagation of electrical signals.
	
	Let $\Omega=[0, L]$ be the domain in one space dimension. In addition to the differential equation, $u$ must fulfill boundary conditions at the boundaries of the domain, $x=0$ and $x=L$. When $\mathcal{L}$ contains up to second-order derivatives, as in the examples above, $m=1$, we need one boundary condition at each of the (two) boundary points, here abstractly specified as
	
	\begin{equation}
		\label{eqa122}
		\mathcal{B}_{0}(u)=0, x=0, \quad \mathcal{B}_{1}(u)=0, x=L
	\end{equation}
	
	\noindent There are three common choices of boundary conditions:
	
	\begin{equation}
		\label{eqa123}
		\mathcal{B}_{i}(u)  =u-g,  \text { Dirichlet condition } \\
	\end{equation}

	\begin{equation}
		\label{eqa124}
		\mathcal{B}_{i}(u)  =-\alpha \frac{d u}{d x}-g,  \text { Neumann condition } \\
	\end{equation}

	\begin{equation}
		\label{eqa125}
		\mathcal{B}_{i}(u) =-\alpha \frac{d u}{d x}-h(u-g),  \text { Robin condition }
	\end{equation}	

	\noindent Here, $g$ and $a$ are specified quantities.
	
	From now on we shall use $u_{e}(x)$ as symbol for the exact solution, fulfilling
	
	\begin{equation}
		\label{eqa126}
		\mathcal{L}\left(u_{\mathrm{e}}\right)=0, \quad x \in \Omega,
	\end{equation}
	
	\noindent while $u(x)$ is our notation for an approximate solution of the differential equation.
	
	\begin{mybox}
		\textbf{Remark on notation.}
		
		\noindent In the literature about the finite element method, is common to use $u$ as the exact solution and $u_{h}$ as the approximate solution, where $h$ is a discretization parameter. However, the vast part of the present text is about the approximate solutions, and having a subscript $h$ attached all the time is cumbersome. Of equal importance is the close correspondence between implementation and mathematics that we strive to achieve in this text: when it is natural to use \textbf{\texttt{u}} and not \textbf{\texttt{u\_h}} in code, we let the mathematical notation be dictated by the code's preferred notation. After all, it is the powerful computer implementations of the finite element method that justifies studying the mathematical formulation and aspects of the method.	
	\end{mybox}
	
	\section[Basic principles for approximating differential equations]{Simple model problems}
	\label{sec:sec_11_2}
	\noindent A common model problem used much in the forthcoming examples is
	
	\begin{equation}
		\label{eqa127}
		-u^{\prime \prime}(x)=f(x), \quad x \in \Omega=[0, L], \quad u(0)=0, u(L)=D .
	\end{equation}

	\noindent A closely related problem with a different boundary condition at $x=0$ reads
	
	\begin{equation}
		\label{eqa128}
		-u^{\prime \prime}(x)=f(x), \quad x \in \Omega=[0, L], \quad u^{\prime}(0)=C, u(L)=D .
	\end{equation}

	\noindent A third variant has a variable coefficient,
	
	\begin{equation}
		\label{eqa129}
		-\left(\alpha(x) u^{\prime}(x)\right)^{\prime}=f(x), \quad x \in \Omega=[0, L], \quad u^{\prime}(0)=C, u(L)=D .
	\end{equation}

	We can easily solve these using sympy. For (\ref{eqa127}) we can write the function
	
	\begin{lstlisting}[numbers=none]
		def model1(f, L, D):
		"""Solve -u'' = f(x), u(0)=0, u(L)=D."""
		u_x = - sp.integrate(f, (x, 0, x)) + c_0
		u = sp.integrate(u_x, (x, 0, x)) + c_1
		r = sp.solve([u.subs(x, 0)-0, u.subs(x,L)-D], [c_0, c_1])
		u = u.subs(c_0, r[c_0]).subs(c_1, r[c_1])
		u = sp.simplify(sp.expand(u))
		return u
	\end{lstlisting}
	
	\noindent Calling \textbf{model1(2, L, D)} results in the solution
	
	\begin{equation}
		\label{eqa130}
		u(x)=\frac{1}{L} x\left(D+L^{2}-L x\right)
	\end{equation}

	\noindent Model (\ref{eqa128}) can be solved by
	
	\begin{lstlisting}[numbers=none]
		def model2(f, L, C, D):
		"""Solve -u'' = f(x), u'(0)=C, u(L)=D."""
		u_x = - sp.integrate(f, (x, 0, x)) + c_0
		u = sp.integrate(u_x, (x, 0, x)) + c_1
		r = sp.solve([sp.diff(u,x).subs(x, 0)-C, u.subs(x,L)-D], [c_0, c_1])
		u = u.subs(c_0, r[c_0]).subs(c_1, r[c_1])
		u = sp.simplify(sp.expand(u))
		return u
	\end{lstlisting}
	
	\noindent to yield
	
	\begin{equation}
		\label{eqa131}
		u(x)=-x^{2}+C x-C L+D+L^{2},
	\end{equation}

	\noindent if $f(x)=2$. Model (\ref{eqa129}) requires a bit more involved code,
	
	
	\begin{lstlisting}[numbers=none]
		def model3(f, a, L, C, D):
		"""Solve -(a*u')' = f(x), u(0)=C, u(L)=D."""
		au_x = - sp.integrate(f, (x, 0, x)) + c_0
		u = sp.integrate(au_x/a, (x, 0, x)) + c_1
		r = sp.solve([u.subs(x, 0)-C, u.subs(x,L)-D], [c_0, c_1])
		u = u.subs(c_0, r[c_0]).subs(c_1, r[c_1])
		u = sp.simplify(sp.expand(u))
		return u
	\end{lstlisting}
	
	\noindent With $f(x)=0$ and $\alpha(x)=1+x^{2}$ we get
	
	$$u(x)=\frac{C \operatorname{atan}(L)-C \operatorname{atan}(x)+D \operatorname{atan}(x)}{\operatorname{atan}(L)}$$
	
\section[Forming the residual]{Forming the residual} 
	\label{sec:sec_11_3}
	\noindent The fundamental idea is to seek an approximate solution $u$ in some space $V$,

	$$
	V=\operatorname{span}\left\{\psi_{0}(x), \ldots, \psi_{N}(x)\right\},
	$$
	
	\noindent which means that $u$ can always be expressed as a linear combination of the basis functions $\left\{\varphi_{i}\right\}_{i \in \mathcal{I}_{s}}$, with $\mathcal{I}_{s}$ as the index set $\{0, \ldots, N\}$ :
	$$
	u(x)=\sum_{j \in \mathcal{I}_{s}} c_{j} \psi_{j}(x)
	$$
	The coefficients $\left\{c_{i}\right\}_{i \in \mathcal{I}_{s}}$ are unknowns to be computed.
	(Later, in Section 14, we will see that if we specify boundary values of $u$ different from zero, we must look for an approximate solution $u(x)=B(x)+$ $\sum_{j} c_{j} \psi_{j}(x)$, where $\sum_{j} c_{j} \psi_{j} \in V$ and $B(x)$ is some function for incorporating the right boundary values. Because of $B(x), u$ will not necessarily lie in $V$. This modification does not imply any difficulties.)
	
	We need principles for deriving $N+1$ equations to determine the $N+1$ unknowns $\left\{c_{i}\right\}_{i \in \mathcal{I}_{s}}$. When approximating a given function $f$ by $u=\sum_{j} c_{j} \varphi_{j}, \mathrm{a}$ key idea is to minimize the square norm of the approximation error $e=u-f$ or (equvalently) demand that $e$ is orthogonal to $V$. Working with $e$ is not so useful here since the approximation error in our case is $e=u_{e}-u$ and $u_{\mathrm{e}}$ is unknown. The only general indicator we have on the quality of the approximate solution is to what degree $u$ fulfills the differential equation. Inserting $u=\sum_{j} c_{j} \psi_{j}$ into $\mathcal{L}(u)$ reveals that the result is not zero, because $u$ is only likely to equal $u_{\mathrm{e}}$. The nonzero result,

	\begin{equation}
		\label{eqa132}
		R=\mathcal{L}(u)=\mathcal{L}\left(\sum_{j} c_{j} \psi_{j}\right),
	\end{equation}

	\noindent is called the residual and measures the error in fulfilling the governing equation.
	Various principles for determining $\left\{c_{i}\right\}_{i \in \mathcal{I}_{\text {s }}}$ try to minimize $R$ in some sense. Note that $R$ varies with $x$ and the $\left\{c_{i}\right\}_{i \in \mathcal{I}_{s}}$ parameters. We may write this dependence explicitly as
	
	\begin{equation}
		\label{eqa133}
		R=R\left(x ; c_{0}, \ldots, c_{N}\right)
	\end{equation}

	\noindent Below, we present three principles for making $R$ small: a least squares method, a projection or Galerkin method, and a collocation or interpolation method.
	
\section[The least squares method]{The least squares method} 
	\label{sec:sec_11_4}

	\noindent The least-squares method aims to find $\left\{c_{i}\right\}_{i \in \mathcal{I}_{s}}$ such that the square norm of the residual
	
	\begin{equation}
		\label{eqa134}
		\|R\|=(R, R)=\int_{\Omega} R^{2} \mathrm{~d} x
	\end{equation}

	\noindent is minimized. By introducing an inner product of two functions $f$ and $g$ on $\Omega$ as
	
	\begin{equation}
		\label{eqa135}
		(f, g)=\int_{\Omega} f(x) g(x) \mathrm{d} x,
	\end{equation}

	\noindent the least-squares method can be defined as
	
	\begin{equation}
		\label{eqa136}
	\min _{c_{0}, \ldots, c_{N}} E=(R, R) .
	\end{equation}

	\noindent Differentiating with respect to the free parameters $\left\{c_{i}\right\}_{i \in \mathcal{I}_{s}}$ gives the $N+1$ equations

	\begin{equation}
		\label{eqa137}
		\int_{\Omega} 2 R \frac{\partial R}{\partial c_{i}} \mathrm{~d} x=0 \quad \Leftrightarrow \quad\left(R, \frac{\partial R}{\partial c_{i}}\right)=0, \quad i \in \mathcal{I}_{s}
	\end{equation}

	\section[The Galerkin method]{The Galerkin method} 
	\label{sec:sec_11_5}
	
	\noindent The least-squares principle is equivalent to demanding the error to be orthogonal to the space $V$ when approximating a function $f$ by $u \in V$. With a differential equation we do not know the true error so we must instead require the residual $R$ to be orthogonal to $V$. This idea implies seeking $\left\{c_{i}\right\}_{i \in \mathcal{I}_{s}}$ such that
	
	\begin{equation}
		\label{eqa138}
		(R, v)=0, \quad \forall v \in V .
	\end{equation}

	\noindent This is the Galerkin method for differential equations.
	
	This statement is equivalent to $R$ being orthogonal to the $N+1$ basis functions only:

	\begin{equation}
		\label{eqa139}
		\left(R, \psi_{i}\right)=0, \quad i \in \mathcal{I}_{s},
	\end{equation}

	resulting in $N+1$ equations for determining $\left\{c_{i}\right\}_{i \in \mathcal{I}_{s}}$.

\section[Basic principles for approximating differential equations]{The Method of Weighted Residuals}
	\label{sec:sec_11_6} 
	\noindent A generalization of the Galerkin method is to demand that $R$ is orthogonal to some space $W$, but not necessarily the same space as $V$ where we seek the unknown function. This generalization is naturally called the \emph{method of weighted residuals}:

	\begin{equation}
		\label{eqa140}
		(R, v)=0, \quad \forall v \in W .
	\end{equation}
	
	\noindent If $\left\{w_{0}, \ldots, w_{N}\right\}$ is a basis for $W$, we can equivalently express the method of weighted residuals as
	
	\begin{equation}
		\label{eqa141}
		\left(R, w_{i}\right)=0, \quad i \in \mathcal{I}_{s} .
	\end{equation}

	\noindent The result is $N+1$ equations for $\left\{c_{i}\right\}_{i \in \mathcal{I}_{s}}$.

	The least-squares method can also be viewed as a weighted residual method with $w_{i}=\partial R / \partial c_{i}$.
	
	\begin{mybox}
		\textbf{Variational formulation of the continuous problem.}
		
		\noindent Formulations like (\ref{eqa140}) (or (\ref{eqa141})) and (\ref{eqa138}) (or (\ref{eqa139})) are known as \emph{variational formulations}. These equations are in this text primarily used for a numerical approximation $u \in V$, where $V$ is a \emph{finite-dimensional} space with dimension $N+1$. However, we may also let $V$ be an \emph{infinite-dimensional} space containing the exact solution $u_{e}(x)$ such that also $u_{e}$ fulfills the same variational formulation. The variational formulation is in that case a mathematical way of stating the problem and acts as alternative to the usual formulation of a differential equation with initial and/or boundary conditions.
	\end{mybox}
	
	
\section[Test and Trial Functions]{Test and Trial Functions} 
	\label{sec:sec_11_7}
	\noindent In the context of the Galerkin method and the method of weighted residuals it is common to use the name \emph{trial function} for the approximate $u=\sum_{j} c_{j} \psi_{j}$. The space containing the trial function is known as the trial space. The function $v$ entering the orthogonality requirement in the Galerkin method and the method of weighted residuals is called \emph{test function}, and so are the $\psi_{i}$ or $w_{i}$ functions that are used as weights in the inner products with the residual. The space where the test functions comes from is naturally called the \emph{test space}.
	
	We see that in the method of weighted residuals the test and trial spaces are different and so are the test and trial functions. In the Galerkin method the test and trial spaces are the same (so far).
	
	\begin{mybox}
		\textbf{Remark.}
		
		\noindent It may be subject to debate whether it is only the form of (\ref{eqa140}) or (\ref{eqa138}) after integration by parts, as explained in Section \ref{sec:sec_11_10}, that qualifies for the term variational formulation. The result after integration by parts is what is obtained after taking the \emph{first variation} of an optimization problem, see Section \ref{sec:sec_11_13} However, here we use variational formulation as a common term for formulations which, in contrast to the differential equation $R=0$, instead demand that an average of $R$ is zero: $(R, v)=0$ for all $v$ in some space.
	\end{mybox}
	
\section[The collocation]{The collocation} 
	\label{sec:sec_11_8}
	\noindent The idea of the collocation method is to demand that $R$ vanishes at $N+1$ selected points $x_{0}, \ldots, x_{N}$ in $\Omega$ :
	
	\begin{equation}
		\label{eqa142}
		R\left(x_{i} ; c_{0}, \ldots, c_{N}\right)=0, \quad i \in \mathcal{I}_{s} .
	\end{equation}

	\noindent The collocation method can also be viewed as a method of weighted residuals with Dirac delta functions as weighting functions. Let $\delta\left(x-x_{i}\right)$ be the Dirac delta function centered around $x=x_{i}$ with the properties that $\delta\left(x-x_{i}\right)=0$ for $x \neq x_{i}$ and
	
	\begin{equation}
		\label{eqa143}
		\int_{\Omega} f(x) \delta\left(x-x_{i}\right) \mathrm{d} x=f\left(x_{i}\right), \quad x_{i} \in \Omega
	\end{equation}

	\noindent Intuitively, we may think of $\delta\left(x-x_{i}\right)$ as a very peak-shaped function around $x=x_{i}$ with integral 1, roughly visualized in Figure \ref{fig:img_46} . Because of (\ref{eqa143}), we can let $w_{i}=\delta\left(x-x_{i}\right)$ be weighting functions in the method of weighted residuals, and (\ref{eqa141}) becomes equivalent to (\ref{eqa142}).
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{img_46}
		\caption{Approximation of delta functions by narrow Gaussian functions.}
		\label{fig:img_46}
	\end{figure}
	
	\noindent \textbf{The subdomain collocation method.} The idea of this approach is to demand the integral of $R$ to vanish over $N+1$ subdomains $\Omega_{i}$ of $\Omega$ :
	
	\begin{equation}
		\label{eqa144}
		\int_{\Omega_{i}} R \mathrm{~d} x=0, \quad i \in \mathcal{I}_{s}
	\end{equation}

	\noindent This statement can also be expressed as a weighted residual method

	\begin{equation}
		\label{eqa145}
		\int_{\Omega} R w_{i} \mathrm{~d} x=0, \quad i \in \mathcal{I}_{s},
	\end{equation}

	\noindent where $w_{i}=1$ for $x \in \Omega_{i}$ and $w_{i}=0$ otherwise.

\section[Examples on using the principles]{Examples on using the principles} 
	\label{sec:sec_11_9}
	\noindent Let us now apply global basis functions to illustrate the principles for minimizing $R$.
	
	\noindent \textbf{The model problem.    } We consider the differential equation problem
	
	\begin{equation}
		\label{eqa146}
		-u^{\prime \prime}(x)=f(x), \quad x \in \Omega=[0, L], \quad u(0)=0, u(L)=0
	\end{equation}
	
	\noindent \textbf{Basis functions.   } Our choice of basis functions $\psi_{i}$ for $V$ is
	
	\begin{equation}
		\label{eqa147}
		\psi_{i}(x)=\sin \left((i+1) \pi \frac{x}{L}\right), \quad i \in \mathcal{I}_{s} .
	\end{equation}

	An important property of these functions is that $\psi_{i}(0)=\psi_{i}(L)=0$, which means that the boundary conditions on $u$ are fulfilled:
	
	$$ u(0)=\sum_{j} c_{j} \psi_{j}(0)=0, \quad u(L)=\sum_{j} c_{j} \psi_{j}(L)=0. $$
	
	\noindent Another nice property is that the chosen sine functions are orthogonal on $\Omega$ :
	
	\begin{equation}
		\label{eqa148}
		\int_{0}^{L} \sin \left((i+1) \pi \frac{x}{L}\right) \sin \left((j+1) \pi \frac{x}{L}\right) 	\mathrm{d} x= \begin{cases}\frac{1}{2} L & i=j \\ 0, & i \neq j\end{cases}
	\end{equation}

	\noindent provided $i$ and $j$ are integers.
	
	\noindent\textbf{The residual.   } We can readily calculate the following explicit expression for the residual:
	
	\begin{equation}
		\label{eqa149}
		R\left(x ; c_{0}, \ldots, c_{N}\right) =u^{\prime \prime}(x)+f(x), \\
		=\frac{d^{2}}{d x^{2}}\left(\sum_{j \in \mathcal{I}_{s}} c_{j} \psi_{j}(x)\right)+f(x), \\
		=\sum_{j \in \mathcal{I}_{s}} c_{j} \psi_{j}^{\prime \prime}(x)+f(x) .
	\end{equation}
	
	\noindent \textbf{The least squares method.   } The equations (\ref{eqa137}) in the least squares method require an expression for $\partial R / \partial c_{i}$. We have
	
	\begin{equation}
		\label{eqa150}
		\frac{\partial R}{\partial c_{i}}=\frac{\partial}{\partial c_{i}}\left(\sum_{j \in 	\mathcal{I}_{s}} c_{j} \psi_{j}^{\prime \prime}(x)+f(x)\right)=\sum_{j \in \mathcal{I}_{s}} \frac{\partial c_{j}}{\partial c_{i}} \psi_{j}^{\prime \prime}(x)=\psi_{i}^{\prime \prime}(x) .
	\end{equation}

	\noindent The governing equations for $\left\{c_{i}\right\}_{i \in \mathcal{I}_{s}}$ are then
	
	\begin{equation}
		\label{eqa151}
		\left(\sum_{j} c_{j} \psi_{j}^{\prime \prime}+f, \psi_{i}^{\prime \prime}\right)=0, \quad i \in \mathcal{I}_{s},
	\end{equation}
	
	\noindent which can be rearranged as
	
	\begin{equation}
		\label{eqa152}
		\sum_{j \in \mathcal{I}_{s}}\left(\psi_{i}^{\prime \prime}, \psi_{j}^{\prime \prime}\right) 	c_{j}=-\left(f, \psi_{i}^{\prime \prime}\right), \quad i \in \mathcal{I}_{s} .
	\end{equation}

	\noindent This is nothing but a linear system
	$$\sum_{j \in \mathcal{I}_{s}} A_{i, j} c_{j}=b_{i}, \quad i \in \mathcal{I}_{s},$$
	with
	
	\begin{equation}
		\label{eqa153}
		\begin{aligned}
			A_{i, j} &=\left(\psi_{i}^{\prime \prime}, \psi_{j}^{\prime \prime}\right) \\
			&=\pi^{4}(i+1)^{2}(j+1)^{2} L^{-4} \int_{0}^{L} \sin \left((i+1) \pi \frac{x}{L}\right) \sin \left((j+1) \pi \frac{x}{L}\right) \mathrm{d} x \\
			&= \begin{cases}\frac{1}{2} L^{-3} \pi^{4}(i+1)^{4} & i=j \\
				0, & i \neq j\end{cases} \\
		\end{aligned}
	\end{equation}

	\begin{equation}
		\label{eqa154}
		b_{i} =-\left(f, \psi_{i}^{\prime \prime}\right)=(i+1)^{2} \pi^{2} L^{-2} \int_{0}^{L} f(x) \sin \left((i+1) \pi \frac{x}{L}\right) \mathrm{d} x
	\end{equation}

	\noindent Since the coefficient matrix is diagonal we can easily solve for
	
	\begin{equation}
		\label{eqa155}
		c_{i}=\frac{2 L}{\pi^{2}(i+1)^{2}} \int_{0}^{L} f(x) \sin \left((i+1) \pi \frac{x}{L}\right) 	\mathrm{d} x .
	\end{equation}

	\noindent With the special choice of $f(x)=2$ can be calculated in \textbf{\texttt{sympy}} by
	
	\begin{lstlisting}[numbers=none]
		from sympy import *
		import sys
		i, j = symbols('i j', integer=True)
		x, L = symbols('x L')
		f = 2
		a = 2*L/(pi**2*(i+1)**2)
		c_i = a*integrate(f*sin((i+1)*pi*x/L), (x, 0, L))
		c_i = simplify(c_i)
		print c_i
	\end{lstlisting}
	
	\noindent The answer becomes
	$$ c_{i}=4 \frac{L^{2}\left((-1)^{i}+1\right)}{\pi^{3}\left(i^{3}+3 i^{2}+3 i+1\right)}$$
	
	\noindent Now, $1+(-1)^{i}=0$ for $i$ odd, so only the coefficients with even index are nonzero. Introducing $i=2 k$ for $k=0, \ldots, N / 2$ to count the relevant indices (for $N$ odd, $k$ goes to $(N-1) / 2)$, we get the solution 

	\begin{equation}
		\label{eqa156}
		u(x)=\sum_{k=0}^{N / 2} \frac{8 L^{2}}{\pi^{3}(2 k+1)^{3}} \sin \left((2 k+1) \pi 	\frac{x}{L}\right)
	\end{equation}

	\noindent The coefficients decay very fast: $c_{2}=c_{0} / 27, c_{4}=c_{0} / 125$. The solution will therefore be dominated by the first term,

	$$ u(x) \approx \frac{8 L^{2}}{\pi^{3}} \sin \left(\pi \frac{x}{L}\right) $$

	\noindent\textbf{The Galerkin method.   } The Galerkin principle (\ref{eqa138}) applied to (\ref{eqa146}) consists of inserting our special residual (\ref{eqa149}) in (\ref{eqa138})

	$$ \left(u^{\prime \prime}+f, v\right)=0, \quad \forall v \in V, $$
	
	\noindent or
	
	\begin{equation}
		\label{eqa157}
		\left(u^{\prime \prime}, v\right)=-(f, v), \quad \forall v \in V .
	\end{equation}\

	\noindent This is the variational formulation, based on the Galerkin principle, of our differential equation. The $\forall v \in V$ requirement is equivalent to demanding the equation $\left(u^{\prime \prime}, v\right)=-(f, v)$ to be fulfilled for all basis functions $v=\psi_{i}, i \in \mathcal{I}_{s}$, see (\ref{eqa138}) and (\ref{eqa139}). We therefore have
	
	\begin{equation}
		\label{eqa158}
		\left(\sum_{j \in \mathcal{I}_{s}} c_{j} \psi_{j}^{\prime \prime}, \psi_{i}\right)=-\left(f, \psi_{i}\right), \quad i \in \mathcal{I}_{s} .
	\end{equation}

	\noindent This equation can be rearranged to a form that explicitly shows that we get a linear system for the unknowns $\left\{c_{i}\right\}_{i \in \mathcal{I}_{s}}$ :

	\begin{equation}
		\label{eqa159}
		\sum_{j \in \mathcal{I}_{s}}\left(\psi_{i}, \psi_{j}^{\prime \prime}\right) c_{j}=\left(f, 	\psi_{i}\right), \quad i \in \mathcal{I}_{s} .
	\end{equation}
	
	\noindent For the particular choice of the basis functions (\ref{eqa147}) we get in fact the same linear system as in the least squares method because $\psi^{\prime \prime}=-(i+1)^{2} \pi^{2} L^{-2} \psi$.\bigbreak
	
	\noindent \textbf{The collocation method.   } For the collocation method (\ref{eqa142}) we need to decide upon a set of $N+1$ collocation points in $\Omega$. A simple choice is to use uniformly spaced points: $x_{i}=i \Delta x$, where $\Delta x=L / N$ in our case $(N \geq 1)$. However, these points lead to at least two rows in the matrix consisting of zeros (since $\psi_{i}\left(x_{0}\right)=0$ and $\psi_{i}\left(x_{N}\right)=0$ ), thereby making the matrix singular and non-invertible. This forces us to choose some other collocation points, e.g., random points or points uniformly distributed in the interior of $\Omega$. Demanding the residual to vanish at these points leads, in our model problem (\ref{eqa146}), to the equations
		
	\begin{equation}
		\label{eqa160}
		-\sum_{j \in \mathcal{I}_{s}} c_{j} \psi_{j}^{\prime \prime}\left(x_{i}\right)=f\left(x_{i}\right), \quad i \in \mathcal{I}_{s},
	\end{equation}

	\noindent which is seen to be a linear system with entries
	$$ A_{i, j}=-\psi_{j}^{\prime \prime}\left(x_{i}\right)=(j+1)^{2} \pi^{2} L^{-2} \sin \left((j+1) \pi \frac{x_{i}}{L}\right) $$
	in the coefficient matrix and entries $b_{i}=2$ for the right-hand side (when $f(x)=2)$.
	
	The special case of $N=0$ can sometimes be of interest. A natural choice is then the midpoint $x_{0}=L / 2$ of the domain, resulting in $A_{0,0}=-\psi_{0}^{\prime \prime}\left(x_{0}\right)=\pi^{2} L^{-2}$, $f\left(x_{0}\right)=2$, and hence $c_{0}=2 L^{2} / \pi^{2} .$ \bigbreak
	
	\noindent \textbf{Comparison.} In the present model problem, with $f(x)=2$, the exact solution is $u(x)=x(L-x)$, while for $N=0$ the Galerkin and least squares method result in $u(x)=8 L^{2} \pi^{-3} \sin (\pi x / L)$ and the collocation method leads to $u(x)=$ $2 L^{2} \pi^{-2} \sin (\pi x / L)$. Since all methods fulfill the boundary conditions $u(0)=$ $u(L)=0$, we expect the largest discrepancy to occur at the midpoint of the domain: $x=L / 2$. The error at the midpoint becomes $-0.008 L^{2}$ for the Galerkin and least squares method, and $0.047 L^{2}$ for the collocation method. \bigbreak 

\section[Integration by parts]{Integration by parts} 
	\label{sec:sec_11_10}
	\noindent A problem arises if we want to apply popular finite element functions to solve our model problem (\ref{eqa146}) by the standard least squares, Galerkin, or collocation methods: the piecewise polynomials $\psi_{i}(x)$ have discontinuous derivatives at the cell boundaries which makes it problematic to compute the second-order derivative. This fact actually makes the least squares and collocation methods less suitable for finite element approximation of the unknown function. (By rewriting the equation $-u^{\prime \prime}=f$ as a system of two first-order equations, $u^{\prime}=v$ and $-v^{\prime}=$ $f$, the least squares method can be applied. Also, differentiating discontinuous functions can actually be handled by distribution theory in mathematics.) The Galerkin method and the method of weighted residuals can, however, be applied together with finite element basis functions if we use \emph{integration by parts} as a means for transforming a second-order derivative to a first-order one.
	
	Consider the model problem (\ref{eqa146}) and its Galerkin formulation
	$$ -\left(u^{\prime \prime}, v\right)=(f, v) \quad \forall v \in V . $$
	
	\noindent Using integration by parts in the Galerkin method, we can move a derivative of $u$ onto $v$ :
	
	\begin{equation}
		\label{eqa161}
		\int_{0}^{L} u^{\prime \prime}(x) v(x) \mathrm{d} x =-\int_{0}^{L} u^{\prime}(x) v^{\prime}(x) \mathrm{d} x+\left[v u^{\prime}\right]_{0}^{L} \\
		=-\int_{0}^{L} u^{\prime}(x) v^{\prime}(x) \mathrm{d} x+u^{\prime}(L) v(L)-u^{\prime}(0) v(0) .
	\end{equation}
	
	\noindent Usually, one integrates the problem at the stage where the $u$ and $v$ functions enter the formulation. Alternatively, but less common, we can integrate by parts in the expressions for the matrix entries:
	=
	\begin{equation}
		\label{eqa162}
		\int_{0}^{L} \psi_{i}(x) \psi_{j}^{\prime \prime}(x) \mathrm{d} x =-\int_{0}^{L} \psi_{i}^{\prime}(x) \psi_{j}^{\prime}(x) d x+\left[\psi_{i} \psi_{j}^{\prime}\right]_{0}^{L} \\
		=-\int_{0}^{L} \psi_{i}^{\prime}(x) \psi_{j}^{\prime}(x) \mathrm{d} x+\psi_{i}(L) \psi_{j}^{\prime}(L)-\psi_{i}(0) \psi_{j}^{\prime}(0) .
	\end{equation}

	\noindent Integration by parts serves to reduce the order of the derivatives and to make the coefficient matrix symmetric since $\left(\psi_{i}^{\prime}, \psi_{j}^{\prime}\right)=\left(\psi_{i}^{\prime}, \psi_{j}^{\prime}\right)$. The symmetry property depends on the type of terms that enter the differential equation. As will be seen later in Section \ref{chap:chap_15}, integration by parts also provides a method for implementing boundary conditions involving $u^{\prime}$.
	
	With the choice (\ref{eqa147}) of basis functions we see that the "boundary terms" $\psi_{i}(L) \psi_{j}^{\prime}(L)$ and $\psi_{i}(0) \psi_{j}^{\prime}(0)$ vanish since $\psi_{i}(0)=\psi_{i}(L)=0$.
	
	\noindent \textbf{Weak form.   } Since the variational formulation after integration by parts make weaker demands on the differentiability of $u$ and the basis functions $\psi_{i}$, the resulting integral formulation is referred to as a \emph{weak form} of the differential equation problem. The original variational formulation with second-order derivatives, or the differential equation problem with second-order derivative, is then the \emph{strong form}, with stronger requirements on the differentiability of the functions.
	
	For differential equations with second-order derivatives, expressed as variational formulations and solved by finite element methods, we will always perform integration by parts to arrive at expressions involving only first-order derivatives.

\section[Boundary function]{Boundary function} 
	\label{sec:sec_11_11}
	\noindent So far we have assumed zero Dirichlet boundary conditions, typically $u(0)=$ $u(L)=0$, and we have demanded that $\psi_{r}(0)=\psi_{i}(L)=0$ for $i \in \mathcal{I}_{s}$. What about a boundary condition like $u(L)=D \neq 0$ ? This condition immediately faces a problem: $u=\sum_{j} c_{j} \varphi_{j}(L)=0$ since all $\varphi_{i}(L)=0$.
	
	A boundary condition of the form $u(L)=D$ can be implemented by demanding that all $\psi_{n}(L)=0$, but adding a boundary function $B(x)$ with the right boundary value, $B(L)=D$, to the expansion for $u$ :
	
	$$
	u(x)=B(x)+\sum_{j \in \mathcal{I}_{s}} c_{j} \psi_{j}(x) .
	$$
	This $u$ gets the right value at $x=L$ :
	$$
	u(L)=B(L)+\sum_{j \in I_{a}} c_{j} \psi_{j}(L)=B(L)=D .
	$$
	The idea is that for any boundary where $u$ is known we demand $\psi_{y}$ to vanish and construct a function $B(x)$ to attain the boundary value of $u$. There are no restrictions how $B(x)$ varies with $x$ in the interior of the domain, so this variation needs to be constructed in some way.
	
	For example, with $u(0)=0$ and $u(L)=D$, we can choose $B(x)=x D / L$, since this form ensures that $B(x)$ fulfills the boundary conditions: $B(0)=0$ and $B(L)=D$. The unknown function is then sought on the form
	
	\begin{equation}
	\label{eqa163}
		u(x)=\frac{x}{L} D+\sum_{j \in \mathcal{I}_{*}} c_{j} \psi_{j}(x)
	\end{equation}

	\noindent with $\psi_{i}(0)=\psi_{1}(L)=0$. 
	
	The $B(x)$ function can be chosen in many ways as long as its boundary values are correct. For example, $B(x)=D(x / L)^{p}$ for any power $p$ will work fine in the above example.
	
	As another example, consider a domain $\Omega=[a, b]$ where the boundary conditions are $u(a)=U_{a}$ and $u(b)=U_{b}$. A class of possible $B(x)$ functions is
	
	\begin{equation}
	\label{eqa164}
		B(x)=U_{a}+\frac{U_{b}-U_{a}}{(b-a)^{\mathrm{p}}}(x-a)^{\mathrm{p}}, \quad p>0 .
	\end{equation}

	\noindent Real applications will most likely use the simplest version, $p=1$, but here such a $p$ parameter was included to demonstrate the ambiguity in the construction of $B(x)$.
	
	\begin{mybox}
		\textbf{Summary.}
		
		\noindent The general procedure of incorporating Dirichlet boundary conditions goes as follows. Let $O \Omega_{E}$ be the part(s) of the boundary OS of the domain $\Omega$ where $u$ is specified. Set $\psi_{u}=0$ at the points in OOR and seek $u$ as
		
		\begin{equation}
		\label{eqa165}
			u(x)=B(x)+\sum_{j \in \mathcal{I}_{s}} c_{y} \psi_{j}(x)
		\end{equation}
			
		where $B(x)$ equals the boundary conditions on $u$ at $\partial \Omega_{E}$.
	\end{mybox}
	

	\noindent \textbf{Remark.   } With the $B(x)$ term, $u$ does not in general lie in $V=\operatorname{span}\left\{\psi_{0}, \ldots, \psi_{N}\right\}$ anymore. Moreover, when a prescribed value of $u$ at the boundary, say u( $a)=U_{a}$ is different from zero, it does not make sense to say that $u$ lies in a vector space, because this space does not obey the requirements of addition and scalar multiplication. For example, $2 u$ does not lie in the space since its boundary value is $2 U_{a}$, which is incorrect. It only makes sense to split $u$ in two parts, as done above, and have the unknown part $\sum_{j} c_{j} \psi_{j}$ in a proper function space.

\section[Abstract notatiom for variational formulations]{Abstract notatiom for variational formulations} 
	\label{sec:sec_11_12}
	\noindent We have seen that variational formulations end up with a formula involving $u$ and $v$, such as $\left(u^{\prime}, v^{\prime}\right)$ and a formula involving $v$ and known functions, such as $(f, v)$. A widely used notation is to introduce an abstract variational statement written as $a(u, v)=L(v)$, where $a(u, v)$ is a so-called \emph{bilinear} form involving all the terms that contain both the test and trial function, while $L(v)$ is a \emph{linear} form containing all the terms without the trial function. For example, the statement
	$$
	\int_{\Omega} u^{\prime} v^{\prime} \mathrm{d} x=\int_{\Omega} f v \mathrm{~d} x \quad \text { or } \quad\left(u^{\prime}, v^{\prime}\right)=(f, v) \quad \forall v \in V
	$$
	can be written in abstract form: find $u$ such that
	$$
	a(u, v)=L(v) \quad \forall v \in V
	$$
	where we have the definitions
	$$
	a(u, v)=\left(u^{\prime}, v^{\prime}\right), \quad L(v)=(f, v) \text {. }
	$$
	
	The term \emph{linear} means that $L\left(\alpha_{1} v_{1}+\alpha_{2} v_{2}\right)=\alpha_{1} L\left(v_{1}\right)+\alpha_{2} L\left(v_{2}\right)$ for two test functions $v_{1}$ and $v_{2}$, and scalar parameters $\alpha_{1}$ and $\alpha_{2}$. Similarly, the term \emph{bilinear} means that $a(u, v)$ is linear in both its arguments:
	$$
	\begin{aligned}
		&a\left(\alpha_{1} u_{1}+\alpha_{2} u_{2}, v\right)=\alpha_{1} a\left(u_{1}, v\right)+\alpha_{2} a\left(u_{2}, v\right) \\
		&a\left(u, \alpha_{1} v_{1}+\alpha_{2} v_{2}\right)=\alpha_{1} a\left(u, v_{1}\right)+\alpha_{2} a\left(u, v_{2}\right)
	\end{aligned}
	$$
	In nonlinear problems these linearity properties do not hold in general and the abstract notation is then $F(u ; v)=0$.
	
	The matrix system associated with $a(u, v)=L(v)$ can also be written in an abstract form by inserting $v=\psi_{i}$ and $u=\sum_{j} c_{j} \psi_{j}$ in $a(u, v)=L(v)$. Using the linear properties, we get
	$$
	\sum_{j \in \mathcal{I}_{s}} a\left(\psi_{j}, \psi_{i}\right) c_{j}=L\left(\psi_{i}\right), \quad i \in \mathcal{I}_{s}
	$$
	which is a linear system
	$$
	\sum_{j \in \mathcal{I}_{s}} A_{i, j} c_{j}=b_{i}, \quad i \in \mathcal{I}_{s}
	$$
	where
	$$
	A_{i, j}=a\left(\psi_{j}, \psi_{i}\right), \quad b_{i}=L\left(\psi_{i}\right)
	$$
	
	\noindent In many problems, $a(u, v)$ is symmetric such that $a\left(\psi_{j}, \psi_{i}\right)=a\left(\psi_{i}, \psi_{j}\right)$. In those cases the coefficient matrix becomes symmetric, $A_{i, j}=A_{j, i}$, a property that can simplify solution algorithms for linear systems and make them more stable in addition to saving memory and computations.
	
	The abstract notation $a(u, v)=L(v)$ for linear differential equation problems is much used in the literature and in description of finite element software (in particular the FEniCS documentation). We shall frequently summarize variational forms using this notation.
	
\section[Basic principles for approximating differential equations]{Variational problems and optimization} 
	\label{sec:sec_11_13}
	
	
	\noindent If $a(u, v)=a(v, u)$, it can be shown that the variational statement
	$$
	a(u, v)=L(v) \quad \forall v \in V,
	$$
	is equivalent to minimizing the functional
	$$
	F(v)=\frac{1}{2} a(v, v)-L(v)
	$$
	over all functions $v \in V$. That is,
	$$
	F(u) \leq F(v) \quad \forall v \in V .
	$$
	Inserting a $v=\sum_{j} c_{j} \psi_{j}$ turns minimization of $F(v)$ into minimization of a quadratic function
	$$
	\bar{F}\left(c_{0}, \ldots, c_{N}\right)=\sum_{j \in \mathcal{I}_{s}} \sum_{i \in \mathcal{I}_{\bar{s}}} a\left(\psi_{i}, \psi_{j}\right) c_{i} c_{j}-\sum_{j \in \mathcal{I}_{s}} L\left(\psi_{j}\right) c_{j}
	$$
	of $N+1$ parameters.
	Minimization of $\bar{F}$ implies
	$$
	\frac{\partial \bar{F}}{\partial c_{i}}=0, \quad i \in \mathcal{I}_{s} .
	$$
	After some algebra one finds
	$$
	\sum j \in \mathcal{I}_{s} a\left(\psi_{i}, \psi_{j}\right) c_{j}=L\left(\psi_{i}\right), \quad i \in \mathcal{I}_{s}
	$$
	which is the same system as that arising from $a(u, v)=L(v)$.\bigbreak 
	Many traditional applications of the finite element method, especially in solid mechanics and structural analysis, start with formulating $F(v)$ from physical principles, such as minimization of energy, and then proceeds with deriving $a(u, v)=L(v)$, which is the equation usually desired in implementations.

\chapter{Examples on variational formulations}
\label{chap:chap_12}
\pagenumbering{arabic}

\section[Variable coefficient]{Variable coefficient}
	\label{sec:sec_12_1}
		\noindent Consider the problem
		\begin{equation}
		\label{eqa166}
			-\frac{d}{d x}\left(\alpha(x) \frac{d u}{d x}\right)=f(x), \quad x \in \Omega=[0, L], u(0)=C, u(L)=D .
		\end{equation}
	
		\noindent There are two new features of this problem compared with previous examples: a variable coefficient $a(x)$ and nonzero Dirichlet| conditions at both boundary points.\smallbreak
		Let us first deal with the boundary conditions. We seek
		$$ u(x)=B(x)+\sum_{j \in \mathcal{I}_{s}} c_{j} \psi_{i}(x), $$
		with $\psi_{i}(0)=\psi_{i}(L)=0$ for $i \in \mathcal{I}_{s}$. The function $B(x)$ must then fulfill $B(0)=C$ and $B(L)=D$. How $B$ varies in between $x=0$ and $x=L$ is not of importance.\smallbreak
		\noindent One possible choice is
		$$ B(x)=C+\frac{1}{L}(D-C) x, $$
		which follows from (164) with $p=1$.\smallbreak
		We seek $(u-B) \in V$. As usual,
		$$ V=\operatorname{span}\left\{\psi_{0}, \ldots, \psi_{N}\right\}, $$
		but the two Dirichlet boundary conditions demand that
		$$ \psi_{i}(0)=\psi_{i}(L)=0, \quad i \in \mathcal{I}_{s} .$$
		Note that any $v \in V$ has the property $v(0)=v(L)=0$.\smallbreak
		The residual arises by inserting our $u$ in the differential equation:
		$$ R=-\frac{d}{d x}\left(\alpha \frac{d u}{d x}\right)-f .$$
		Galerkin's method is
		$$(R, v)=0, \quad \forall v \in V,$$
		or written with explicit integrals,
		$$\int_{\Omega}\left(\frac{d}{d x}\left(\alpha \frac{d u}{d x}\right)-f\right) v \mathrm{~d} x=0, \quad \forall v \in V$$
		We proceed with integration by parts to lower the derivative from second to first order:
		$$
		-\int_{\Omega} \frac{d}{d x}\left(\alpha(x) \frac{d u}{d x}\right) v \mathrm{~d} x=\int_{\Omega} \alpha(x) \frac{d u}{d x} \frac{d v}{d x} \mathrm{~d} x-\left[\alpha \frac{d u}{d x} v\right]_{0}^{L} .
		$$\smallbreak
		The boundary term vanishes since $v(0)=v(L)=0$. The variational formulation is then
		$$
		\int_{\Omega} \alpha(x) \frac{d u}{d x} \frac{d v}{d x} \mathrm{~d} x=\int_{\Omega} f(x) v \mathrm{~d} x, \quad \forall v \in V .
		$$
		The variational formulation can alternatively be written in a more compact form:
		$$
		\left(\alpha u^{\prime}, v^{\prime}\right)=(f, v), \quad \forall v \in V .
		$$
		The corresponding abstract notation reads
		$$
		a(u, v)=L(v) \quad \forall v \in V
		$$
		with
		$$
		a(u, v)=\left(\alpha u^{\prime}, v^{\prime}\right), \quad L(v)=(f, v) .
		$$
		Note that the $a$ in the notation $a(\cdot, \cdot)$ is not to be mixed with the variable coefficient $a(x)$ in the differential equation.\smallbreak
		We may insert $u=B+\sum_{j} c_{j} \psi_{j}$ and $v=\psi_{i}$ to derive the linear system:
		$$\left(\alpha B^{\prime}+\alpha \sum_{j \in \mathcal{I}_{s}} c_{j} \psi_{j}^{\prime}, \psi_{i}^{\prime}\right)=\left(f, \psi_{i}\right), \quad i \in \mathcal{I}_{s} .$$
		Isolating everything with the $c_{j}$ coefficients on the left-hand side and all known terms on the right-hand side gives
		$$\sum_{j \in \mathcal{I}_{s}}\left(\alpha \psi_{j}^{\prime}, \psi_{i}^{\prime}\right) c_{j}=\left(f, \psi_{i}\right)+\left(a(D-C) L^{-1}, \psi_{i}^{\prime}\right), \quad i \in \mathcal{I}_{s} .$$
		This is nothing but a linear system $\sum_{j} A_{i, j} c_{j}=b_{i}$ with
		$$
		\begin{aligned}
			A_{i, j} &=\left(a \psi_{j}^{\prime}, \psi_{i}^{\prime}\right)=\int_{\Omega} \alpha(x) \psi_{j}^{\prime}(x), \psi_{i}^{\prime}(x) \mathrm{d} x \\
			b_{i} &=\left(f, \psi_{i}\right)+\left(a(D-C) L^{-1}, \psi_{i}^{\prime}\right)=\int_{\Omega}\left(f(x) \psi_{i}(x)+\alpha(x) \frac{D-C}{L} \psi_{i}^{\prime}(x)\right) \mathrm{d} x
		\end{aligned}
		$$\bigbreak
	\section[First-order derivative in the equation and boundary condition]{First-order derivative in the equation and boundary condition}
		\label{sec:sec_12_2}
		\noindent The next problem to formulate in variational form reads
		
		\begin{equation}
		\label{eqa167}
			-u^{\prime \prime}(x)+b u^{\prime}(x)=f(x), \quad x \in \Omega=[0, L], u(0)=C, u^{\prime}(L)=E .
		\end{equation}
	
		\noindent The new features are a first-order derivative $u^{\prime}$ in the equation and the boundary condition involving the derivative: $u^{\prime}(L)=E$. Since we have a Dirichlet condition at $x=0$, we must force $\psi_{i}(0)=0$ and use a boundary function to take care of the condition $u(0)=C$. Because there is no Dirichlet condition on $x=L$ we do not make any requirements to $\psi_{i}(L)$. The simplest possible choice of $B(x)$ is $B(x)=C$.\smallbreak
		The expansion for $u$ becomes
		$$
		u=C+\sum_{j \in \mathcal{I}_{s}} c_{j} \psi_{i}(x) .
		$$ \smallbreak
		The variational formulation arises from multiplying the equation by a test function $v \in V$ and integrating over $\Omega$ :
		$$
		\left(-u^{\prime \prime}+b u^{\prime}-f, v\right)=0, \quad \forall v \in V
		$$
		\noindent We apply integration by parts to the $u^{\prime \prime} v$ term only. Although we could also integrate $u^{\prime} v$ by parts, this is not common. The result becomes
		$$
		\left(u^{\prime}+b u^{\prime}, v^{\prime}\right)=(f, v)+\left[u^{\prime} v\right]_{0}^{L}, \quad \forall v \in V .
		$$
		Now, $v(0)=0$ so
		$$
		\left[u^{\prime} v\right]_{0}^{L}=u^{\prime}(L) v(L)=E v(L),
		$$
		because $u^{\prime}(L)=E$. Integration by parts allows us to take care of the Neumann condition in the boundary term.
		
		\begin{mybox}
			\textbf{Natural and essential boundary conditions.}
			
			\noindent Omitting a boundary term like $\left[u^{\prime} v\right]_{0}^{L}$ implies that we actually impose the condition $u^{\prime}=0$ unless there is a Dirichlet condition (i.e., $v=0$ ) at that point! This result has great practical consequences, because it is easy to forget the boundary term, and this mistake may implicitly set a boundary condition! Since homogeneous Neumann conditions can be incorporated without doing anything, and non-homogeneous Neumann conditions can just be inserted in the boundary term, such conditions are known as \emph{natural boundary conditions}. Dirichlet conditions requires more essential steps in the mathematical formulation, such as forcing all $\varphi_{i}=0$ on the boundary and constructing a $B(x)$, and are therefore known as \emph{essential boundary conditions}.
		\end{mybox}
	
		The final variational form reads
		$$ \left(u^{\prime}, v^{\prime}\right)+\left(b u^{\prime}, v\right)=(f, v)+E v(L), \quad \forall v \in V . $$
		In the abstract notation we have 
		$$ a(u, v)=L(v) \quad \forall v \in V $$ 
		with the particular formulas 
		$$ a(u, v)=\left(u^{\prime}, v^{\prime}\right)+\left(b u^{\prime}, v\right), \quad L(v)=(f, v)+E v(L) $$ \smallbreak
		The associated linear system is derived by inserting $u=B+\sum_{j} c_{j} \psi_{j}$ and replacing $v$ by $\psi_{i}$ for $i \in \mathcal{I}_{s}$. Some algebra results in
		$$	\sum_{j \in \mathcal{I}_{s}} \underbrace{\left(\left(\psi_{j}^{\prime}, \psi_{i}^{\prime}\right)+\left(b \psi_{j}^{\prime}, \psi_{i}\right)\right)}_{A_{i, j}} c_{j}=\underbrace{\left(f, \psi_{i}\right)+E \psi_{i}(L)}_{b_{i}} . $$
		Observe that in this problem, the coefficient matrix is not symmetric, because of the term
		$$ \left(b \psi_{j}^{\prime}, \psi_{i}\right)=\int_{\Omega} b \psi_{j}^{\prime} \psi_{i} \mathrm{~d} x \neq \int_{\Omega} b \psi_{i}^{\prime} \psi_{j} \mathrm{~d} x=\left(\psi_{i}^{\prime}, b \psi_{j}\right) $$
		
	\section[Nonlinear coefficient]{Nonlinear coefficient}
		\label{sec:sec_12_3}
		\noindent Finally, we show that the techniques used above to derive variational forms also apply to nonlinear differential equation problems as well. Here is a model problem with a nonlinear coefficient and right-hand side:

		\begin{equation}
		\label{eqa168}
			-\left(\alpha(u) u^{\prime}\right)^{\prime}=f(u), \quad x \in[0, L], u(0)=0, u^{\prime}(L)=E .
		\end{equation}
	
		\noindent Our space $V$ has basis $\left\{\psi_{i}\right\}_{i \in \mathcal{I}_{s}}$, and because of the condition $u(0)=0$, we must require $\psi_{i}(0)=0, i \in \mathcal{I}_{s}$.
		
		Galerkin's method is about inserting the approximate $u$, multiplying the differential equation by $v \in V$, and integrate,
		$$
		-\int_{0}^{L} \frac{d}{d x}\left(\alpha(u) \frac{d u}{d x}\right) v \mathrm{~d} x=\int_{0}^{L} f(u) v \mathrm{~d} x \quad \forall v \in V
		$$
		The integration by parts does not differ from the case where we have $\alpha(x)$ instead of $\alpha(u)$ :
		$$
		\int_{0}^{L} \alpha(u) \frac{d u}{d x} \frac{d v}{d x} \mathrm{~d} x=\int_{0}^{L} f(u) v \mathrm{~d} x+\left[\alpha(u) v u^{\prime}\right]_{0}^{L} \quad \forall v \in V
		$$
		The term $\alpha(u(0)) v(0) u^{\prime}(0)=0$ since $v(0)$. The other term, $\alpha(u(L)) v(L) u^{\prime}(L)$, is used to impose the other boundary condition $u^{\prime}(L)=E$, resulting in
		
		$$
		\int_{0}^{L} \alpha(u) \frac{d u}{d x} \frac{d v}{d x} v \mathrm{~d} x=\int_{0}^{L} f(u) v \mathrm{~d} x+\alpha(u(L)) v(L) E \quad \forall v \in V
		$$
		or alternatively written more compactly as
		$$
		\left(\alpha(u) u^{\prime}, v^{\prime}\right)=(f(u), v)+\alpha(L) v(L) E \quad \forall v \in V .
		$$
		Since the problem is nonlinear, we cannot identify a bilinear form $a(u, v)$ and a linear form $L(v)$. An abstract notation is typically find $u$ such that
		$$
		F(u ; v)=0 \quad \forall v \in V,
		$$
		with
		$$
		F(u ; v)=\left(a(u) u^{\prime}, v^{\prime}\right)-(f(u), v)-a(L) v(L) E .
		$$
		By inserting $u=\sum_{j} c_{j} \psi_{j}$ we get a nonlinear system of algebraic equations for the unknowns $c_{i}, i \in \mathcal{I}_{s}$. Such systems must be solved by constructing a sequence of linear systems whose solutions hopefully converge to the solution of the nonlinear system. Frequently applied methods are Picard iteration and Newton's method.\bigbreak 
	
	\section[Computing with Dirichlet and Neumann conditions]{Computing with Dirichlet and Neumann conditions}
	\label{sec:sec_12_4}
		\noindent Let us perform the necessary calculations to solve
		$$
		-u^{\prime \prime}(x)=2, \quad x \in \Omega=[0,1], \quad u^{\prime}(0)=C, u(1)=D,
		$$
		using a global polynomial basis $\psi_{i} \sim x^{i}$. The requirements on $\psi_{i}$ is that $\psi_{i}(1)=0$, because $u$ is specified at $x=1$, so a proper set of polynomial basis functions can be
		$$
		\psi_{i}(x)=(1-x)^{i+1}, \quad i \in \mathcal{I}_{s} .
		$$
		A suitable $B(x)$ function to handle the boundary condition $u(1)=D$ is $B(x)=$ $D x$. The variational formulation becomes
		$$
		\left(u^{\prime}, v^{\prime}\right)=(2, v)-C v(0) \quad \forall v \in V .
		$$
		The entries in the linear system are then
		$$
		\begin{aligned}
			A_{i, j} &=\left(\psi_{j}, \psi_{i}\right)=\int_{0}^{1} \psi_{i}^{\prime}(x) \psi_{j}^{\prime}(x) \mathrm{d} x=\int_{0}^{1}(i+1)(j+1)(1-x)^{i+j} \mathrm{~d} x=\frac{i j+i+j+1}{i+j+1} \\
			b_{i} &=\left(2, \psi_{i}\right)-\left(D, \psi_{i}^{\prime}\right)-C \psi_{i}(0) \\
			&=\int_{0}^{1}\left(2 \psi_{i}(x)-D \psi_{i}^{\prime}(x)\right) \mathrm{d} x-C \psi_{i}(0) \\
			&=\int_{0}^{1}\left(2(1-x)^{i+1}-D(i+1)(1-x)^{i}\right) \mathrm{d} x-C \psi_{i}(0) \\
			&=\frac{2-(2+i)(D+C)}{i+2}
		\end{aligned}
		$$\smallbreak 
		With $N=1$ the global matrix system is
		$$
		\left(\begin{array}{cc}
			1 & 1 \\
			1 & 4 / 3
		\end{array}\right)\left(\begin{array}{c}
			c_{0} \\
			c_{1}
		\end{array}\right)=\left(\begin{array}{c}
			-C+D+1 \\
			2 / 3-C+D
		\end{array}\right)
		$$
		The solution becomes $c_{0}=-C+D+2$ and $c_{1}=-1$, resulting in
		
		\begin{equation}
		\label{eqa169}
			u(x)=1-x^{2}+D+C(x-1)
		\end{equation}
			
		The exact solution is found by. integrating twice and applying the boundary conditions, either by hand or using sympy as shown in Section 11.2. It appears that the numerical solution coincides with the exact one. This result is to be expected because if $\left(u_{\mathrm{e}}-B\right) \in V, u=u_{\mathrm{e}}$, as proved next. \bigbreak
			
	\section[When the numerical method is exact]{When the numerical method is exact}
		\label{sec:sec_12_5}
		\noindent We have some variational formulation: find $(u-B) \in V$ such that $a(u, v)=$ $L(u) \forall V$. The exact solution also fulfills $a\left(u_{e}, v\right)=L(v)$, but normally $\left(u_{e}-B\right)$ lies in a much larger (infinite-dimensional) space. Suppose, nevertheless, that $u_{\mathrm{e}}=B+E$, where $E \in V$. That is, apart from Dirichlet conditions, $u_{\mathrm{e}}$ lines in our finite-dimensional space $V$ we use to compute $u$. Writing also $u$ on the same form $u=B+F$, we have
		$$
		\begin{aligned}
			a(B+E, v)=L(v) & \forall v \in V \\
			a(B+F, v)=L(v) & \forall v \in V
		\end{aligned}
		$$
		Subtracting the equations show that $a(E-F, v)=0$ for all $v \in V$, and therefore $E-F=0$ and $u=u_{\mathrm{e}}$.
			
		The case treated in Section $12.4$ is of the type where $u_{\mathrm{e}}-B$ is a quadratic function that is 0 at $x=1$, and therefore $\left(u_{\mathrm{e}}-B\right) \in V$, and the method finds the exact solution.
		
	\chapter{Computing with finite elements}
	\label{chap:chap_13}
	\pagenumbering{arabic}
	
	\noindent The purpose of this section is to demonstrate in detail how the finite element method can the be applied to the model problem
	$$
	-u^{\prime \prime}(x)=2, \quad x \in(0, L), u(0)=u(L)=0,
	$$
	with variational formulation
	$$
	\left(u^{\prime}, v^{\prime}\right)=(2, v) \quad \forall v \in V
	$$
	The variational formulation is derived in Section \ref{sec:sec_11_10}
	
	\section[Finite element mesh and basis functions]{Finite element mesh and basis functions}
		\label{sec:sec_13_1}
		
		\noindent We introduce a finite element mesh with $N_{e}$ cells, all with length $h$, and number the cells from left to right. global nodes. Choosing $\mathrm{P} 1$ elements, there are two nodes per cell, and the coordinates of the nodes become
		$$
		x_{i}=i h, \quad h=L / N_{e}, \quad i=0, \ldots, N_{n}=N_{e}+1,
		$$
		provided we number the nodes from left to right.\smallbreak
		Each of the nodes, $i$, is associated a finite element basis function $\varphi_{i}(x)$. When approximating a given function $f$ by a finite element function $u$, we expand $u$ using finite element basis functions associated with all nodes in the mesh, i.e., $N=N_{n}$. However, when solving differential equations we will often have $N<N_{n}$ because of Dirichlet boundary conditions. Why this is the case will now be explained in detail.\smallbreak
		In our case with homogeneous Dirichlet boundary conditions we do not need any boundary function $B(x)$ and can work with the expansion
		
		\begin{equation}
		\label{eqa170}
			u(x)=\sum_{j \in \mathcal{I}_{s}} c_{j} \psi_{j}(x) .
		\end{equation}
	
		\noindent Because of the boundary conditions, we must demand $\psi_{i}(0)=\psi_{i}(L)=0, i \in \mathcal{I}_{s}$. When $\psi_{i}, i=0, \ldots, N$, is to be selected among the finite element basis functions $\varphi_{j}, i=0, \ldots, N_{n}$, we have to avoid using $\varphi_{j}$ functions that do not vanish at $x_{0}=0$ and $x_{N_{n}}=L$. However, all $\varphi_{j}$ vanish at these two nodes for $j=1, \ldots, N_{n}$. Only basis functions associated with the end nodes, $\varphi_{0}$ and $\varphi_{N_{n}}$, violate the boundary conditions of our differential equation. Therefore, we select the basis functions $\varphi_{i}$ to be the set of finite element basis functions associated with all the interior nodes in the mesh:
		$$
		\psi_{i}=\varphi_{i+1}, \quad i=0, \ldots, N .
		$$
		Here, $N=N_{n}-2$.\smallbreak
		In the general case, the nodes are not necessarily numbered from left to right, so we introduce a mapping from the node numbering, or more precisely the degree of freedom numbering, to the numbering of the unknowns in the final equation system. These unknowns take on the numbers $0, \ldots, N$. Unknown number $j$ in the linear system corresponds to degree of freedom number $\nu(j)$, $j \in \mathcal{I}_{s}$. We can then write
		$$
		\psi_{i}=\varphi_{\nu(i)}, \quad i=0, \ldots, N .
		$$
		With a regular numbering as in the present example, $\nu(j)=j+1, j=1, \ldots, N=$ $N_{n}-2$.
	
	\section[Computation in the global physical domain]{Computation in the global physical domain}
		\label{sec:sec_13_2}
		\noindent We shall first perform a computation in the x coordinate system because the integrals can be easily computed here by simple, visual, geometric considerations.
		This is called a global approach since we work in the $x$ coordinate system and compute integrals on the global domain $[0, L]$.\smallbreak
		The entries in the coefficient matrix and right-hand side are
		$$
		A_{i, j}=\int_{0}^{L} \psi_{i}^{\prime}(x) \psi_{j}^{\prime}(x) \mathrm{d} x, \quad b_{i}=\int_{0}^{L} 2 \psi_{i}(x) \mathrm{d} x, \quad i, j \in \mathcal{I}_{s} .
		$$
		Expressed in terms of finite element basis functions $\varphi_{i}$ we get the alternative expressions
		$$
		A_{i, j}=\int_{0}^{L} \varphi_{i+1}^{\prime}(x) \varphi_{j+1}^{\prime}(x) \mathrm{d} x, \quad b_{i}=\int_{0}^{L} 2 \varphi_{i+1}(x) \mathrm{d} x, \quad i, j \in \mathcal{I}_{s} .
		$$
		For the following calculations the subscripts on the finite element basis functions are more conveniently written as $i$ and $j$ instead of $i+1$ and $j+1$, so our notation becomes
		$$
		A_{i-1, j-1}=\int_{0}^{L} \varphi_{i}^{\prime}(x) \varphi_{j}^{\prime}(x) \mathrm{d} x, \quad b_{i-1}=\int_{0}^{L} 2 \varphi_{i}(x) \mathrm{d} x
		$$
		where the $i$ and $j$ indices run as $i, j=1, \ldots, N_{n}-1=N+1$.\smallbreak
		The $\varphi_{i}(x)$ function is a hat function with peak at $x=x_{i}$ and a linear variation in $\left[x_{i-1}, x_{i}\right]$ and $\left[x_{i}, x_{i+1}\right]$. The derivative is $1 / h$ to the left of $x_{i}$ and $-1 / h$ to the right, or more formally,
	
		\begin{equation}
		\label{eqa171}
			\varphi_{i}^{\prime}(x)= \begin{cases}0, & x<x_{i-1}, \\ h^{-1}, & x_{i-1} \leq x<x_{i}, \\ -h^{-1}, & x_{i} \leq x<x_{i+1}, \\ 0, & x \geq x_{i+1}\end{cases}
		\end{equation}
	
		Figure \ref{fig:img_47} shows $\varphi_{1}^{\prime}(x)$ and $\varphi_{2}^{\prime}(x)$.
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.7\linewidth]{img_47}
			\caption{Illustration of the derivative of piecewise linear basis functions associated with nodes in cell 2.}
			\label{fig:img_47}
		\end{figure}
		We realize that $\varphi_{i}^{\prime}$ and $\varphi_{j}^{\prime}$ has no overlap, and hence their product vanishes, unless $i$ and $j$ are nodes belonging to the same cell. The only nonzero contributions to the coefficient matrix are therefore
		$$
		\begin{aligned}
			A_{i-1, i-2} &=\int_{0}^{L} \varphi_{i}^{\prime}(x) \varphi_{i-1}^{\prime}(x) \mathrm{d} x, \\
			A_{i-1, i-1} &=\int_{0}^{L} \varphi_{i}^{\prime}(x)^{2} \mathrm{~d} x, \\
			A_{i-1, i} &=\int_{0}^{L} \varphi_{i}^{\prime}(x) \varphi_{i+1}^{\prime}(x) \mathrm{d} x,
		\end{aligned}
		$$
		for $i=1, \ldots, N_{n}-1$, but for $i=1, A_{i-1, i-2}$ is not defined, and for $i=N_{n}-1$, $A_{i-1, i}$ is not defined.
		
		We see that $\varphi_{i-1}^{\prime}(x)$ and $\varphi_{i}^{\prime}(x)$ have overlap of one cell $\Omega^{(i-1)}=\left[x_{i-1}, x_{i}\right]$ and that their product then is $-1 / h^{2}$. The integrand is constant and therefore $A_{i-1, i-2}=-h^{-2} h=-h^{-1}$. A similar reasoning can be applied to $A_{i-1, i}$, which also becomes $-h^{-1}$. The integral of $\varphi_{i}^{\prime}(x)^{2}$ gets contributions from two cells, $\Omega^{(i-1)}=\left[x_{i-1}, x_{i}\right]$ and $\Omega^{(i)}=\left[x_{i}, x_{i+1}\right]$, but $\varphi_{i}^{\prime}(x)^{2}=h^{-2}$ in both cells, and the length of the integration interval is $2 h$ so we get $A_{i-1, i-1}=2 h^{-1}$.
		
		The right-hand side involves an integral of $2 \varphi_{i}(x), i=1, \ldots, N_{n}-1$, which is just the area under a hat function of height 1 and width $2 h$, i.e., equal to $h$. Hence, $b_{i-1}=2 h$.
		
		To summarize the linear system, we switch from $i$ to $i+1$ such that we can write
		$$
		A_{i, i-1}=A_{i, i-1}=-h^{-1}, \quad A_{i, i}=2 h^{-1}, \quad b_{i}=2 h .
		$$
		
		The equation system to be solved only involves the unknowns $c_{i}$ for $i \in \mathcal{I}_{s}$. With our numbering of unknowns and nodes, we have that $c_{i}$ equals $u\left(x_{i+1}\right)$. The complete matrix system that takes the following form:
		
		\begin{equation}
		\label{eqa172}
			\frac{1}{h}\left(\begin{array}{ccccccccc}
				2 & -1 & 0 & \cdots & \cdots & \cdots & \cdots & \cdots & 0 \\
				-1 & 2 & -1 & \ddots & & & & & \vdots \\
				0 & -1 & 2 & -1 & \ddots & & & & \vdots \\
				\vdots & \ddots & & \ddots & \ddots & 0 & & & \vdots \\
				\vdots & & \ddots & \ddots & \ddots & \ddots & \ddots & & \vdots \\
				\vdots & & & 0 & -1 & 2 & -1 & \ddots & \vdots \\
				\vdots & & & & \ddots & \ddots & \ddots & \ddots & 0 \\
				\vdots & & & & & \ddots & \ddots & \ddots & -1 \\
				0 & \cdots & \cdots & \cdots & \cdots & \cdots & 0 & -1 & 2
			\end{array}\right)\left(\begin{array}{c}
				c_{0} \\
				\vdots \\
				\vdots \\
				\vdots \\
				\vdots \\
				\vdots \\
				\vdots \\
				\vdots \\
				c_{N}
			\end{array}\right)=\left(\begin{array}{c}
				2 h \\
				\vdots \\
				\vdots \\
				\vdots \\
				\vdots \\
				\vdots \\
				\vdots \\
				\vdots \\
				2 h
			\end{array}\right)
		\end{equation}
	
	\section[Comparison with a finite difference discretization]{Comparison with a finite difference discretization}
		\noindent A typical row in the matrix system can be written as
	
		\begin{equation}
		\label{eqa173}
			-\frac{1}{h} c_{i-1}+\frac{2}{h} c_{i}-\frac{1}{h} c_{i+1}=2 h .
		\end{equation}
	
		\noindent Let us introduce the notation $u_{j}$ for the value of $u$ at node $j: u_{j}=u\left(x_{j}\right)$ since we have the interpretation $u\left(x_{j}\right)=\sum_{j} c_{j} \varphi\left(x_{j}\right)=\sum_{j} c_{j} \delta_{i j}=c_{j}$. The unknowns $c_{0}, \ldots, c_{N}$ are $u_{1}, \ldots, u_{N_{n}}$. Shifting $i$ with $i+1$ in (173) and inserting $u_{i}=c_{i-1}$, we get
	
		\begin{equation}
		\label{eqa174}
			-\frac{1}{h} u_{i-1}+\frac{2}{h} u_{i}-\frac{1}{h} u_{i+1}=2 h,
		\end{equation}
	
		A finite difference discretization of $-u^{\prime \prime}(x)=2$ by a centered, second-order finite difference approximation $u^{\prime \prime}\left(x_{i}\right) \approx\left[D_{x} D_{x} u\right]_{i}$ with $\Delta x=h$ yields
	
		\begin{equation}
		\label{eqa175}
			-\frac{u_{i-1}-2 u_{i}+u_{i+1}}{h^{2}}=2,
		\end{equation}
		which is, in fact, equivalent to (174) if (174) is divided by $h$. Therefore, the finite difference and the finite element method are equivalent in this simple test problem.
		
		Sometimes a finite element method generates the finite difference equations on a uniform mesh, and sometimes the finite element method generates equations that are different. The differences are modest, but may influence the numerical quality of the solution significantly, especially in time-dependent problems.\bigbreak 
		
\section[Cellwise computations]{Cellwise computations}
	\label{sec:sec_13_4}
	\noindent We now employ the cell by cell computational procedure where an element matrix and vector are calculated for each cell and assembled in the global linear system. All integrals are mapped to the local reference coordinate system $X \in[-1,1]$. In the present case, the matrix entries contain derivatives with respect to $x$,
	$$
	A_{i-1, j-1}^{(e)}=\int_{\Omega^{(\varepsilon)}} \varphi_{i}^{\prime}(x) \varphi_{j}^{\prime}(x) \mathrm{d} x=\int_{-1}^{1} \frac{d}{d x} \tilde{\varphi}_{r}(X) \frac{d}{d x} \tilde{\varphi}_{s}(X) \frac{h}{2} \mathrm{~d} X,
	$$
	where the global degree of freedom $i$ is related to the local degree of freedom $r$ through $i=q(e, r)$. Similarly, $j=q(e, s)$. The local degrees of freedom run as $r, s=0,1$ for a P1 element.\bigbreak

	\textbf{The integral for the element matrix.  } There are simple formulas for the basis functions $\tilde{\varphi}_{r}(X)$ as functions of $X$. However, we now need to find the derivative of $\tilde{\varphi}_{r}(X)$ with respect to $x$. Given
	$$
	\tilde{\varphi}_{0}(X)=\frac{1}{2}(1-X), \quad \tilde{\varphi}_{1}(X)=\frac{1}{2}(1+X),
	$$
	we can easily compute $d \tilde{\varphi}_{r} / d X$ :
	$$
	\frac{d \tilde{\varphi}_{0}}{d X}=-\frac{1}{2}, \quad \frac{d \tilde{\varphi}_{1}}{d X}=\frac{1}{2}
	$$
	From the chain rule,
	
	\begin{equation}
		\label{eqa176}
		\frac{d \tilde{\varphi}_{r}}{d x}=\frac{d \tilde{\varphi}_{r}}{d X} \frac{d X}{d x}=\frac{2}{h} \frac{d \tilde{\varphi}_{r}}{d X}
	\end{equation}	

	\noindent The transformed integral is then
	$$
	A_{i-1, j-1}^{(e)}=\int_{\Omega(\varepsilon)} \varphi_{i}^{\prime}(x) \varphi_{j}^{\prime}(x) \mathrm{d} x=\int_{-1}^{1} \frac{2}{h} \frac{d \tilde{\varphi}_{r}}{d X} \frac{2}{h} \frac{d \tilde{\varphi}_{s}}{d X} \frac{h}{2} \mathrm{~d} X
	$$
	\textbf{The integral for the element vector.  } The right-hand side is transformed according to
	$$
	b_{i-1}^{(e)}=\int_{\Omega(e)} 2 \varphi_{i}(x) \mathrm{d} x=\int_{-1}^{1} 2 \tilde{\varphi}_{r}(X) \frac{h}{2} \mathrm{dX}, \quad i=q(e, r), r=0,1
	$$
	\textbf{Detailed calculations of the element matrix and vector.   } Specifically for P1 elements we arrive at the following calculations for the element matrix entries:
	$$
	\begin{aligned}
		\tilde{A}_{0,0}^{(e)} &=\int_{-1}^{1} \frac{2}{h}\left(-\frac{1}{2}\right) \frac{2}{h}\left(-\frac{1}{2}\right) \frac{2}{h} \mathrm{~d} X=\frac{1}{h} \\
		\tilde{A}_{0,1}^{(e)} &=\int_{-1}^{1} \frac{2}{h}\left(-\frac{1}{2}\right) \frac{2}{h}\left(\frac{1}{2}\right) \frac{2}{h} \mathrm{~d} X=-\frac{1}{h} \\
		\tilde{A}(e) &=\int_{1,0}^{1} \frac{2}{h}\left(\frac{1}{2}\right) \frac{2}{h}\left(-\frac{1}{2}\right) \frac{2}{h} \mathrm{~d} X=-\frac{1}{h} \\
		\tilde{A}_{1,1}^{(e)} &=\int_{-1}^{1} \frac{2}{h}\left(\frac{1}{2}\right) \frac{2}{h}\left(\frac{1}{2}\right) \frac{2}{h} \mathrm{~d} X=\frac{1}{h}
	\end{aligned}
	$$
	The element vector entries become
	$$
	\begin{aligned}
		&\tilde{b}_{0}^{(e)}=\int_{-1}^{1} 2 \frac{1}{2}(1-X) \frac{h}{2} \mathrm{~d} X=h \\
		&\tilde{b}_{1}^{(e)}=\int_{-1}^{1} 2 \frac{1}{2}(1+X) \frac{h}{2} \mathrm{~d} X=h
	\end{aligned}
	$$
	Expressing these entries in matrix and vector notation, we have
	\begin{equation}
			\label{eqa177}
		\tilde{A}^{(e)}=\frac{1}{h}\left(\begin{array}{rr}
			1 & -1 \\
			-1 & 1
		\end{array}\right), \quad \tilde{b}^{(e)}=h\left(\begin{array}{l}
			1 \\
			1
		\end{array}\right)
	\end{equation}

	\noindent \textbf{Contributions from the first and last cell.   } The first and last cell involve only one unknown and one basis function because of the Dirichlet boundary conditions at the first and last node. The element matrix therefore becomes a $1 \times 1$ matrix and there is only one entry in the element vector. On cell 0 , only $\psi_{0}=\varphi_{1}$ is involved, corresponding to integration with $\tilde{\varphi}_{1}$. On cell $N_{e}$, only $\psi_{N}=\varphi_{N_{n}-1}$ is involved, corresponding to integration with $\tilde{\varphi}_{0}$. We then get the special end-cell contributions
	
	\begin{equation}
		\label{eqa178}
		\tilde{A}^{(e)}=\frac{1}{h}(1), \quad \tilde{b}^{(e)}=h(1),
	\end{equation}

	for $e=0$ and $e=N_{e}$. In these cells, we have only one degree of freedom, not two as in the interior cells.\bigbreak
	
	\noindent \textbf{Assembly.   } The next step is to assemble the contributions from the various cells. The assembly of an element matrix and vector into the global matrix and right-hand side can be expressed as
	$$
	A_{q(e, r), q(e, s)}=A_{q(e, r), q(e, s)}+\tilde{A}_{r, s}^{(e)}, \quad b_{q(e, r)}=b_{q(e, r)}+\tilde{b}_{r}^{(e)},
	$$
	for $r$ and $s$ running over all local degrees of freedom in cell $e$.\smallbreak
	To make the assembly algorithm more precise, it is convenient to set up Python data structures and a code snippet for carrying out all details of the algorithm. For a mesh of four equal-sized P1 elements and $L=2$ we have
	\begin{lstlisting}[numbers=none]
		vertices =[0,0.5,1,1.5,2]
		cells =[[0,1],[1,2],[2,3],[3,4]]
		dof_map =[[0],[0,1],[1,2],[2]]
	\end{lstlisting}
	The total number of degrees of freedom is 3 , being the function values at the internal 3 nodes where $u$ is unknown. In cell 0 we have global degree of freedom 0 , the next cell has $u$ unknown at its two nodes, which become global degrees of freedom 0 and 1 , and so forth according to the \textbf{\texttt{dof\_map}} list. The mathematical $q(e, r)$ quantity is nothing but the \textbf{\texttt{dof\_map}} list.
	Assume all element matrices are stored in a list \textbf{\texttt{Ae}} such that \textbf{\texttt{Ae[e][i, j]}} is $\tilde{A}_{i, j}^{(e)}$. A corresponding list for the element vectors is named \textbf{\texttt{be}}, where \textbf{\texttt{be[e][r]}} is $\tilde{b}_{r}^{(e)}$. A Python code snippet illustrates all details of the assembly algorithm:
	\begin{lstlisting}[numbers=none]
		# A[i,j]: coefficient matrix, b[i]: right-hand side
		for e in range(len(Ae)):
		for r in range(Ae[e].shape[0]):
		for s in range(Ae[e].shape[1]):
		A[dof_map[e,r],dof_map[e,s]] += Ae[e][i,j]
		b[dof_map[e,r]] += be[e][i,j]
	\end{lstlisting}
	
	The general case with \textbf{\texttt{N\_e}} P1 elements of length \textbf{\texttt{h}} has
	\begin{lstlisting}[numbers=none]
		N_n = N_e + 1
		vertices = [i*h for i in range(N_n)]
		cells = [[e, e+1] for e in range(N_e)]
		dof_map = [[0]] + [[e-1, e] for i in range(1, N_e)] + [[N_n-2]]
	\end{lstlisting} \smallbreak
	Carrying out the assembly results in a linear system that is identical to (\ref{eqa172}), which is not surprising since the procedures is mathematically equivalent to the calculations in the physical domain.
	
	A fundamental problem with the matrix system we have assembled is that the boundary conditions are not incorporated if $u(0)$ or $u(L)$ are different from zero. The next sections deals with this issue.
	
\chapter{Boundary conditions: specified nonzero value}
\label{chap:chap_14}
\pagenumbering{arabic}
\setcounter{page}{90}
\noindent We have to take special actions to incorporate Dirichlet conditions, such as $u(L)=D$, into the computational procedures. The present section outlines alternative, yet mathematically equivalent, methods. \bigbreak
	\section[General construction of a boundary function]{General construction of a boundary function}
		\label{sec:sec_14_1}
		\noindent In Section \ref{sec:sec_11_11} we introduce a boundary function $B(x)$ to deal with nonzero Dirichlet boundary conditions for $u$. The construction of such a function is not always trivial, especially not in multiple dimensions. However, a simple and general construction idea exists when the basis functions have the property
		$$
		\varphi_{i}\left(x_{j}\right)=\delta_{i j}, \quad \delta_{i j}= \begin{cases}1, & i=j \\ 0, & i \neq j\end{cases}
		$$
		where $x_{j}$ is a boundary point. Examples on such functions are the Lagrange interpolating polynomials and finite element functions.
		
		Suppose now that $u$ has Dirichlet boundary conditions at nodes with numbers $i \in I_{b}$. For example, $I_{b}=\left\{0, N_{n}\right\}$ in a $1 \mathrm{D}$ mesh with node numbering from left to right. Let $U_{i}$ be the corresponding prescribed values of $u\left(x_{i}\right)$. We can then, in general, use
	
		\begin{equation}
			\label{eqa179}
			B(x)=\sum_{j \in I_{b}} U_{j} \varphi_{j}(x) .
		\end{equation}
	
		\noindent It is easy to verify that $B\left(x_{i}\right)=\sum_{j \in I_{b}} U_{j} \varphi_{j}\left(x_{i}\right)=U_{i}$.\smallbreak
		The unknown function can then be written as
		
		\begin{equation}
			\label{eqa180}
			u(x)=\sum_{j \in I_{b}} U_{j} \varphi_{j}(x)+\sum_{j \in \mathcal{I}_{s}} c_{j} 	\varphi_{\nu(j)}
		\end{equation}
	
		\noindent where $\nu(j)$ maps unknown number $j$ in the equation system to node $\nu(j)$. We can easily show that with this $u$, a Dirichlet condition $u\left(x_{k}\right)=U_{k}$ is fulfilled:
		$$
		u\left(x_{k}\right)=\sum_{j \in I_{b}} U_{j} \underbrace{\varphi_{j}(x)}_{\neq 0 \text { only for } j=k}+\sum_{j \in \mathcal{I}_{s}} c_{j} \underbrace{\varphi_{\nu(j)}\left(x_{k}\right)}_{=0, k \notin \mathcal{I}_{s}}=U_{k}
		$$
		
		Some examples will further clarify the notation. With a regular left-to-right numbering of nodes in a mesh with $\mathrm{P} 1$ elements, and Dirichlet conditions at $x=0$, we use finite element basis functions associated with the nodes $1,2, \ldots, N_{n}$, implying that $\nu(j)=j+1, j=0, \ldots, N$, where $N=N_{n}-1$. For the particular mesh below the expansion becomes
		$$
		u(x)=U_{0} \varphi_{0}(x)+c_{0} \varphi_{1}(x)+c_{1} \varphi_{2}(x)+\cdots+c_{4} \varphi_{5}(x) .
		$$
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.7\linewidth]{img_48}
			\label{figi:img_48}
		\end{figure}
	
		Here is a mesh with an irregular cell and node numbering:
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.7\linewidth]{img_49}
			\label{figi:img_49}
		\end{figure}
	
		Say we in this latter mesh have Dirichlet conditions on the left-most and right-most node, with numbers 3 and 1, respectively. Then we can number the unknowns at the interior nodes from left to right, giving $\nu(0)=0, \nu(1)=4$, $\nu(2)=5, \nu(3)=2$. This gives
		$$
		B(x)=U_{3} \varphi_{3}(x)+U_{1} \varphi_{1}(x),
		$$
		and
		$$
		u(x)=B(x)+\sum_{j=0}^{3} c_{j} \varphi_{\nu(j)}=U_{3} \varphi_{3}+U_{1} \varphi_{1}+c_{0} \varphi_{0}+c_{1} \varphi_{4}+c_{2} \varphi_{5}+c_{3} \varphi_{2}
		$$
		
		Switching to the more standard case of left-to-right numbering and boundary conditions $u(0)=C, u(L)=D$, we have $N=N_{n}-2$ and
		$$
		\begin{aligned}
			u(x) &=C \varphi_{0}+D_{\varphi_{n}}+\sum_{j \in I_{n}} c_{j} \varphi_{j+1} \\
			&=C \varphi_{0}+D_{\varphi_{N}}+c_{0} \varphi_{1}+c_{1} \varphi_{2}+\cdots+c_{N} \varphi_{N_{n}-1}
		\end{aligned}
		$$
		
		The idea of constructing $B$ described here generalizes almost trivially to $2 \mathrm{D}$ and 3D problems: $B=\sum_{j \in I_{b}} U_{j} \varphi_{j}$, where $I_{b}$ is the index set containing the numbers of all the nodes on the boundaries where Dirichlet values are prescribed.
		
	\section[Example on computing with finite element-based a boundary function]{Example on computing with finite element-based a boundary function}
		\label{sec:sec_14_2}
		\noindent Let us see how the model problem $-u^{\prime \prime}=2, u(0)=C, u(L)=D$, is affected by a $B(x)$ to incorporate boundary values. Inserting the expression
		$$
		u(x)=B(x)+\sum_{j \in \mathcal{I}_{x}} c_{j} \psi_{j}(x)
		$$
		in $-\left(u^{\prime \prime}, \psi_{i}\right)=\left(f, \psi_{i}\right)$ and integrating by parts results in a linear system with
		$$
		A_{i, j}=\int_{0}^{L} \psi_{i}^{\prime}(x) \psi_{j}^{\prime}(x) \mathrm{d} x, \quad b_{i}=\int_{0}^{L}\left(f(x)-B^{\prime}(x)\right) \psi_{i}(x) \mathrm{d} x
		$$
		We choose $\psi_{i}=\varphi_{i+1}, i=0, \ldots, N=N_{n}-2$ if the node numbering is from left to right. (Later we also need the assumption that the cells too are numbered from left to right.) The boundary function becomes
		$$
		B(x)=C \varphi_{0}(x)+D \varphi_{N_{n}}(x) .
		$$
		The expansion for $u(x)$ is
		$$
		u(x)=B(x)+\sum_{j \in \mathcal{I}_{x}} c_{j} \varphi_{j+1}(x) .
		$$
		We can write the matrix and right-hand side entries as
		$$
		A_{i-1, j-1}=\int_{0}^{L} \varphi_{i}^{\prime}(x) \varphi_{j}^{\prime}(x) \mathrm{d} x, \quad b_{i-1}=\int_{0}^{L}\left(f(x)-C \varphi_{0}^{\prime}(x)-D \varphi_{N_{\mathrm{m}}}^{\prime}(x)\right) \varphi_{i}(x) \mathrm{d} x
		$$
		for $i, j=1, \ldots, N+1=N_{n}-1$. Note that we have here used $B^{\prime}=C \varphi_{0}^{\prime}+D_{\varphi^{\prime}} N_{n}$.\bigbreak
		\textbf{Computations in physical coordinates.   } Most of the terms in the linear system have already been computed so we concentrate on the new contribution from the boundary function. The integral $\left.C \int_{0}^{L} \varphi_{0}^{\prime}(x)\right) \varphi_{i}(x) \mathrm{d} x$ can only get a nonzero contribution from the first cell, $\Omega^{(0)}=\left[x_{0}, x_{1}\right]$ since $\varphi_{0}^{\prime}(x)=0$ on all other cells. Moreover, $\varphi_{0}^{\prime}(x) \varphi_{i}(x) \mathrm{d} x \neq 0$ only for $i=0$ and $i=1$ (but $i=0$ is excluded), since $\varphi_{i}=0$ on the first cell if $i>1$. With a similar reasoning we realize that $\left.D \int_{0}^{L} \varphi_{N_{n}}^{\prime}(x)\right) \varphi_{i}(x) \mathrm{d} x$ can only get a nonzero contribution from the last cell. From the explanations of the calculations in Section \ref{sec:sec_3_6} we then find that
		$$
		\int_{0}^{L} \varphi_{0}^{\prime}(x) \varphi_{1}(x) \mathrm{d} x=\frac{1}{h} \cdot \frac{1}{h}=-\frac{1}{2}, \quad \int_{0}^{L} \varphi_{N_{n}}^{\prime}(x) \varphi_{N_{n}-1}(x) \mathrm{d} x=\frac{1}{h} \cdot \frac{1}{h}=\frac{1}{2}
		$$
		The extra boundary term because of $B(x)$ boils down to adding $C / 2$ to $b_{0}$ and $-D / 2$ to $b_{N}$.\bigbreak 
		
		\noindent \textbf{Cellwise computations on the reference element.   } As an equivalent alternative, we now turn to cellwise computations. The element matrices and vectors are calculated as Section \ref{sec:sec_13_4}, so we concentrate on the impact of the new term involving $B(x)$. We observe that $C \varphi_{0}^{\prime}=0$ on all cells except $e=0$, and $D \varphi_{N_{n}}^{\prime}=0$ on all cells except $e=N_{e}$. In this case there is only one unknown in these cells since $u(0)$ and $u(L)$ are prescribed, so the element vector has only one entry. The entry for the last cell, $e=N_{c}$, becomes
		$$
		\bar{b}_{0}^{(e)}=\int_{-1}^{1}\left(f-D \frac{2}{h} \frac{d \bar{\varphi}_{1}}{d X}\right) \bar{\varphi}_{0} \frac{h}{2} \mathrm{~d} X=\left(\frac{h}{2}\left(2-D \frac{2}{h} \frac{1}{2}\right) \int_{-1}^{1} \bar{\varphi}_{0} \mathrm{~d} X=h-D / 2\right.
		$$
		Similar computations on the first cell yield
		$$
		\tilde{b}_{0}^{(0)}=\int_{-1}^{1}\left(f-C \frac{2}{h} \frac{d \bar{\varphi}_{0}}{d X}\right) \bar{\varphi}_{1} \frac{h}{2} \mathrm{~d} X=\left(\frac{h}{2}\left(2+C \frac{2}{h} \frac{1}{2}\right) \int_{-1}^{1} \bar{\varphi}_{1} \mathrm{~d} X=h+C / 2\right.
		$$
		When assembling these contributions, we see that $b_{0}$ gets right-hand side of the linear system gets an extra term $C / 2$ and $b_{N}$ gets $-D / 2$, as in the computations in the physical domain.\bigbreak 
	
	\section[Modification of the linear system]{Modification of the linear system}
		\label{sec:sec_14_3}
		\noindent From an implementational point of view, there is a convenient alternative to adding the $B(x)$ function and using only the basis functions associated with nodes where $u$ is truly unknown. Instead of seeking
		
		\begin{equation}
			\label{eqa181}
			u(x)=\sum_{j \in I_{b}} U_{j} \varphi_{j}(x)+\sum_{j \in I_{a}} c_{j} \varphi_{\nu(j)}(x)
		\end{equation}
	
		\noindent we use the sum over all degrees of freedom, including the known boundary values:
		
		\begin{equation}
			\label{eqa182}
			u(x)=\sum_{j \in \mathcal{I}_{s}} c_{j} \varphi_{j}(x) .
		\end{equation}
	
		\noindent Note that the collections of unknowns $\left\{c_{i}\right\}_{i \in \mathcal{I}_{s}}$ in (\ref{eqa181}) and (\ref{eqa182}) are different: in (\ref{eqa181}) $N$ counts the number of nodes where $u$ is not known, while in (\ref{eqa181}) $N$ counts all the nodes $\left(N=N_{n}\right)$.
		
		The idea is to compute the entries in the linear system as if no Dirichlet values are prescribed. Afterwards, we modify the linear system to ensure that the known $c_{j}$ values are incorporated.
		
		A potential problem arises for the boundary term $\left[u^{\prime} v\right]_{0}^{L}$ from the integration by parts: imagining no Dirichlet conditions means that we no longer require $v=0$ at Dirichlet points, and the boundary term is then nonzero at these points. However, when we modify the linear system, we will erase whatever the contribution from $\left[u^{\prime} v\right]_{0}^{L}$ should be at the Dirichlet points in the right-hand side of the linear system. We can therefore safely forget $\left[u^{\prime} v\right]_{0}^{L}$ at any point where a Dirichlet condition applies.\bigbreak
		
		\noindent \textbf{Computations in the physical system.   } Let us redo the computations in the example in Section \ref{sec:sec_14_1}. We solve $-u^{\prime \prime}=2$ with $u(0)=0$ and $u(L)=D$. The expressions for $A_{i, j}$ and $b_{i}$ are the same, but the numbering is different as the numbering of unknowns and nodes now coincide:
		$$
		A_{i, j}=\int_{0}^{L} \varphi_{i}^{\prime}(x) \varphi_{j}^{\prime}(x) \mathrm{d} x, \quad b_{i}=\int_{0}^{L} f(x) \varphi_{i}(x) \mathrm{d} x
		$$
		for $i, j=0, \ldots, N=N_{n}$. The integrals involving basis functions corresponding to interior mesh nodes, $i, j=1, \ldots, N_{n}-1$, are obviously the same as before. We concentrate on the contributions from $\varphi_{0}$ and $\varphi_{N_{n}}$ :
		$$
		\begin{aligned}
			A_{0,0} &=\int_{0}^{L}\left(\varphi_{0}^{\prime}\right)^{2} \mathrm{~d} x=\int_{0}^{x_{1}}=\left(\varphi_{0}^{\prime}\right)^{2} \mathrm{~d} x \frac{1}{h} \\
			A_{0,1} &=\int_{0}^{L} \varphi_{0}^{\prime} \varphi_{1}^{\prime} \mathrm{d} x=\int_{0}^{x_{1}} \varphi_{0}^{\prime} \varphi_{1}^{\prime} \mathrm{d} x=-\frac{1}{h} \\
			A_{N, N} &=\int_{0}^{L}\left(\varphi_{0}^{\prime}\right)^{2} \mathrm{~d} x=\int_{x_{N_{n}-1}}^{x_{n}}\left(\varphi_{0}^{\prime}\right)^{2} \mathrm{~d} x=\frac{1}{h} \\
			A_{N, N-1} &=\int_{0}^{L}\left(\varphi_{0}^{\prime}\right)^{2} \mathrm{~d} x=\int_{x_{N_{n}-1}}\left(\varphi_{0}^{\prime}\right)^{2} \mathrm{~d} x=-\frac{1}{h}
		\end{aligned}
		$$
		The new terms on the right-hand side are also those involving $\varphi_{0}$ and $\varphi_{N_{n}}$ :
		$$
		\begin{aligned}
			b_{0} &=\int_{0}^{L} 2 \varphi_{0}(x) \mathrm{d} x=\int_{0}^{x_{1}} 2 \varphi_{0}(x) \mathrm{d} x=h, \\
			b_{N} &=\int_{0}^{L} 2 \varphi_{N_{n}} \mathrm{~d} x=\int_{x_{N_{n}-1}}^{x_{N_{n}}} 2 \varphi_{N_{n}} \mathrm{~d} x=h .
		\end{aligned}
		$$
		The complete matrix system, involving all degrees of freedom, takes the form
		\begin{equation}
			\label{eqa183}
			\frac{1}{h}\left(\begin{array}{ccccccccc}
				1 & -1 & 0 & \cdots & \cdots & \cdots & \cdots & \cdots & 0 \\
				-1 & 2 & -1 & \ddots & & & & & \vdots \\
				0 & -1 & 2 & -1 & \ddots & & & & \vdots \\
				\vdots & \ddots & & \ddots & \ddots & 0 & & & \vdots \\
				\vdots & & \ddots & \ddots & \ddots & \ddots & \ddots & & \vdots \\
				\vdots & & & 0 & -1 & 2 & -1 & \ddots & \vdots \\
				\vdots & & & & \ddots & \ddots & \ddots & \ddots & 0 \\
				\vdots & & & & & \ddots & \ddots & \ddots & -1 \\
				0 & \ldots & \ldots & \cdots & \cdots & \cdots & 0 & -1 & 1
			\end{array}\right)\left(\begin{array}{c}
				c_{0} \\
				\vdots \\
				\vdots \\
				\vdots \\
				\vdots \\
				\vdots \\
				\vdots \\
				\vdots \\
				c_{N}
			\end{array}\right)=\left(\begin{array}{c}
				h \\
				2 h \\
				\vdots \\
				\vdots \\
				\vdots \\
				\vdots \\
				\vdots \\
				2 h \\
				h
			\end{array}\right)
		\end{equation}
	
		Incorporation of Dirichlet values can now be done by replacing the first and last equation by $c_{0}=0$ and $c_{N}=D$. This action changes the system to
		
	\begin{equation}
		\label{eqa184}	
		\frac{1}{h}\left(\begin{array}{ccccccccc}
			h & 0 & 0 & \cdots & \cdots & \cdots & \cdots & \cdots & 0 \\
			-1 & 2 & -1 & \ddots & & & & & \vdots \\
			0 & -1 & 2 & -1 & \ddots & & & & \vdots \\
			\vdots & \ddots & & \ddots & \ddots & 0 & & & \vdots \\
			\vdots & & \ddots & \ddots & \ddots & \ddots & \ddots & & \vdots \\
			\vdots & & & 0 & -1 & 2 & -1 & \ddots & \vdots \\
			\vdots & & & & \ddots & \ddots & \ddots & \ddots & 0 \\
			\vdots & & & & & \ddots & \ddots & \ddots & -1 \\
			0 & \cdots & \cdots & \cdots & \cdots & \cdots & 0 & 0 & h
		\end{array}\right)\left(\begin{array}{c}
			c_{0} \\
			\vdots \\
			\vdots \\
			\vdots \\
			\vdots \\
			\vdots \\
			\vdots \\
			\vdots \\
			c_{N}
		\end{array}\right)=\left(\begin{array}{c}
			0 \\
			2 h \\
			\vdots \\
			\vdots \\
			\vdots \\
			\vdots \\
			\vdots \\
			2 h \\
			D
		\end{array}\right)
	\end{equation}

		Note that because we do not require $\varphi_{i}(0)=0$ and $\varphi_{i}(L), i \in \mathcal{I}_{s}$, the boundary term $\left[u^{\prime} v\right]_{0}^{L}$ gives in principle contributions $u^{\prime}(0) \varphi_{0}(0)$ to $b_{0}$ and $u^{\prime}(L) \varphi_{N}(L)$ to $b_{N}\left(u^{\prime} \varphi_{i}\right.$ vanishes for $x=0$ or $x=L$ for $\left.i=1, \ldots, N-1\right)$. Nevertheless, we erase these contributions in $b_{0}$ and $b_{N}$ and insert boundary values instead. This argument shows why we can drop computing $\left[u^{\prime} v\right]_{0}^{L}$ at Dirichlet nodes when we implement the Dirichlet values by modifying the linear system.
		
	\section[Symmetric modification of the linear system]{Symmetric modification of the linear system}
		\label{sec:sec_14_4}
		The original matrix system (\ref{eqa172}) is symmetric, but the modifications in (\ref{eqa184}) destroy the symmetry. Our described modification will in general destroy an initial symmetry in the matrix system. This is not a particular computational disadvantage for tridiagonal systems arising in $1 \mathrm{D}$ problems, but may be more serious in $2 \mathrm{D}$ and 3D problems when the systems are large and exploiting symmetry can be important for halving the storage demands, speeding up computations, and/or making the solution algorithm more robust. Therefore, an alternative modification which preserves symmetry is frequently applied.\smallbreak
		Let $c_{k}$ be a coefficient corresponding to a known value $u\left(x_{k}\right)=U_{k}$. We want to replace equation $k$ in the system by $c_{k}=U_{k}$, i.e., insert zeroes in row number $k$ in the coefficient matrix, set 1 on the diagonal, and replace $b_{k}$ by $U_{k}$. A symmetry-preserving modification consists in first subtracting column number $k$ in the coefficient matrix, i.e., $A_{i, k}$ for $i \in \mathcal{I}_{s}$, times the boundary value $U_{k}$, from the right-hand side: $b_{i} \leftarrow b_{i}-A_{i, k} U_{k}$. Then we put zeroes in row number $k$ and column number $k$ in the coefficient matrix, and finally set $b_{k}=U_{k}$. The steps in algorithmic form becomes
		\begin{enumerate}
			\item $b_{i} \leftarrow b_{i}-A_{i, k} U_{k}$ for $i \in \mathcal{I}_{s}$ \bigbreak
			\item $A_{i, k}=A_{k, i}=0$ for $i \in \mathcal{I}_{s}$ \bigbreak
			\item $A_{k, k}=1$ \bigbreak
			\item $b_{i}=U_{k}$ \bigbreak
		\end{enumerate}
		
		\noindent This modification goes as follows for the specific linear system written out in (\ref{eqa183}) in Section \ref{sec:sec_14_3}. First we subtract the first column in the coefficient matrix, times the boundary value, from the right-hand side. Because $c_{0}=0$, this subtraction has no effect. Then we subtract the last column, times the boundary value $D$, from the right-hand side. This action results in $b_{N-1}=2 h+D / h$ and $b_{N}=h-2 D / h$. Thereafter, we place zeros in the first and last row and column in the coefficient matrix and 1 on the two corresponding diagonal entries. Finally, we set $b_{0}=0$ and $b_{N}=D$. The result becomes
		
		\begin{equation}
			\label{eqa185}	
			\frac{1}{h}\left(\begin{array}{ccccccccc}
				h & 0 & 0 & \cdots & \cdots & \cdots & \cdots & \cdots & 0 \\
				0 & 2 & -1 & \ddots & & & & & \vdots \\
				0 & -1 & 2 & -1 & \ddots & & & & \vdots \\
				\vdots & \ddots & & \ddots & \ddots & 0 & & & \vdots \\
				\vdots & & \ddots & \ddots & \ddots & \ddots & \ddots & & \vdots \\
				\vdots & & & 0 & -1 & 2 & -1 & \ddots & \vdots \\
				\vdots & & & & \ddots & \ddots & \ddots & \ddots & 0 \\
				\vdots & & & & & \ddots & \ddots & \ddots & 0 \\
				0 & \cdots & \cdots & \cdots & \cdots & \cdots & 0 & 0 & h
			\end{array}\right)\left(\begin{array}{c}
				c_{0} \\
				\vdots \\
				\vdots \\
				\vdots \\
				\vdots \\
				\vdots \\
				\vdots \\
				\vdots \\
				c_{N}
			\end{array}\right)=\left(\begin{array}{c}
				0 \\
				2 h \\
				\vdots \\
				\vdots \\
				\vdots \\
				\vdots \\
				\vdots \\
				2 h + D/h \\
				D
			\end{array}\right)
		\end{equation}
	\bigbreak
	\section[Modification of the element matrix and vector]{Modification of the element matrix and vector}
		\noindent The modifications of the global linear system can alternatively be done for the element matrix and vector. (The assembled system will get the value $n$ on the main diagonal if $n$ elements contribute to the same unknown, but the factor $n$ will also appear on the right-hand side and hence cancel out.)
		
		We have, in the present computational example, the element matrix and vector (\ref{eqa177}). The modifications are needed in cells where one of the degrees of freedom is known. Here, this means the first and last cell. We compute the element matrix and vector as there are no Dirichlet conditions. The boundary term $\left[u^{\prime} v\right]_{0}^{L}$ is simply forgotten at nodes that have Dirichlet conditions because the modification of the element vector will anyway erase the contribution from the boundary term. In the first cell, local degree of freedom number 0 is known and the modification becomes
		\begin{equation}
			\label{eqa186}	
			\tilde{A}^{(0)}=A=\frac{1}{h}\left(\begin{array}{rr}
				h & 0 \\
				-1 & 1
			\end{array}\right), \quad \tilde{b}(0)=\left(\begin{array}{l}
				0 \\
				h
			\end{array}\right)
		\end{equation}
		In the last cell we set
		\begin{equation}
			\label{eqa187}
			\bar{A}^{\left(N_{e}\right)}=A=\frac{1}{h}\left(\begin{array}{rr}
				1 & -1 \\
				0 & h
			\end{array}\right), \quad \vec{b}^{\left(N_{c}\right)}=\left(\begin{array}{l}
				h \\
				D
			\end{array}\right)
		\end{equation}
	
		We can also perform the symmetric modification. This operation affects only the last cell with a nonzero Dirichlet condition. The algorithm is the same as for the global linear system, resulting in
		\begin{equation}
			\label{eqa188}
			\tilde{A}^{(N-1)}=A=\frac{1}{h}\left(\begin{array}{cc}
				h & 0 \\
				0 & 1
			\end{array}\right), \quad \tilde{b}^{(N-1)}=\left(\begin{array}{c}
				h+D / h \\
				D
			\end{array}\right) .
		\end{equation}
		The reader is encouraged to assemble the element matrices and vectors and check that the result coincides with the system (\ref{eqa185}).
		
\chapter{Boundary conditions: specified derivative}
\label{chap:chap_15}
\pagenumbering{arabic}

\noindent Suppose our model problem $-u^{\prime \prime}(x)=f(x)$ features the boundary conditions $u^{\prime}(0)=C$ and $u(L)=D$. As already indicated in Section \ref{chap:chap_12}, the former condition can be incorporated through the boundary term that arises from integration by parts. This details of this method will now be illustrated in the context of finite element basis functions.
	\section[Boundary conditions: specified derivative]{The variational formulation}
		\label{sec:sec_15_1}
		\noindent Starting with the Galerkin method,
		$$
		\int_{0}^{L}\left(u^{\prime \prime}(x)+f(x)\right) \psi_{i}(x) \mathrm{d} x=0, \quad i \in \mathcal{I}_{s}
		$$
		integrating $u^{\prime \prime} \psi_{i}$ by parts results in
		$$
		\int_{0}^{L} u^{\prime}(x)^{\prime} \psi_{i}^{\prime}(x) \mathrm{d} x-\left(u^{\prime}(L) \psi_{i}(L)-u^{\prime}(0) \psi_{i}(0)\right)=\int_{0}^{L} f(x) \psi_{i}(x) \mathrm{d} x, \quad i \in \mathcal{I}_{s}
		$$\smallbreak
		The first boundary term, $u^{\prime}(L) \psi_{i}(L)$, vanishes because $u(L)=D$. There are two arguments for this result, explained in detail below. The second boundary term, $u^{\prime}(0) \psi_{i}(0)$, can be used to implement the condition $u^{\prime}(0)=C$, provided $\psi_{i}(0) \neq 0$ for some $i$ (but with finite elements we fortunately have $\psi_{0}(0)=1$ ). The variational form of the differential equation then becomes
		$$
		\int_{0}^{L} u^{\prime}(x) \varphi_{i}^{\prime}(x) \mathrm{d} x+C \varphi_{i}(0)=\int_{0}^{L} f(x) \varphi_{i}(x) \mathrm{d} x, \quad i \in \mathcal{I}_{s}
		$$
		\bigbreak
		
	\section[Boundary conditions: specified derivative]{Boundary term vanishes because of the test functions}
		\label{sec:sec_15_2}
		\noindent At points where $u$ is known we may require $\psi_{i}$ to vanish. Here, $u(L)=D$ and then $\psi_{i}(L)=0, i \in \mathcal{I}_{s}$. Obviously, the boundary term $u^{\prime}(L) \psi_{i}(L)$ then vanishes.\smallbreak
		The set of basis functions $\left\{\psi_{i}\right\}_{i \in \mathcal{I}_{s}}$ contains in this case all the finite element basis functions on the mesh, expect the one that is 1 at $x=L$.
		The basis function that is left out is used in a boundary function $B(x)$ instead. With a left-to-right numbering, $\psi_{i}=\varphi_{i}, i=0, \ldots, N_{n}-1$, and $B(x)=D \varphi_{N_{n}}$ :
		$$
		u(x)=D \varphi_{N_{n}}(x)+\sum_{j=0}^{N=N_{n}-1} c_{j} \varphi_{j}(x) .
		$$
		
		Inserting this expansion for $u$ in the variational form (15.1) leads to the linear system
	
		\begin{equation}
			\label{eqa189}
			\sum_{j=0}^{N}\left(\int_{0}^{L} \varphi_{i}^{\prime}(x) \varphi_{j}^{\prime}(x) \mathrm{d} 	x\right) c_{j}=\int_{0}^{L}\left(f(x) \varphi_{i}(x)-D \varphi_{N_{n}}^{\prime}(x) \varphi_{i}(x)\right) \mathrm{d} x-C \varphi_{i}(0)
		\end{equation}
	
		for $i=0, \ldots, N=N_{n}-1$ \bigbreak 
		
	\section[Boundary conditions: specified derivative]{Boundary term vanishes because of linear system modifications}	
		\label{sec:sec_15_3}
		\noindent We may, as an alternative to the approach in the previous section, use a basis $\left\{\psi_{i}\right\}_{i \in \mathcal{I}_{s}}$ which contains all the finite element functions on the mesh: $\psi_{i}=\varphi_{i}$, $i=0, \ldots, N_{n}=N$. In this case, $u^{\prime}(L) \psi_{i}(L)=u^{\prime}(L) \varphi_{i}(L) \neq 0$ for the $i$ corresponding to the boundary node at $x=L$ (where $\varphi_{i}=1$ ). The number of this node is $i=N_{n}=N$ if a left-to-right numbering of nodes is utilized.
		
		However, even though $u^{\prime}(L) \varphi_{N}(L) \neq 0$, we do not need to compute this term. For $i<N$ we realize that $\varphi_{i}(L)=0$. The only nonzero contribution to the right-hand side from the affects $b_{N}(i=N)$. Without a boundary function we must implement the condition $u(L)=D$ by the equivalent statement $c_{N}=D$ and modify the linear system accordingly. This modification will earse the last row and replace $b_{N}$ by another value. Any attempt to compute the boundary term $u^{\prime}(L) \varphi_{N}(L)$ and store it in $b_{N}$ will be lost. Therefore, we can safely forget about boundary terms corresponding to Dirichlet boundary conditions also when we use the methods from Section \ref{sec:sec_14_3} or Section \ref{sec:sec_14_4}.\smallbreak
		The expansion for $u$ reads
		$$
		u(x)=\sum_{j \in \mathcal{I}_{s}} c_{j} \varphi_{j}(x), \quad B(x)=D \varphi_{N}(x),
		$$
		with $N=N_{n}$. Insertion in the variational form (\ref{sec:sec_15_1}) leads to the linear system
	
		\begin{equation}
			\label{eqa190}
			\sum_{j \in \mathcal{I}_{s}}\left(\int_{0}^{L} \varphi_{i}^{\prime}(x) \varphi_{j}^{\prime}(x) \mathrm{d} x\right) c_{j}=\int_{0}^{L}\left(f(x) \varphi_{i}(x)\right) \mathrm{d} x-C \varphi_{i}(0), \quad i \in 	\mathcal{I}_{s}
		\end{equation}
	
		\noindent After having computed the system, we replace the last row by $c_{N}=D$, either straightforwardly as in Section refreffem:deq:1D:fem:essBC:Bfunc:modsys or in a symmetric fashion as in Section refreffem:deq:1D:fem:essBC:Bfunc:modsys:symm. These modifications can also be performed in the element matrix and vector for the right-most cell.
	
	\section[Direct computation of the global linear system]{Direct computation of the global linear system}
		\label{sec:sec_15_4}
		We now turn to actual computations with P1 finite elements. The focus is on how the linear system and the element matrices and vectors are modified by the condition $u^{\prime}(0)=C$.
		
		Consider first the approach where Dirichlet conditions are incorporated by a $B(x)$ function and the known degree of freedom $C_{N_{n}}$ is left out from the linear system (see Section \ref{sec:sec_15_2}). The relevant formula for the linear system is given by (\ref{eqa189}). There are three differences compared to the extensively computed case where $u(0)=0$ in Sections \ref{sec:sec_13_2} and \ref{sec:sec_13_4}. First, because we do not have a
		Dirichlet condition at the left boundary, we need to extend the linear system (\ref{eqa172}) with an equation associated with the node $x_{0}=0$. According to Section \ref{sec:sec_14_3}, this extension consists of including $A_{0,0}=1 / h, A_{0,1}=-1 / h$, and $b_{0}=h$. For $i>0$ we have $A_{i, i}=2 / h, A_{i-1, i}=A_{i, i+1}=-1 / h$. Second, we need to include the extra term $-C \varphi_{i}(0)$ on the right-hand side. Since all $\varphi_{i}(0)=0$ for $i=1, \ldots, N$, this term reduces to $-C \varphi_{0}(0)=-C$ and affects only the first equation $(i=0)$. We simply add $-C$ to $b_{0}$ such that $b_{0}=h-C$. Third, the boundary term $-\int_{0}^{L} D \varphi_{N_{n}}(x) \varphi_{i} \mathrm{~d} x$ must be computed. Since $i=0, \ldots, N=N_{n}-1$, this integral can only get a nonzero contribution with $i=N_{n}-1$ over the last cell. The result becomes $-D h / 6$. The resulting linear system can be summarized in the form
		
	\begin{equation}
		\label{eqa191}
		\frac{1}{h}\left(\begin{array}{ccccccccc}
			1 & -1 & 0 & \cdots & \cdots & \cdots & \cdots & \cdots & 0 \\
			-1 & 2 & -1 & \ddots & & & & & \vdots \\
			0 & -1 & 2 & -1 & \ddots & & & & \vdots \\
			\vdots & \ddots & & \ddots & \ddots & 0 & & & \vdots \\
			\vdots & & \ddots & \ddots & \ddots & \ddots & \ddots & & \vdots \\
			\vdots & & & 0 & -1 & 2 & -1 & \ddots & \vdots \\
			\vdots & & & & \ddots & \ddots & \ddots & \ddots & 0 \\
			\vdots & & & & & \ddots & \ddots & \ddots & -1 \\
			0 & \cdots & \cdots & \cdots & \cdots & \cdots & 0 & -1 & 2
		\end{array}\right)\left(\begin{array}{c}
			c_{0} \\
			\vdots \\
			\vdots \\
			\vdots \\
			\vdots \\
			\vdots \\
			\vdots \\
			\vdots \\
			c_{N}
		\end{array}\right)=\left(\begin{array}{c}
			h-C \\
			2 h \\
			\vdots \\
			\vdots \\
			\vdots \\
			\vdots \\
			\vdots \\
			\vdots \\
			2h -Dh/6
		\end{array}\right)
	\end{equation}
		
		Next we consider the technique where we modify the linear system to incorporate Dirichlet conditions (see Section \ref{sec:sec_15_3}). Now $N=N_{n}$. The two differences from the case above is that the $-\int_{0}^{L} D \varphi_{N_{n}} \varphi_{i} \mathrm{~d} x$ term is left out of the right-hand side and an extra last row associated with the node $x_{N_{n}}=L$ where the Dirichlet condition applies is appended to the system. This last row is anyway replaced by the condition $C_{N}=D$ or this condition can be incorporated in a symmetric fashion. Using the simplest, former approach gives
		
		\begin{equation}
			\label{eqa192}
			\frac{1}{h}\left(\begin{array}{ccccccccc}
				1 & -1 & 0 & \cdots & \cdots & \cdots & \cdots & \cdots & 0 \\
				-1 & 2 & -1 & \ddots & & & & & \vdots \\
				0 & -1 & 2 & -1 & \ddots & & & & \vdots \\
				\vdots & \ddots & & \ddots & \ddots & 0 & & & \vdots \\
				\vdots & & \ddots & \ddots & \ddots & \ddots & \ddots & & \vdots \\
				\vdots & & & 0 & -1 & 2 & -1 & \ddots & \vdots \\
				\vdots & & & & \ddots & \ddots & \ddots & \ddots & 0 \\
				\vdots & & & & & \ddots & -1 & 2 & -1 \\
				0 & \cdots & \cdots & \cdots & \cdots & \cdots & 0 & 0 & 1
			\end{array}\right)\left(\begin{array}{c}
				c_{0} \\
				\vdots \\
				\vdots \\
				\vdots \\
				\vdots \\
				\vdots \\
				\vdots \\
				\vdots \\
				c_{N}
			\end{array}\right)=\left(\begin{array}{c}
				h-C \\
				2 h \\
				\vdots \\
				\vdots \\
				\vdots \\
				\vdots \\
				\vdots \\
				2 h \\
				D
			\end{array}\right)
		\end{equation}
	
	\section[Cellwise computations]{Cellwise computations}
		\label{sec:sec_15_5}
		\noindent Now we compute with one element at a time, working in the reference coordinate system $X \in[-1,1]$. We need to see how the $u^{\prime}(0)=C$ condition affects the element matrix and vector. The extra term $-C \varphi_{i}(0)$ in the variational formulation only affects the element vector in the first cell. On the reference cell, $-C \varphi_{i}(0)$ is transformed to $-C \tilde{\varphi}_{r}(-1)$, where $r$ counts local degrees of freedom. We have $\tilde{\varphi}_{0}(-1)=1$ and $\tilde{\varphi}_{1}(-1)=0$ so we are left with the contribution $-C \tilde{\varphi}_{0}(-1)=-C$ to $\tilde{b}_{0}^{(0)}$ :

		\begin{equation}
			\label{eqa193}
			\tilde{A}^{(0)}=A=\frac{1}{h}\left(\begin{array}{rr}
				1 & 1 \\
				-1 & 1
			\end{array}\right), \quad \tilde{b}^{(0)}=\left(\begin{array}{c}
				h-C \\
				h
			\end{array}\right)
		\end{equation}
	
		\noindent No other element matrices or vectors are affected by the $-C \varphi_{i}(0)$ boundary term.
		
		There are two alternative ways of incorporating the Dirichlet condition. Following Section \ref{sec:sec_15_2}, we get a $1 \times 1$ element matrix in the last cell and an element vector with an extra term containing $D$ :
	
		\begin{equation}
			\label{eqa194}
			\tilde{A}^{(e)}=\frac{1}{h}(1), \quad \tilde{b}^{(e)}=h(1-D / 6),
		\end{equation}
	
		Alternatively, we include the degree of freedom at the node with $u$ specified. The element matrix and vector must then be modified to constrain the $\tilde{c}_{1}=c_{N}$ value at local node $r=1$ :
		\begin{equation}
			\label{eqa195}
			\tilde{A}^{\left(N_{c}\right)}=A=\frac{1}{h}\left(\begin{array}{ll}
				1 & 1 \\
				0 & 1
			\end{array}\right), \quad \tilde{b}^{\left(N_{\epsilon}\right)}=\left(\begin{array}{c}
				h \\
				D
			\end{array}\right)
		\end{equation}
	
\chapter{Implementation}
\label{chap:chap_16}
\pagenumbering{arabic}

\noindent It is tempting to create a program with symbolic calculations to perform all the steps in the computational machinery, both for automating the work and for documenting the complete algorithms. As we have seen, there are quite many details involved with finite element computations and incorporation of boundary conditions. An implementation will also act as a structured summary of all these details.

	\section[Implementation]{Global basis functions}
		\label{sec:sec_16_1}
		\noindent We first consider implementations when $\psi_{i}$ are global functions are hence different from zero on most of $\Omega=[0, L]$ so all integrals need integration over the entire domain. Since the expressions for the entries in the linear system depend on the differential equation problem being solved, the user must supply the necessary formulas via Python functions. The implementations here attempt to perform symbolic calculations, but fall back on numerical computations if the symbolic ones fail.
		
		The user must prepare a function \textbf{\texttt{integrand\_lhs(psi, i, j)}} for returning the integrand of the integral that contributes to matrix entry $(i, j)$. The psi variable is a Python dictionary holding the basis functions and their derivatives in symbolic form. More precisely, \textbf{\texttt{psi[q]}} is a list of
		$$
		\left\{\frac{d^{q} \psi_{0}}{d x^{q}}, \ldots, \frac{d^{q} \psi_{N}}{d x^{q}}\right\} .
		$$
		Similarly, \textbf{\texttt{integrand\_rhs(psi, i)}} returns the integrand for entry number $i$ in the right-hand side vector.
		
		Since we also have contributions to the right-hand side vector, and potentially also the matrix, from boundary terms without any integral, we introduce two additional functions, \textbf{\texttt{boundary\_lhs(psi, i, j)}} and \textbf{\texttt{boundary\_rhs(psi, i)}} for returning terms in the variational formulation that are not to be integrated over the domain $\Omega$. Examples shown later will explain in more detail how these user-supplied function may look like.
		
		The linear system can be computed and solved symbolically by the following function:
		\begin{lstlisting}[numbers=none]
			import sympy as sp
			def solve(integrand_lhs, integrand_rhs, psi, Omega,
					boundary_lhs=None, boundary_rhs=None):
				N = len(psi[0]) - 1
				A = sp.zeros((N+1, N+1))
				b = sp.zeros((N+1, 1))
				x = sp.Symbol('x')
				for i in range(N+1):
					for j in range(i, N+1):
						integrand = integrand_lhs(psi, i, j)
						I = sp.integrate(integrand, (x, Omega[0], Omega[1]))
						if boundary_lhs is not None:
							I += boundary_lhs(psi, i, j)
						A[i,j] = A[j,i] = I # assume symmetry
					integrand = integrand_rhs(psi, i)
					I = sp.integrate(integrand, (x, Omega[0], Omega[1]))
					if boundary_rhs is not None:
						I += boundary_rhs(psi, i)
					b[i,0] = I
				c = A.LUsolve(b)
				u = sum(c[i,0]*psi[0][i] for i in range(len(psi[0])))
				return u
		\end{lstlisting}\smallbreak
		
		Not surprisingly, symbolic solution of differential equations, discretized by a Galerkin or least squares method with global basis functions, is of limited interest beyond the simplest problems, because symbolic integration might be very time consuming or impossible, not only in \textbf{\texttt{sympy}} but also in \href{https://www.wolframalpha.com/}{WolframAlpha} (which applies the perhaps most powerful symbolic integration software available today: Mathematica). Numerical integration as an option is therefore desirable.\smallbreak
		The extended \textbf{\texttt{solve}} function below tries to combine symbolic and numerical integration. The latter can be enforced by the user, or it can be invoked after a non-successful symbolic integration (being detected by an \textbf{\texttt{Integral}} object as the result of the integration in \textbf{\texttt{sympy}}). Note that for a numerical integration, symbolic expressions must be converted to Python functions (using \textbf{\texttt{lambdify}}), and the expressions cannot contain other symbols than $\mathrm{x}$. The real \textbf{\texttt{solve}} routine in the \textbf{\texttt{\href{https://github.com/hplgit/INF5620/blob/master/src/fem/varform1D.py}{varform1D.py}}} file has error checking and meaningful error messages in such cases. The \textbf{\texttt{solve}} code below is a condensed version of the real one, with the purpose of showing how to automate the Galerkin or least squares method for solving differential equations in $1 \mathrm{D}$ with global basis functions:
		\begin{lstlisting}[numbers=none]
			def solve(integrand_lhs, integrand_rhs, psi, Omega,
				boundary_lhs=None, boundary_rhs=None, symbolic=True):
			N = len(psi[0]) - 1
			A = sp.zeros((N+1, N+1))
			b = sp.zeros((N+1, 1))
			x = sp.Symbol('x')
			for i in range(N+1):
				for j in range(i, N+1):
					integrand = integrand_lhs(psi, i, j)
					if symbolic:
						I = sp.integrate(integrand, (x, Omega[0], Omega[1]))
						if isinstance(I, sp.Integral):
							symbolic = False # force num.int. hereafter
					if not symbolic:
						integrand = sp.lambdify([x], integrand)
						I = sp.mpmath.quad(integrand, [Omega[0], Omega[1]])
					if boundary_lhs is not None:
						I += boundary_lhs(psi, i, j)
					A[i,j] = A[j,i] = I
				integrand = integrand_rhs(psi, i)
				if symbolic:
					I = sp.integrate(integrand, (x, Omega[0], Omega[1]))
					if isinstance(I, sp.Integral):
						symbolic = False
				if not symbolic:
					integrand = sp.lambdify([x], integrand)
					I = sp.mpmath.quad(integrand, [Omega[0], Omega[1]])
				if boundary_rhs is not None:
					I += boundary_rhs(psi, i)
				b[i,0] = I
			c = A.LUsolve(b)
			u = sum(c[i,0]*psi[0][i] for i in range(len(psi[0])))
			return u
		\end{lstlisting}
	\section[Example: constant right-hand side]{Example: constant right-hand side}
		\label{sec:sec_16_2}
		\noindent To demonstrate the code above, we address
		$$
		-u^{\prime \prime}(x)=b, \quad x \in \Omega=[0,1], \quad u(0)=1, u(1)=0,
		$$
		with $b$ as a (symbolic) constant. A possible basis for the space $V$ is $\psi_{i}(x)=$ $x^{i+1}(1-x), i \in \mathcal{I}_{s}$. Note that $\psi_{i}(0)=\psi_{i}(1)=0$ as required by the Dirichlet conditions. We need a $B(x)$ function to take care of the known boundary values of $u$. Any function $B(x)=1-x^{p}, p \in \mathbb{R}$, is a candidate, and one arbitrary choice from this family is $B(x)=1-x^{3}$. The unknown function is then written as
		$$
		u(x)=B(x)+\sum_{j \in \mathcal{I}_{s}} c_{j} \psi_{j}(x)
		$$\smallbreak
		Let us use the Galerkin method to derive the variational formulation. Multiplying the differential equation by $v$ and integrate by parts yield
		$$
		\int_{0}^{1} u^{\prime} v^{\prime} \mathrm{d} x=\int_{0}^{1} f v \mathrm{~d} x \quad \forall v \in V,
		$$
		and with $u=B+\sum_{j} c_{j} \psi_{j}$ we get the linear system
		
		\begin{equation}
			\label{eqa196}
			\sum_{j \in \mathcal{I}_{s}}\left(\int_{0}^{1} \psi_{i}^{\prime} \psi_{j}^{\prime} \mathrm{d} x\right) c_{j}=\int_{0}^{1}\left(f-B^{\prime}\right) \psi_{i} \mathrm{~d} x, \quad i \in \mathcal{I}_{s}
		\end{equation}
		
		The application can be coded as follows in \textbf{\texttt{sympy}}:
		
		\begin{lstlisting}[numbers=none]
			x, b = sp.symbols('x b')
			f = b
			B = 1 - x**3
			dBdx = sp.diff(B, x)
			
			# Compute basis functions and their derivatives
			N = 3
			psi = {0: [x**(i+1)*(1-x) for i in range(N+1)]}
			psi[1] = [sp.diff(psi_i, x) for psi_i in psi[0]]
			
			def integrand_lhs(psi, i, j):
				return psi[1][i]*psi[1][j]
				
			def integrand_rhs(psi, i):
				return f*psi[0][i] - dBdx*psi[1][i]
				
			Omega = [0, 1]
			
			u_bar = solve(integrand_lhs, integrand_rhs, psi, Omega,
			verbose=True, symbolic=True)
			
			u = B + u_bar
			print 'solution u:', sp.simplify(sp.expand(u))
		\end{lstlisting}
	
	\noindent The printout of $\mathrm{u}$ reads \textbf{\texttt{-b*x**2/2 + b*x/2 - x + 1}}. Note that expanding $\mathrm{u}$ and then simplifying is in the present case necessary to get a compact, final expression with sympy. A non-expanded u might be preferable in other cases this depends on the problem in question.\smallbreak
	The exact solution $u_{e}(x)$ can be derived by some \textbf{\texttt{sympy}} code that closely follows the examples in Section \ref{sec:sec_11_2}. The idea is to integrate $-u^{\prime \prime}=b$ twice and determine the integration constants from the boundary conditions:

		\begin{lstlisting}[numbers=none]
			C1, C2 = sp.symbols('C1 C2') # integration constants
			f1 = sp.integrate(f, x) + C1
			f2 = sp.integrate(f1, x) + C2
			# Find C1 and C2 from the boundary conditions u(0)=0, u(1)=1
			s = sp.solve([u_e.subs(x,0) - 1, u_e.subs(x,1) - 0], [C1, C2])
			# Form the exact solution
			u_e = -f2 + s[C1]*x + s[C2]
			print 'analytical solution:', u_e
			print 'error:', sp.simplify(sp.expand(u - u_e))
		\end{lstlisting}
	
	
	\noindent The last line prints 0 , which is not surprising when $u_{\mathrm{e}}(x)$ is a parabola and our approximate $u$ contains polynomials up to degree 4. It suffices to have $N=1$, i.e., polynomials of degree 2 , to recover the exact solution.
	
	We can play around with the code and test that with $f \sim x^{p}$, the solution is a polynomial of degree $p+2$, and $N=p+1$ guarantees that the approximate solution is exact.
	
	Although the symbolic code is capable of integrating many choices of $f(x)$, the symbolic expressions for $u$ quickly become lengthy and non-informative, so numerical integration in the code, and hence numerical answers, have the greatest application potential.\bigbreak
	
	\section[Finite elements]{Finite elements}
		\label{sec:sec_16_3}
		 \noindent Implementation of the finite element algorithms for differential equations follows closely the algorithm for approximation of functions. The new additional ingredients are
		 \begin{enumerate}
		 	\item other types of integrands (as implied by the variational formulation)
		  	\item additional boundary terms in the variational formulation for Neumann boundary conditions
		 	\item modification of element matrices and vectors due to Dirichlet boundary conditions
		\end{enumerate}
		 
		 Point 1 and 2 can be taken care of by letting the user supply functions defining the integrands and boundary terms on the left- and right-hand side of the equation system:
		 
		\begin{lstlisting}[numbers=none]
			integrand_lhs(phi, r, s, x)
			boundary_lhs(phi, r, s, x)
			integrand_rhs(phi, r, x)
			boundary_rhs(phi, r, x)
		\end{lstlisting}
	 
	 	\noindent Here, \textbf{\texttt{phi}} is a dictionary where \textbf{\texttt{phi[q]}} holds a list of the derivatives of order $q$ of the basis functions at the an evaluation point; \textbf{\texttt{r}} and \textbf{\texttt{s}} are indices for the corresponding entries in the element matrix and vector, and \textbf{\texttt{x}} is the global coordinate value corresponding to the current evaluation point.
	 	
	 	Given a mesh represented by \textbf{\texttt{vertices}}, \textbf{\texttt{cells}}, and \textbf{\texttt{dof\_map}} as explained before, we can write a pseudo Python code to list all the steps in the computational algorithm for finite element solution of a differential equation.
	 	
	 	\begin{lstlisting}[numbers=none]
			<Declare global matrix and rhs: A, b>
			
			for e in range(len(cells)):
			
				# Compute element matrix and vector
				n = len(dof_map[e]) # no of dofs in this element
				h = vertices[cells[e][1]] - vertices[cells[e][1]]
				<Declare element matrix and vector: A_e, b_e>
			
				# Integrate over the reference cell
				points, weights = <numerical integration rule>
				for X, w in zip(points, weights):
					phi = <basis functions and derivatives at X>
					detJ = h/2
					x = <affine mapping from X>
					for r in range(n):
						for s in range(n):
							A_e[r,s] += integrand_lhs(phi, r, s, x)*detJ*w
						b_e[r] += integrand_rhs(phi, r, x)*detJ*w
			
				# Add boundary terms
				for r in range(n):
					for s in range(n):
						A_e[r,s] += boundary_lhs(phi, r, s, x)*detJ*w
					b_e[r] += boundary_rhs(phi, r, x)*detJ*w
			
				# Incorporate essential boundary conditions
				for r in range(n):
					global_dof = dof_map[e][r]
					if global_dof in essbc_dofs:
						# dof r is subject to an essential condition
					value = essbc_docs[global_dof]
					# Symmetric modification
					b_e -= value*A_e[:,r]
					A_e[r,:] = 0
					A_e[:,r] = 0
					A_e[r,r] = 1
					b_e[r] = value
			
				# Assemble
				for r in range(n):
					for s in range(n):
						A[dof_map[e][r], dof_map[e][r]] += A_e[r,s]
					b[dof_map[e][r] += b_e[r]
			<solve linear system>
		\end{lstlisting}
\chapter{Variational formulations in 2D and 3D}
\label{chap:chap_17}
\pagenumbering{arabic}
	\noindent The major difference between deriving variational formulations in $2 \mathrm{D}$ and $3 \mathrm{D}$ compared to $1 \mathrm{D}$ is the rule for integrating by parts. A typical second-order term in a PDE may be written in dimension-independent notation as
	$$
	\nabla^{2} u \text { or } \nabla \cdot(a(\boldsymbol{x}) \nabla u) .
	$$
	The explicit forms in a 2D problem become
	$$
	\nabla^{2} u=\nabla \cdot \nabla u=\frac{\partial^{2} u}{\partial x^{2}}+\frac{\partial^{2} u}{\partial y^{2}},
	$$
	and
	$$
	\nabla \cdot(a(\boldsymbol{x}) \nabla u)=\frac{\partial}{\partial x}\left(a(x, y) \frac{\partial u}{\partial x}\right)+\frac{\partial}{\partial y}\left(a(x, y) \frac{\partial u}{\partial y}\right) .
	$$
	We shall continue with the latter operator as the form arises from just setting $a=1$.
	
	The general rule for integrating by parts is often referred to as \href{https://en.wikipedia.org/wiki/Green's_identities}{Green's first identity}:
	
	\begin{equation}
		\label{eqa197}
		-\int_{\Omega} \nabla \cdot(a(\boldsymbol{x}) \nabla u) v \mathrm{~d} x=\int_{\Omega} a(\boldsymbol{x}) \nabla u \cdot \nabla v \mathrm{~d} x-\int_{\partial \Omega} a \frac{\partial u}{\partial n} v \mathrm{~d} s,
	\end{equation}

	where $\partial \Omega$ is the boundary of $\Omega$ and $\partial u / \partial n=n \cdot \nabla u$ is the derivative of $u$ in the outward normal direction, $\boldsymbol{n}$ being an outward unit normal to $\partial \Omega$. The integrals $\int_{\Omega}() \mathrm{d} x$ are area integrals in $2 \mathrm{D}$ and volume integrals in $3 \mathrm{D}$, while $\int_{\partial \Omega}() \mathrm{d} s$ is a line integral in 2D and a surface integral in 3D. \smallbreak
	Let us divide the boundary into two parts:
	\begin{itemize}
		\item $\partial \Omega_{N}$, where we have Neumann conditions $-a \frac{\partial u}{\partial n}=g$, and
		\item $\partial \Omega_{D}$, where we have Dirichlet conditions $u=u_{0}$.
	\end{itemize} \smallbreak
	\noindent The test functions $v$ are required to vanish on $\partial \Omega_{D}$.
	
	\textbf{Example.   } Here is a quite general, stationary, linear PDE arising in many problems:
	
	\begin{equation}
		\label{eqa198}
		v \cdot \nabla u+\alpha u=\nabla \cdot(a \nabla u)+f, \quad x \in \Omega \\
	\end{equation}

	\begin{equation}
		\label{eqa199}
		u=u_{0}, \quad x \in \partial \Omega_{D} \\
	\end{equation}

	\begin{equation}
		\label{eqa200}
		-a \frac{\partial u}{\partial n}=g, \quad x \in \partial \Omega_{N}
	\end{equation}
	
	\noindent The vector field $v$ and the scalar functions $a, \alpha, f, u_{0}$, and $g$ may vary with the spatial coordinate $\boldsymbol{x}$ and must be known.
	
	Such a second-order PDE needs exactly one boundary condition at each point of the boundary, so $\partial \Omega_{N} \cup \partial \Omega_{D}$ must be the complete boundary $\partial \Omega$.
	
	Assume that the boundary function $u_{0}(\boldsymbol{x})$ is defined for all $\boldsymbol{x} \in \Omega$. The unknown function can then be expanded as
	$$
	u=B+\sum_{j \in \mathcal{I}_{*}} c_{j} \psi_{j}, \quad B=u_{0}
	$$
	The variational formula is obtained from Galerkin's method, which technically implies multiplying the PDE by a test function $v$ and integrating over $\Omega$ :
	$$
	\int_{\Omega}(v \cdot \nabla u+\alpha u) v \mathrm{~d} x=\int_{\Omega} \nabla \cdot(a \nabla u) \mathrm{d} x+\int_{\Omega} f v \mathrm{~d} x .
	$$
	The second-order term is integrated by parts, according to
	$$
	\int_{\Omega} \nabla \cdot(a \nabla u) v \mathrm{~d} x=-\int_{\Omega} a \nabla u \cdot \nabla v \mathrm{~d} x+\int_{\partial \Omega} a \frac{\partial u}{\partial n} v \mathrm{~d} s .
	$$
	The variational form now reads
	$$
	\int_{\Omega}(v \cdot \nabla u+\alpha u) v \mathrm{~d} x=-\int_{\Omega} a \nabla u \cdot \nabla v \mathrm{~d} x+\int_{\partial \Omega} a \frac{\partial u}{\partial n} v \mathrm{~d} s+\int_{\Omega} f v \mathrm{~d} x .
	$$
	The boundary term can be developed further by noticing that $v \neq 0$ only on $\partial \Omega_{N}$,
	$$
	\int_{\partial \Omega} a \frac{\partial u}{\partial n} v \mathrm{~d} s=\int_{\partial \Omega_{N}} a \frac{\partial u}{\partial n} v \mathrm{~d} s,
	$$
	and that on $\partial \Omega_{N}$, we have the condition $a \frac{\partial u}{\partial n}=-g$, so the term becomes
	$$
	-\int_{\partial \Omega_{N}} g v \mathrm{~d} s
	$$
	The variational form is then
	$$
	\int_{\Omega}(\boldsymbol{v} \cdot \nabla u+\alpha u) v \mathrm{~d} x=-\int_{\Omega} a \nabla u \cdot \nabla v \mathrm{~d} x-\int_{\partial \Omega_{N}} g v \mathrm{~d} s+\int_{\Omega} f v \mathrm{~d} x
	$$
	
	Instead of using the integral signs we may use the inner product notation:
	$$
	(v \cdot \nabla u, v)+(\alpha u, v)=-(a \nabla u, \nabla v)-(g, v)_{N}+(f, v) .
	$$
	The subscript $N$ in $(g, v)_{N}$ is a notation for a line or surface integral over $\partial \Omega_{N}$.\smallbreak Inserting the $u$ expansion results in
	$$
	\begin{aligned}
		\sum_{j \in I_{x}}\left(\left(\boldsymbol{v} \cdot \nabla \psi_{j}, \psi_{i}\right)+\left(\alpha \psi_{j}, \psi_{i}\right)+\left(a \nabla \psi_{j}, \nabla \psi_{i}\right)\right) c_{j}=\\
		\left(g, \psi_{i}\right)_{N}+\left(f, \psi_{i}\right)-\left(\boldsymbol{v} \cdot \nabla u_{0}, \psi_{i}\right)+\left(\alpha u_{0}, \psi_{i}\right)+\left(a \nabla u_{0}, \nabla \psi_{i}\right)
	\end{aligned}
	$$
	This is a linear system with matrix entries
	$$
	A_{i, j}=\left(v \cdot \nabla \psi_{j}, \psi_{i}\right)+\left(\alpha \psi_{j,} \psi_{i}\right)+\left(a \nabla \psi_{j}, \nabla \psi_{i}\right)
	$$
	and right-hand side entries
	$$
	b_{i}=\left(g, \psi_{i}\right)_{N}+\left(f, \psi_{i}\right)-\left(v \cdot \nabla u_{0}, \psi_{i}\right)+\left(\alpha u_{0}, \psi_{i}\right)+\left(a \nabla u_{0}, \nabla \psi_{i}\right)
	$$
	for $i, j \in \mathcal{I}_{8}$.\smallbreak
	In the finite element method, we usually express $u_{0}$ in terms of basis functions and restrict $i$ and $j$ to run over the degrees of freedom that are not prescribed as Dirichlet conditions. However, we can also keep all the $c_{j}, j \in \mathcal{I}_{s}$, as unknowns drop the $u_{0}$ in the expansion for $u$, and incorporate all the known $c_{j}$ values in the linear system. This has been explained in detail in the 1D case.\bigbreak 
	
	\section[Transformation to a reference cell in 2D and 3D]{Transformation to a reference cell in 2D and 3D}
		\label{sec:sec_17_1}
	 
		\noindent We consider an integral of the type
		
		\begin{equation}
			\label{eqa201}
			\int_{\left.\Omega^{(} \Leftrightarrow\right)} a(\boldsymbol{x}) \nabla \varphi_{i} \cdot \nabla \varphi_{j} \mathrm{~d} x
		\end{equation}
	
		\noindent Where the $\varphi_{i}$ functions are finite element basis functions in $2 \mathrm{D}$ or $3 \mathrm{D}$, defined in the physical domain. Suppose we want to calculate this integral over a reference cell, denoted by $\Omega^{r}$, in a coordinate system with coordinates $\boldsymbol{X}=\left(X_{0}, X_{1}\right)(2 \mathrm{D})$ or $\boldsymbol{X}=\left(X_{0}, X_{1}, X_{2}\right)$ (3D). The mapping between a point $\boldsymbol{X}$ in the reference coordinate system and the corresponding point $\boldsymbol{x}$ in the physical coordinate system is given by a vector relation $\boldsymbol{x}(\boldsymbol{X})$. The corresponding Jacobian, $J$, of this mapping has entries
		$$
		J_{i, j}=\frac{\partial x_{j}}{\partial X_{i}}
		$$
		The change of variables requires $\mathrm{d} x$ to be replaced by det $J \mathrm{~d} X$. The derivatives in the $\nabla$ operator in the variational form are with respect to $\boldsymbol{x}$, which we may denote by $\nabla_{\boldsymbol{x}}$. The $\varphi_{i}(\boldsymbol{x})$ functions in the integral are replaced by local basis functions $\tilde{\varphi}_{r}(\boldsymbol{X})$ so the integral features $\nabla_{\boldsymbol{x}} \tilde{\varphi}_{r}(\boldsymbol{X})$. We readily have
		$\nabla_{\boldsymbol{X}} \tilde{\varphi}_{r}(\boldsymbol{X})$ from formulas for the basis functions in the reference cell, but the desired quantity $\nabla_{\boldsymbol{x}} \tilde{\varphi}_{r}(\boldsymbol{X})$ requires some efforts to compute. All the details are provided below.\smallbreak
		Let $i=q(e, r)$ and consider two space dimensions. By the chain rule,
		$$
		\frac{\partial \tilde{\varphi}_{r}}{\partial X}=\frac{\partial \varphi_{i}}{\partial X}=\frac{\partial \varphi_{i}}{\partial x} \frac{\partial x}{\partial X}+\frac{\partial \varphi_{i}}{\partial y} \frac{\partial y}{\partial X},
		$$
		and
		$$
		\frac{\partial \tilde{\varphi}_{r}}{\partial Y}=\frac{\partial \varphi_{i}}{\partial Y}=\frac{\partial \varphi_{i}}{\partial x} \frac{\partial x}{\partial Y}+\frac{\partial \varphi_{i}}{\partial y} \frac{\partial y}{\partial Y} .
		$$
		We can write these two equations as a vector equation
		$$
		\left[\begin{array}{l}
			\frac{\partial \bar{\varphi}_{r}}{\partial X} \\
			\frac{\partial \varphi_{r}}{\partial Y}
		\end{array}\right]=\left[\begin{array}{ll}
			\frac{\partial x}{\partial X} & \frac{\partial y}{\partial X} \\
			\frac{\partial x}{\partial Y} & \frac{\partial y}{\partial Y}
		\end{array}\right]\left[\begin{array}{l}
			\frac{\partial \varphi_{i}}{\partial x} \\
			\frac{\partial \varphi_{i}}{\partial y}
		\end{array}\right]
		$$
		Identifying
		$$
		\nabla_{\boldsymbol{X}} \tilde{\varphi}_{r}=\left[\begin{array}{l}
			\frac{\partial \bar{\varphi}_{r}}{\partial X} \\
			\frac{\partial \varphi_{r}}{\partial Y}
		\end{array}\right], \quad J=\left[\begin{array}{ll}
			\frac{\partial x}{\partial X} & \frac{\partial y}{\partial X} \\
			\frac{\partial x}{\partial Y} & \frac{\partial y}{\partial Y}
		\end{array}\right], \quad \nabla_{\boldsymbol{x} \varphi_{r}}=\left[\begin{array}{c}
			\frac{\partial \varphi_{i}}{\partial x} \\
			\frac{\partial \varphi_{i}}{\partial y}
		\end{array}\right],
		$$
		we have the relation
		$$
		\nabla_{\boldsymbol{X}} \bar{\varphi}_{r}=J \cdot \nabla_{\boldsymbol{x}} \varphi_{i},
		$$
		which we can solve with respect to $\nabla_{\boldsymbol{x}} \varphi_{i}$ :
		
		\begin{equation}
			\label{eqa202}
			\nabla_{\boldsymbol{x}} \varphi_{i}=J^{-1} \cdot \nabla_{\boldsymbol{X}} 	\tilde{\varphi}_{r} .
		\end{equation}
	
		\noindent On the reference cell, $\varphi_{i}(\boldsymbol{x})=\tilde{\varphi}_{r}(\boldsymbol{X})$, so
		
		\begin{equation}
			\label{eqa203}
			\nabla_{\boldsymbol{x}} \tilde{\varphi}_{r}(\boldsymbol{X})=J^{-1}(\boldsymbol{X}) \cdot \nabla_{\boldsymbol{X} \tilde{\varphi}_{r}}(\boldsymbol{X}) .
		\end{equation}
	
		This means that we have the following transformation of the integral in the physical domain to its counterpart over the reference cell:
		\begin{equation}
			\label{eqa204}
			\int_{\Omega}^{(e)} a(\boldsymbol{x}) \nabla_{\boldsymbol{x} \varphi_{i}} \cdot 	\nabla_{\boldsymbol{x} \varphi_{j}} \mathrm{~d} x \int_{\bar{\Omega}^{r}} a(\boldsymbol{x}(\boldsymbol{X}))\left(J^{-1} \cdot \nabla_{\boldsymbol{X}} \tilde{\varphi}_{r}\right) \cdot\left(J^{-1} \cdot \nabla \tilde{\varphi}_{s}\right) \operatorname{det} J \mathrm{~d} X
		\end{equation}\bigbreak
	
	\section[Numerical integration]{Numerical integration}
		\label{sec:sec_17_2}
		\noindent Integrals are normally computed by numerical integration rules. For multidimensional cells, various families of rules exist. All of them are similar to what is shown in $1 \mathrm{D}: \int f \mathrm{~d} x \approx \sum_{j} w_{i} f\left(\boldsymbol{x}_{j}\right)$, where $w_{j}$ are weights and $\boldsymbol{x}_{j}$ are corresponding points.
		
		The file \href{https://github.com/hplgit/INF5620/blob/master/src/fem/numint.py}{numint.py} contains the functions \textbf{\texttt{quadrature\_for\_triangles(n)}} and \textbf{\texttt{quadrature\_for\_tetrahedra(n)}}, which returns lists of points and weights corresponding to integration rules with \textbf{\texttt{n}} points over the reference triangle with vertices $(0,0),(1,0),(0,1)$, and the reference tetrahedron with vertices $(0,0,0),(1,0,0),(0,1,0),(0,0,1)$, respectively. For example, the first two rules for integration over a triangle have 1 and 3 points:
		
		\begin{lstlisting}[numbers=none]
			>>> import numint
			>>> x, w = numint.quadrature_for_triangles(num_points=1)
			>>> x
			[(0.3333333333333333, 0.3333333333333333)]
			>>> w
			[0.5]
			>>> x, w = numint.quadrature_for_triangles(num_points=3)
			>>> x
			[(0.16666666666666666, 0.16666666666666666),
			(0.66666666666666666, 0.16666666666666666),
			(0.16666666666666666, 0.66666666666666666)]
			>>> w
			[0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
		\end{lstlisting}
		Rules with $1,3,4$, and 7 points over the triangle will exactly integrate polynomials of degree $1,2,3$, and 4 , respectively. In $3 \mathrm{D}$, rules with $1,4,5$, and 11 points over the tetrahedron will exactly integrate polynomials of degree $1,2,3$, and 4 , respectively. \bigbreak
		
	\section[Convenient formulas for P1 elements in 2D]{Convenient formulas for P1 elements in 2D}
	\label{sec:sec_17_3}
	\noindent We shall now provide some formulas for piecewise linear $\varphi_{i}$ functions and their integrals \emph{in the physical coordinate system}. These formulas make it convenient to compute with P1 elements without the need to work in the reference coordinate system and deal with mappings and Jacobians. A lot of computational and algorithmic details are hidden by this approach.
	
	Let $\Omega^{(e)}$ be cell number $e$, and let the three vertices have global vertex numbers $I, J$, and $K$. The corresponding coordinates are $\left(x_{I}, y_{I}\right),\left(x_{J}, y_{J}\right)$, and $\left(x_{K}, y_{K}\right)$. The basis function $\varphi_{I}$ over $\Omega^{(e)}$ have the explicit formula
	
	\begin{equation}
		\label{eqa205}
		\varphi_{I}(x, y)=\frac{1}{2} \Delta\left(\alpha_{I}+\beta_{I} x+\gamma_{I} y\right)
	\end{equation}

	\noindent where
	
	\begin{equation}
		\label{eqa206}
			\alpha_{I} =x_{J} y_{K}-x_{K} y_{J}, 
	\end{equation}

	\begin{equation}
		\label{eqa207}
			\beta_{I} =y_{J}-y_{K}
	\end{equation}

	\begin{equation}
		\label{eqa208}
			\gamma_{I} =x_{K}-x_{J},
	\end{equation}

	\begin{equation}
		\label{eqa209}
			2 \Delta =\operatorname{det}\left(\begin{array}{ccc}
				1\quad  x_{I}\quad  y_{I} \\
				1\quad  x_{J}\quad  y_{J} \\
				1\quad  x_{K}\quad  y_{K}
			\end{array}\right)
	\end{equation}

	\noindent The quantity $\Delta$ is the area of the cell. \smallbreak
	The following formula is often convenient when computing element matrices and vectors:
	
	\begin{equation}
		\label{eqa210}
		\int_{\left.\Omega^{(}\right)} \varphi_{I}^{p} \varphi_{J}^{q} \varphi_{K}^{r} d x d y=\frac{p ! q ! r !}{(p+q+r+2) !} 2 \Delta
	\end{equation}

	\noindent (Note that the $q$ in this formula is not to be mixed with the $q(e, r)$ mapping of degrees of freedom.)
	
	As an example, the element matrix entry $\int_{\Omega(c)} \varphi_{I} \varphi_{J} \mathrm{~d} x$ can be computed by setting $p=q=1$ and $r=0$, when $I \neq J$, yielding $\Delta / 12$, and $p=2$ and $q=r=0$, when $I=J$, resulting in $\Delta / 6$. We collect these numbers in a local element matrix:
	$$
	\frac{\Delta}{12}\left[\begin{array}{lll}
		2 & 1 & 1 \\
		1 & 2 & 1 \\
		1 & 1 & 2
	\end{array}\right]
	$$
	
	The common element matrix entry $\int_{\Omega^{(e)}} \nabla \varphi_{I} \cdot \nabla \varphi_{J} \mathrm{~d} x$, arising from a Laplace term $\nabla^{2} u$, can also easily be computed by the formulas above. We have
	$$
	\nabla \varphi_{I} \cdot \nabla \varphi_{J}=\frac{\Delta^{2}}{4}\left(\beta_{I} \beta_{J}+\gamma_{I} \gamma_{J}\right)=\text { const, }
	$$
	so that the element matrix entry becomes $\frac{1}{4} \Delta^{3}\left(\beta_{I} \beta_{J}+\gamma_{I} \gamma_{J}\right)$.\smallbreak
	From an implementational point of view, one will work with local vertex numbers $r=0,1,2$, parameterize the coefficients in the basis functions by $r$, and look up vertex coordinates through $q(e, r)$.\smallbreak
	Similar formulas exist for integration of P1 elements in 3D.
\chapter{Summary}
\label{chap:chap_18}
\pagenumbering{arabic}
\begin{itemize}
	\item When approximating $f$ by $u=\sum_{j} c_{j} \varphi_{j}$, the least squares method and the Galerkin/projection method give the same result. The interpolation/collocation method is simpler and yields different (mostly inferior) results.
	\item Fourier series expansion can be viewed as a least squares or Galerkin approximation procedure with sine and cosine functions.
	\item Basis functions should optimally be orthogonal or almost orthogonal, because this gives little round-off errors when solving the linear system, and the coefficient matrix becomes diagonal or sparse.
	\item Finite element basis functions are \emph{piecewise} polynomials, normally with discontinuous derivatives at the cell boundaries. The basis functions overlap very little, leading to stable numeries and sparse matrices.
	\item To use the finite element method for differential equations, we use the Galerkin method or the method of weighted residuals to arrive at a variational form. Technically, the differential equation is multiplied by a test function and integrated over the domain. Second-order derivatives are integrated by parts to allow for typical finite element basis functions that have discontinuous derivatives.
	\item The least squares method is not much used for finite element solution of differential equations of second order, because it then involves second-order derivatives which cause trouble for basis functions with discontinuous derivatives.
	\item We have worked with two common finite element terminologies and associated data structures (both are much used, especially the first one, while the other is more general):
		\begin{enumerate}
			\item[1.] \emph{elements, nodes, and mapping between local and global node numbers}
			\item[2.] \emph{an extended element concept consisting of cell, vertices, degrees of freedom, local basis functions, geometry mapping, and mapping between local and global degrees of freedom}
		\end{enumerate}
	\item The meaning of the word "element" is multi-fold: the geometry of a finite element (also known as a cell), the geometry and its basis functions, or all information listed under point 2 above.
	\item One normally computes integrals in the finite element method element by element (cell by cell), either in a local reference coordinate system or directly in the physical domain.
	\item The advantage of working in the reference coordinate system is that the mathematical expressions for the basis functions depend on the element type only, not the geometry of that element in the physical domain. The disadvantage is that a mapping must be used, and derivatives must be transformed from reference to physical coordinates.
	\item Element contributions to the global linear system are collected in an element matrix and vector, which must be assembled into the global system using the degree of freedom mapping (\textbf{\texttt{dof\_map}}) or the node numbering mapping (\textbf{\texttt{elements}}), depending on which terminology that is used.
	\item Dirichlet conditions, involving prescribed values of $u$ at the boundary, are implemented either via a boundary function that take on the right Dirichlet values, while the basis functions vanish at such boundaries. In the finite element method, one has a general expression for the boundary function, but one can also incorporate Dirichlet conditions in the element matrix and vector or in the global matrix system.
	\item Neumann conditions, involving prescribed values of the derivative (or flux) of $u$, are incorporated in boundary terms arising from integrating terms with second-order derivatives by part. Forgetting to account for the boundary terms implies the condition $\partial u / \partial n=0$ at parts of the boundary where no Dirichlet condition is set.
\end{itemize}

\chapter{Time-dependent problems}
\label{chap:chap_19}
\pagenumbering{arabic}
	\noindent The finite element method is normally used for discretization in space. There are two alternative strategies for performing a discretization in time:
	
	\begin{itemize}
		\item use finite differences for time derivatives to arrive at a recursive set of spatial problems that can be discretized by the finite element method, or
		\item discretize in space by finite elements first, and then solve the resulting system of ordinary differential equations (ODEs) by some standard method for ODEs.
	\end{itemize}
	 
	\noindent We shall exemplify these strategies using a simple diffusion problem
	
	\begin{equation}
	\label{eqa211}
		\frac{\partial u}{\partial t}=\alpha \nabla^{2} u+f(\boldsymbol{x}, t),\boldsymbol{x} \in \Omega, t \in(0, T], \\
	\end{equation}

	\begin{equation}
	\label{eqa212}
		u(\boldsymbol{x}, 0)=I(\boldsymbol{x}), \boldsymbol{x} \in \Omega, \\
	\end{equation}

	\begin{equation}
	\label{eqa213}
		\frac{\partial u}{\partial n}=0,\boldsymbol{x} \in \partial \Omega, t \in(0, T]
	\end{equation}

	\noindent Here, $u(\boldsymbol{x}, t)$ is the unknown function, $\alpha$ is a constant, and $f(x, t)$ and $I(x)$ are given functions. We have assigned the particular boundary condition (\ref{eqa213}) to minimize the details on handling boundary conditions in the finite element method.

\section[Discretization in time by a Forward Euler scheme]{Discretization in time by a Forward Euler scheme}
	\label{sec:sec_19_1}
	\noindent \textbf{Time discretization.   } We can apply a finite difference method in time to (\ref{eqa211}). First we need a mesh in time, here taken as uniform with mesh points $t_{n}=n \Delta t, n=0,1, \ldots, N_{t}$. A Forward Euler scheme consists of sampling (\ref{eqa211}) at $t_{n}$ and approximating the time derivative by a forward difference $\left[D_{t}^{+} u\right]^{n} \approx\left(u^{n+1}-u^{n}\right) / \Delta t$. This approximation turns (\ref{eqa211}) into a differential equation that is discrete in time, but still continuous in space. With a finite difference operator notation we can write the time-discrete problem as

	\begin{equation}
	\label{eqa214}	
		\left[D_{t}^{+} u=\alpha \nabla^{2} u+f\right]^{n},
	\end{equation}

	\noindent for $n=1,2, \ldots, N_{t}-1$. Writing this equation out in detail and isolating the unknown $u^{n+1}$ on the left-hand side, demonstrates that the time-discrete problem is a recursive set of problems that are continuous in space:

	\begin{equation}
	\label{eqa215}	
		u^{n+1}=u^{n}+\Delta t\left(\alpha \nabla^{2} u^{n}+f\left(x, t_{n}\right)\right) .
	\end{equation}

	\noindent Given $u^{0}=I$, we can use (\ref{eqa215}) to compute $u^{1}, u^{2}, \ldots, u^{N_{t}}$.\smallbreak
	For absolute clarity in the various stages of the discretizations, we introduce $u_{\mathrm{e}}(\boldsymbol{x}, t)$ as the exact solution of the space-and time-continuous partial differential equation (\ref{eqa211}) and $u_{\mathrm{e}}^{n}(\boldsymbol{x})$ as the time-discrete approximation, arising from the finite difference method in time (\ref{eqa214}). More precisely, $u_{\mathrm{e}}$ fulfills
		
	\begin{equation}
	\label{eqa216}	
		\frac{\partial u_{\mathrm{e}}}{\partial t}=\alpha \nabla^{2} u_{\mathrm{e}}+f(\boldsymbol{x}, t),
	\end{equation}

	\noindent while $u_{e}^{n+1}$, with a superscript, is the solution of the time-discrete equations
	\begin{equation}
	\label{eqa217}	
		u_{\mathrm{e}}^{n+1}=u_{\mathrm{e}}^{n}+\Delta t\left(\alpha \nabla^{2} u_{\mathrm{e}}^{n}+f\left(\boldsymbol{x}, t_{n}\right)\right) .
	\end{equation}

		\textbf{Space discretization.  } We now introduce a finite element approximation to $u_{\mathrm{e}}^{n}$ and $u_{\mathrm{e}}^{n+1}$ in (\ref{eqa217}), where the coefficients depend on the time level:
		
		\begin{equation}
			\label{eqa218}	
			u_{\mathrm{e}}^{n} \approx u^{n}=\sum_{j=0}^{N} c_{j}^{n} \psi_{j}(\boldsymbol{x}), \\
		\end{equation}
	
		\begin{equation}
			\label{eqa219}	
			u_{\mathrm{e}}^{n+1} \approx u^{n+1}=\sum_{j=0}^{N} c_{j}^{n+1} \psi_{j}(\boldsymbol{x}) .
		\end{equation}
	
		\noindent Note that, as before, $N$ denotes the number of degrees of freedom in the spatial domain. The number of time points is denoted by $N_{t}$. We define a space $V$ spanned by the basis functions $\left\{\psi_{i}\right\}_{i \in \mathcal{I}_{x}}$. \bigbreak 
	
	\section[Variational forms]{Variational forms}	
	\label{sec:sec_19_2}
		\noindent A weighted residual method with weighting functions $w_{i}$ can now be formulated. We insert (\ref{eqa218}) and (\ref{eqa219}) in (\ref{eqa217}) to obtain the residual
		$$
		R=u^{n+1}-u^{n}-\Delta t\left(\alpha \nabla^{2} u^{n}+f\left(x, t_{n}\right)\right) .
		$$
		The weighted residual principle,
		$$
		\int_{\Omega} R w \mathrm{~d} x=0, \quad \forall w \in W,
		$$
		results in
		$$
		\int_{\Omega}\left[u^{n+1}-u^{n}-\Delta t\left(\alpha \nabla^{2} u^{n}+f\left(\boldsymbol{x}, t_{n}\right)\right)\right] w \mathrm{~d} x=0, \quad \forall w \in W .
		$$
		From now on we use the Galerkin method so $W=V$. Isolating the unknown $u^{n+1}$ on the left-hand side gives
		$$
		\int_{\Omega} u^{n+1} \psi_{i} \mathrm{~d} x=\int_{\Omega}\left[u^{n}-\Delta t\left(\alpha \nabla^{2} u^{n}+f\left(\boldsymbol{x}, t_{n}\right)\right)\right] v \mathrm{~d} x, \quad \forall v \in V .
		$$
		As usual in spatial finite element problems involving second-order derivatives, we apply integration by parts on the term $\int\left(\nabla^{2} u^{n}\right) v d x$ :
		$$
		\int_{\Omega} \alpha\left(\nabla^{2} u^{n}\right) v \mathrm{~d} x=-\int_{\Omega} \alpha \nabla u^{n} \cdot \nabla v \mathrm{~d} x+\int_{\partial \Omega} \alpha \frac{\partial u^{n}}{\partial n} v \mathrm{~d} x
		$$
		\noindent The last term vanishes because we have the Neumann condition $\partial u^{n} / \partial n=0$ for all $n$. Our discrete problem in space and time then reads
	
	\begin{equation}
	\label{eqa220}	
		\int_{\Omega} u^{n+1} v \mathrm{~d} x=\int_{\Omega} u^{n} v d x-\Delta t \int_{\Omega} \alpha \nabla u^{n} \cdot \nabla v \mathrm{~d} x+\Delta t \int_{\Omega} f^{n} v \mathrm{~d} x, \quad \forall v \in V .
	\end{equation}

		This is the variational formulation of our recursive set of spatial problems.
		
		
		\begin{mybox}
		\textbf{Nonzero Dirichlet boundary conditions.}	
			\noindent As in stationary problems, we can introduce a boundary function $B(x, t)$ to take care of nonzero Dirichlet conditions:
			
			\begin{equation}
				\label{eqa221}	
				u_{\mathrm{e}}^{n} \approx u^{n}=B\left(\boldsymbol{x}, t_{n}\right)+\sum_{j=0}^{N} c_{j}^{n} \psi_{j}(\boldsymbol{x}), \\
			\end{equation}
		
			\begin{equation}
				\label{eqa222}	
				u_{\mathrm{e}}^{n+1} \approx u^{n+1}=B\left(\boldsymbol{x}, t_{n+1}\right)+\sum_{j=0}^{N} c_{j}^{n+1} \psi_{j}(\boldsymbol{x}) .
			\end{equation}
			
		\end{mybox}
		
		
	\section[Simplified notation for the solution at recent time levels]{Simplified notation for the solution at recent time levels}
		\label{sec:sec_19_3}
		\noindent In a program it is only necessary to store $u^{n+1}$ and $u^{n}$ at the same time. We therefore drop the $n$ index in programs and work with two functions: $u$ for $u^{n+1}$, the new unknown, and $u_{-} 1$ for $u^{n}$, the solution at the previous time level. This is also convenient in the mathematies to maximize the correspondence with the code. From now on $u_{1}$ means the discrete unknown at the previous time level $\left(u^{n}\right)$ and $u$ represents the discrete unknown at the new time level $\left(u^{n+1}\right)$. Equation (\ref{eqa220}) with this new naming convention is expressed as
		\begin{equation}
			\label{eqa223}	
			\int_{\Omega} u v d x=\int_{\Omega} u_{1} v d x-\Delta t \int_{\Omega} \alpha \nabla u_{1} \cdot \nabla v \mathrm{~d} x+\Delta t \int_{\Omega} f^{n} v \mathrm{~d} x
		\end{equation}
	
		This variational form can alternatively be expressed by the inner product notation:
		\begin{equation}
			\label{eqa224}	
			(u, v)=\left(u_{1}, v\right)-\Delta t\left(\alpha \nabla u_{1}, \nabla v\right)+\left(f^{n}, v\right)
		\end{equation}
	
	\section[Deriving the linear systems]{Deriving the linear systems}
		\label{sec:sec_19_4}
		\noindent To derive the equations for the new unknown coefficients $c_{j}^{n+1}$, npw just called $c_{j}$, we insert
		$$
		u=\sum_{j=0}^{N} c_{j} \psi_{j}(\boldsymbol{x}), \quad u_{1}=\sum_{j=0}^{N} c_{1, j} \psi_{j}(\boldsymbol{x})
		$$
		in (\ref{eqa223}) or (\ref{eqa224}), let the equation hold for all $v=\psi, i=0, \ldots, \mathrm{N}$, and order the terms as matrix-vector products:
	\begin{equation}
			\label{eqa225}	
		\sum_{j=0}^{N}\left(\psi_{i}, \psi_{j}\right) c_{j}=\sum_{j=0}^{N}\left(\psi_{i}, \psi_{j}\right) c_{1, j}-\Delta t \sum_{j=0}^{N}\left(\nabla \psi_{i}, \alpha \nabla \psi_{j}\right) c_{1, j}+\left(f^{n}, \psi_{i}\right), \quad i=0, \ldots, N .
	\end{equation}
		This is a linear system $\sum_{j} A_{i, j} c_{j}=b_{i}$ with
		$$
		A_{i, j}=\left(\psi_{i,} \psi_{j}\right)
		$$
		and
		$$
		b_{i}=\sum_{j=0}^{N}\left(\psi_{i}, \psi_{j}\right) c_{1, j}-\Delta t \sum_{j=0}^{N}\left(\nabla \psi_{i}, \alpha \nabla \psi_{j}\right) c_{1, j}+\left(f^{n}, \psi_{i}\right)
		$$\smallbreak
		It is instructive and convenient for implementations to write the linear system on the form

	\begin{equation}
		\label{eqa226}	
		M c=M c_{1}-\Delta t K c_{1}+f
	\end{equation}

		where\bigbreak
		
		$\begin{aligned}
			M &=\left\{M_{i, j}\right\}, \quad M_{i, j}=\left(\psi_{i}, \psi_{j}\right), \quad i, j \in \mathcal{I}_{s}, \\ K &=\left\{K_{i, j}\right\}, \quad K_{i, j}=\left(\nabla \psi_{i}, \alpha \nabla \psi_{j}\right), \quad i, j \in \mathcal{I}_{s}, \\ f &=\left\{\left(f\left(\boldsymbol{x}, t_{n}\right), \psi_{i}\right)\right\}_{i \in \mathcal{I}_{s}}, \\ c &=\left\{c_{i}\right\}_{i \in \mathcal{I}_{s}}, \\ c_{1} &=\left\{c_{1, i}\right\}_{i \in \mathcal{I}_{s}} .
		 \end{aligned}$
		
		\noindent We realize that $M$ is the matrix arising from a term with the zero-th derivative of $u$, and called the mass matrix, while $K$ is the matrix arising from a Laplace term $\nabla^{2} u$. The $K$ matrix is often known as the \emph{stiffness matrix}. (The terms mass and stiffness stem from the early days of finite elements when applications to vibrating structures dominated. The mass matrix arises from the mass times acceleration term in Newton's second law, while the stiffness matrix arises from the elastic forces in that law. The mass and stiffness matrix appearing in a diffusion have slightly different mathematical formulas.)\smallbreak
		\noindent \textbf{Remark.   } The mathematical symbol $f$ has two meanings, either the function $f(\boldsymbol{x}, t)$ in the PDE or the $f$ vector in the linear system to be solved at each time level. The symbol $u$ also has different meanings, basically the unknown in the PDE or the finite element function representing the unknown at a time level. The actual meaning should be evident from the context.
		
	\section[Computational algorithm]{Computational algorithm}
		\label{sec:sec_19_5}
		\noindent We observe that $M$ and $K$ can be precomputed so that we can avoid computing the matrix entries at every time level. Instead, some matrix-vector multiplications will produce the linear system to be solved. The computational algorithm has the following steps:

		\begin{enumerate}
		\item [1.] Compute $M$ and $K$.
		\item [2.] Initialize $u^{0}$ by interpolation or projection
		\item [3.] For $n=1,2, \ldots, N_{t}$ :
			\begin{enumerate}
				\item[(a)] compute $b=M c_{1}-\Delta t K c_{1}+f$
				\item[(b)] solve $M c=b$
				\item[(c)] set $c_{1}=c$
			\end{enumerate}
		\end{enumerate}
	
		\noindent In case of finite element basis functions, interpolation of the initial condition at the nodes means $c_{1, j}=I\left(\boldsymbol{x}_{j}\right)$. Otherwise one has to solve the linear system $\sum_{j} \psi_{j}\left(x_{i}\right) c_{j}=I\left(x_{i}\right)$, where $\boldsymbol{x}_{j}$ denotes an interpolation point. Projection (or Galerkin's method) implies solving a linear system with $M$ as coefficient matrix $: \sum_{j} M_{i, j} c_{1, j}=\left(I, \psi_{i}\right), i \in \mathcal{I}_{s}$.
		
	\section[Comparing P1 elements with the finite difference method]{Comparing P1 elements with the finite difference method}
		\label{sec:sec_19_6}
		
		\noindent We can compute the $M$ and $K$ matrices using P1 elements in 1D. A uniform mesh on $[0, L]$ is introduced for this purpose. Since the boundary conditions are solely of Neumann type in this sample problem, we have no restrictions on the basis functions $\psi_{i}$ and can simply choose $\psi_{i}=\varphi_{i}, i=0, \ldots, N=N_{n}$.
		
		From Section \ref{sec:sec_13_2} or \ref{sec:sec_13_4} we have that the $K$ matrix is the same as we get from the finite difference method: $h\left[D_{x} D_{x} u\right]_{i}^{n}$, while from Section \ref{sec:sec_5_2} we know that $M$ can be interpreted as the finite difference approximation $\left[u+\frac{1}{6} h^{2} D_{x} D_{x} u\right]_{i}^{n}$ (times $h$ ). The equation system $M c=b$ in the algorithm is therefore equivalent to the finite difference scheme
		
		\begin{equation}
		\label{eqa227}	
			\left[D_{t}^{+}\left(u+\frac{1}{6} h^{2} D_{x} D_{x} u\right)=\alpha D_{x} D_{x} u+f\right]_{i}^{n}
		\end{equation}
	
		(More precisely, $M c=b$ divided by $h$ gives the equation above.)\smallbreak
		\noindent \textbf{Lumping the mass matrix.   } By applying Trapezoidal integration one can turn $M$ into a diagonal matrix with $(h / 2, h, \ldots, h, h / 2)$ on the diagonal. Then there is no need to solve a linear system at each time level, and the finite element scheme becomes identical to a standard finite difference method
		
		\begin{equation}
		\label{eqa228}	
			\left[D_{t}^{+} u=\alpha D_{x} D_{x} u+f\right]_{i}^{n}
		\end{equation}
		
		\noindent The Trapezoidal integration is not as accurate as exact integration and introduces therefore an error. Whether this error has a good or bad influence on the overall numerical method is not immediately obvious, and is analyzed in detail in Section \ref{sec:sec_19_10}. The effect of the error is at least not more severe than what is produced by the finite difference method.
		
		Making $M$ diagonal is usually referred to as lumping the \emph{mass matrix}. There is an alternative method to using an integration rule based on the node points; one can sum the entries in each row, place the sum on the diagonal, and set all other entries in the row equal to zero. For P1 elements the methods of lumping the mass matrix give the same result.
	\section[Discretization in time by a Backward Euler scheme]{Discretization in time by a Backward Euler scheme}
		\label{sec:sec_19_7}
		\noindent \textbf{Time discretization.   } The Backward Euler scheme in time applied to our diffusion problem can be expressed as follows using the finite difference operator notation:
		$$
		\left[D_{t}^{-} u=\alpha \nabla^{2} u+f(\boldsymbol{x}, t)\right]^{n} .
		$$
		Written out, and collecting the unknown $u^{n}$ on the left-hand side and all the known terms on the right-hand side, the time-discrete differential equation becomes
		\begin{equation}
		\label{eqa229}	
			u_{\mathrm{e}}^{n}-\Delta t\left(\alpha \nabla^{2} u_{\mathrm{e}}^{n}+f\left(\boldsymbol{x}, t_{n}\right)\right)=u_{\mathrm{e}}^{n-1}
		\end{equation}
		Equation (\ref{eqa229}) can compute $u_{\mathrm{e}}^{1}, u_{\mathrm{e}}^{2}, \ldots, u_{\mathrm{e}}^{N_{t}}$, if we have a start $u_{\mathrm{e}}^{0}=I$ from the initial condition. However, (\ref{eqa229}) is a partial differential equation in space and needs a solution method based on discretization in space. For this purpose we use an expansion as in (\ref{eqa218})-(\ref{eqa219}).\bigbreak
		\noindent \textbf{Variational forms.   } Inserting (\ref{eqa218})-(\ref{eqa219}) in (\ref{eqa229}), multiplying by $\psi_{i}$ (or $v \in V$ ), and integrating by parts, as we did in the Forward Euler case, results in the variational form
		\begin{equation}
		\label{eqa230}	
			\int_{\Omega}\left(u^{n} v+\Delta t \alpha \nabla u^{n} \cdot \nabla v\right) \mathrm{d} x=\int_{\Omega} u^{n-1} v \mathrm{~d} x-\Delta t \int_{\Omega} f^{n} v \mathrm{~d} x, \quad \forall v \in V
		\end{equation}
	
		\noindent Expressed with $u$ as $u^{n}$ and $u_{1}$ as $u^{n-1}$, this becomes
	
		\begin{equation}
			\label{eqa231}	
			\int_{\Omega}(u v+\Delta t \alpha \nabla u \cdot \nabla v) \mathrm{d} x=\int_{\Omega} u_{1} v \mathrm{~d} x+\Delta t \int_{\Omega} f^{n} v \mathrm{~d} x
		\end{equation}
	
		\noindent or with the more compact inner product notation,
	
		\begin{equation}
			\label{eqa232}	
		(u, v)+\Delta t(\alpha \nabla u, \nabla v)=\left(u_{1}, v\right)+\Delta t\left(f^{n}, v\right)
		\end{equation}
	
		\noindent \textbf{Linear systems.   } Inserting $u=\sum_{j} c_{j} \psi_{i}$ and $u_{1}=\sum_{j} c_{1, j} \psi_{i}$, and choosing $v$ to be the basis functions $\psi_{i} \in V, i=0, \ldots, N$, together with doing some algebra, lead to the following linear system to be solved at each time level:
		
		\begin{equation}
			\label{eqa233}	
			(M+\Delta t K) c=M c_{1}+f
		\end{equation}
	
		\noindent where $M, K$, and $f$ are as in the Forward Euler case. This time we really have to solve a linear system at each time level. The computational algorithm goes as follows.
		
		\begin{itemize}
			\item[1.] Compute $M, K$, and $A=M+\Delta t K$
			\item[2.] Initialize $u^{0}$ by interpolation or projection
			\item[3.] For $n=1,2, \ldots, N_{t}$ :
			\begin{itemize}
				\item[(a)] compute $b=M c_{1}+f$
				\item[(b)] solve $A c=b$
				\item[(c)] set $c_{1}=c$
			\end{itemize}
		\end{itemize}
		
		\noindent In case of finite element basis functions, interpolation of the initial condition at the nodes means $c_{1, j}=I\left(\boldsymbol{x}_{j}\right)$. Otherwise one has to solve the linear system $\sum_{j} \psi_{j}\left(x_{i}\right) c_{j}=I\left(x_{i}\right)$, where $\boldsymbol{x}_{j}$ denotes an interpolation point. Projection (or Galerkin's method) implies solving a linear system with $M$ as coefficient matrix $: \sum_{j} M_{i, j} c_{1, j}=\left(I, \psi_{i}\right), i \in \mathcal{I}_{s}$.
		
		We know what kind of finite difference operators the $M$ and $K$ matrices correspond to (after dividing by $h$ ), so (\ref{eqa233}) can be interpreted as the following finite difference method:
		
		\begin{equation}
		\label{eqa234}
			\left[D_{t}^{-}\left(u+\frac{1}{6} h^{2} D_{x} D_{x} u\right)=\alpha D_{x} D_{x} u+f\right]_{i}^{n} .
		\end{equation}
	
		The mass matrix $M$ can be lumped, as explained in Section \ref{sec:sec_19_6}, and then the linear system arising from the finite element method with P1 elements corresponds to a plain Backward Euler finite difference method for the diffusion equation:

		\begin{equation}	
		\label{eqa235}
			\left[D_{t}^{-} u=\alpha D_{x} D_{x} u+f\right]_{i}^{n} .
		\end{equation}
	
\section[Dirichlet boundary conditions]{Dirichlet boundary conditions}
	\label{sec:sec_19_8}
		\noindent Suppose now that the boundary condition (\ref{eqa213}) is replaced by a mixed Neumann and Dirichlet condition,
		
		\begin{equation}
		\label{236}
			u(\boldsymbol{x}, t)=u_{0}(\boldsymbol{x}, t),\boldsymbol{x} \in \partial \Omega_{D}, \\
		\end{equation}
	
		\begin{equation}
		\label{237}
			-\alpha \frac{\partial}{\partial n} u(\boldsymbol{x}, t)=g(\boldsymbol{x}, t), \boldsymbol{x} \in \partial \Omega_{N}
		\end{equation}
	
		\noindent Using a Forward Euler discretization in time, the variational form at a time level becomes
		
		\begin{equation}
		\label{eqa238}
			\int_{\Omega} u^{n+1} v \mathrm{~d} x=\int_{\Omega}\left(u^{n}-\Delta t \alpha \nabla u^{n}, \nabla v\right) \mathrm{d} x-\Delta t \int_{\partial \Omega_{N}} g v \mathrm{~d} s, \quad \forall v \in V
		\end{equation}
	
		\noindent \textbf{Boundary function.   } The Dirichlet condition $u=u_{0}$ at $\partial \Omega_{D}$ can be incorporated through a boundary function $B(\boldsymbol{x})=u_{0}(\boldsymbol{x})$ and demanding that $v=0$ at $\partial \Omega_{D}$. The expansion for $u^{n}$ is written as
		
		\begin{equation}
		\label{eqa239}
			u^{n}(\boldsymbol{x})=u_{0}\left(\boldsymbol{x}, t_{n}\right)+\sum_{j \in 	\mathcal{I}_{\boldsymbol{x}}} c_{j}^{n} \psi_{j}(\boldsymbol{x}) .
		\end{equation}
	
		\noindent Inserting this expansion in the variational formulation and letting it hold for all basis functions $\psi_{i}$ leads to the linear system
		$$
		\begin{aligned}
			\sum_{j \in \mathcal{I}_{a}}\left(\int_{\Omega} \psi_{i} \psi_{j} \mathrm{~d} x\right) c_{j}^{n+1}=& \sum_{j \in \mathcal{I}_{\boldsymbol{a}}}\left(\int_{\Omega}\left(\psi_{i} \psi_{j}-\Delta t \alpha \nabla \psi_{i} \cdot \nabla \psi_{j}\right) \mathrm{d} x\right) c_{j}^{n}-\\
			& \int_{\Omega}\left(u_{0}\left(\boldsymbol{x}, t_{n+1}\right)-u_{0}\left(\boldsymbol{x}, t_{n}\right)+\Delta t \alpha \nabla u_{0}\left(\boldsymbol{x}, t_{n}\right) \cdot \nabla \psi_{i}\right) \mathrm{d} x \\
			&+\Delta t \int_{\Omega} f \psi_{i} \mathrm{~d} x-\Delta t \int_{\partial \Omega_{N}} g \psi_{i} \mathrm{~d} s, \quad i \in \mathcal{I}_{s} .
		\end{aligned}
		$$
		In the following, we adopt the convention that the unknowns $c_{j}^{n+1}$ are written as $c_{j}$, while the known $c_{j}^{n}$ from the previous time level are denoted by $c_{1, j}$.\bigbreak
		\noindent \textbf{Finite element basis functions.   } When using finite elements, each basis function $\varphi_{i}$ is associated with a node $x_{i}$. We have a collection of nodes $\left\{x_{i}\right\}_{i \in I_{b}}$ on the boundary $\partial \Omega_{D}$. Suppose $U_{k}^{n}$ is the known Dirichlet value at $x_{k}$ at time $t_{n}\left(U_{k}^{n}=u_{0}\left(x_{k}, t_{n}\right)\right)$. The appropriate boundary function is then
		$$
		B\left(\boldsymbol{x}, t_{n}\right)=\sum_{j \in I_{b}} U_{j}^{n} \varphi_{j} .
		$$
		The unknown coefficients $c_{j}$ are associated with the rest of the nodes, which have numbers $\nu(i), i \in \mathcal{I}_{s}=\{0, \ldots, N\}$. The basis functions for $V$ are chosen as $\psi_{i}=\varphi_{\nu(i)}, i \in \mathcal{I}_{s}$, and all of these vanish at the boundary nodes as they should. The expansion for $u^{n+1}$ and $u^{n}$ become
		$$
		\begin{aligned}
			u^{n} &=\sum_{j \in I_{b}} U_{j}^{n} \varphi_{j}+\sum_{j \in \mathcal{I}_{x}} c_{1, j} \varphi_{\nu(j)} \\
			u^{n+1} &=\sum_{j \in I_{b}} U_{j}^{n+1} \varphi_{j}+\sum_{j \in \mathcal{I}_{x}} c_{j} \varphi_{\nu(j)}
		\end{aligned}
		$$
		
		The equations for the unknown coefficients $c_{i}$ become
		$$
		\begin{aligned}
			\sum_{j \in \mathcal{I}_{x}}\left(\int_{\Omega} \varphi_{i} \varphi_{j} \mathrm{~d} x\right) c_{j}=& \sum_{j \in \mathcal{I}_{x}}\left(\int_{\Omega}\left(\varphi_{i} \varphi_{j}-\Delta t \alpha \nabla \varphi_{i} \cdot \nabla \varphi_{j}\right) \mathrm{d} x\right) c_{1, j}-\\
			& \sum_{j \in I_{b}} \int_{\Omega}\left(\varphi_{i} \varphi_{j}\left(U_{j}^{n+1}-U_{j}^{n}\right)+\Delta t \alpha \nabla \varphi_{i} \cdot \nabla \varphi_{j} U_{j}^{n}\right) \mathrm{d} x \\
			&+\Delta t \int_{\Omega} f \varphi_{i} \mathrm{~d} x-\Delta t \int_{\partial \Omega_{N}} g \varphi_{i} \mathrm{~d} s, \quad i \in \mathcal{I}_{s}
		\end{aligned}
		$$
		\noindent \textbf{Modification of the linear system.} Instead of introducing a boundary function $B$ we can work with basis functions associated with all the nodes and incorporate the Dirichlet conditions by modifying the linear system. Let $\mathcal{I}_{s}$ be the index set that counts all the nodes: $\left\{0,1, \ldots, N=N_{n}\right\}$. The expansion for $u^{n}$ is then $\sum_{j \in \mathcal{I}_{x}} c_{j}^{n} \varphi_{j}$ and the variational form becomes
		$$
		\begin{aligned}
			\sum_{j \in \mathcal{I}_{x}}\left(\int_{\Omega} \varphi_{i} \varphi_{j} \mathrm{~d} x\right) c_{j}=& \sum_{j \in \mathcal{I}_{x}}\left(\int_{\Omega}\left(\varphi_{i} \varphi_{j}-\Delta t \alpha \nabla \varphi_{i} \cdot \nabla \varphi_{j}\right) \mathrm{d} x\right) c_{1, j} \\
			&-\Delta t \int_{\Omega} f \varphi_{i} \mathrm{~d} x-\Delta t \int_{\partial \Omega_{N}} g \varphi_{i} \mathrm{~d} s
		\end{aligned}
		$$
		We introduce the matrices $M$ and $K$ with entries $M_{i, j}=\int_{\Omega} \varphi_{i} \varphi_{j} \mathrm{~d} x$ and $K_{i, j}=$ $\int_{\Omega} \alpha \nabla \varphi_{i} \cdot \nabla \varphi_{j} \mathrm{~d} x$, respectively. In addition, we define the vectors $c, c_{1}$, and $f$ with entries $c_{i}, c_{1, i}$, and $\int_{\Omega} f \varphi_{i} \mathrm{~d} x-\int_{\partial \Omega_{N}} g \varphi_{i} \mathrm{~d} s$. The equation system can then be written as
		$$
		M c=M c_{1}-\Delta t K c_{1}+\Delta t f .
		$$
		When $M, K$, and $b$ are assembled without paying attention to Dirichlet boundary conditions, we need to replace equation $k$ by $c_{k}=U_{k}$ for $k$ corresponding to all boundary nodes $\left(k \in I_{b}\right)$. The modification of $M$ consists in setting $M_{k, j}=0, j \in \mathcal{I}_{s}$, and the $M_{k, k}=1$. Alternatively, a modification that preserves the symmetry of $M$ can be applied. At each time level one forms $b=M c_{1}-\Delta t K c_{1}+\Delta t f$ and sets $b_{k}=U_{k}^{n+1}, k \in I_{b}$, and solves the system $M c=b$.
		
		In case of a Backward Euler method, the system becomes (\ref{eqa233}). We can write the system as $A c=b$, with $A=M+\Delta t K$ and $b=M c_{1}+f$. Both $M$ and $K$ needs to be modified because of Dirichlet boundary conditions, but the diagonal entries in $K$ should be set to zero and those in $M$ to unity. In this way, $A_{k, k}=1$. The right-hand side must read $b_{k}=U_{k}^{n}$ for $k \in I_{b}$ (assuming the unknown is sought at time level $\left.t_{n}\right)$.
	\section[Example: Oscillating Dirichlet boundary condition]{Example: Oscillating Dirichlet boundary condition}
		\label{sec:sec_19_9}
		 \noindent We shall address the one-dimensional initial-boundary value problem
		 
		\begin{equation}
		\label{240}
			u_{t}=\left(\alpha u_{x}\right)_{x}+f, \quad \boldsymbol{x} \in \Omega=[0, L], t \in(0, T]
		\end{equation}
	
		\begin{equation}
	 	\label{241}
			u(x, 0)=0, \quad \boldsymbol{x} \in \Omega,
		\end{equation}
	
		\begin{equation}
		\label{eqa242}
			u(0, t)=a \sin \omega t, \quad t \in(0, T),
		\end{equation}
	
		\begin{equation}
		\label{eqa243}
			u_{x}(L, t)=0, \quad t \in(0, T] .
		\end{equation}
	
		 \noindent A physical interpretation may be that $u$ is the temperature deviation from a constant mean temperature in a body $\Omega$ that is subject to an oscillating temperature (e.g., day and night, or seasonal, variations) at $x=0$.
		 
		 We use a Backward Euler scheme in time and P1 elements of constant length $h$ in space. Incorporation of the Dirichlet condition at $x=0$ through modifying the linear system at each time level means that we carry out the computations as explained in Section \ref{sec:sec_19_7} and get a system (\ref{eqa233}). The $M$ and $K$ matrices computed without paying attention to Dirichlet boundary conditions become
		 
	\begin{equation}
		\label{244}
		 M=\frac{h}{6}\left(
		 \begin{array}{ccccccccc}2 & 1 & 0 & \cdots & \cdots & \cdots & \cdots & \cdots & 0 \\ 1 & 4 & 1 & \ddots & & & & & \vdots \\ 0 & 1 & 4 & 1 & \ddots & & & & \vdots \\ \vdots & \ddots & & \ddots & \ddots & 0 & & & \vdots \\ \vdots & & \ddots & \ddots & \ddots & \ddots & \ddots & & \vdots \\ \vdots & & & 0 & 1 & 4 & 1 & \ddots & \vdots \\ \vdots & & & & \ddots & \ddots & \ddots & \ddots & 0 \\ \vdots & & & & & \ddots & 1 & 4 & 1 \\ 0 & \cdots & \cdots & \cdots & \cdots & \cdots & 0 & 1 & 2\end{array}\right)
	\end{equation}	 
	
	\begin{equation}
		\label{245}
		 K=\frac{\alpha}{h}\left(\begin{array}{ccccccccc}1 & -1 & 0 & \cdots & \cdots & \cdots & \cdots & \cdots & 0 \\ -1 & 2 & -1 & \ddots & & & & & \vdots \\ 0 & -1 & 2 & -1 & \ddots & & & & \vdots \\ \vdots & \ddots & & \ddots & \ddots & 0 & & & \vdots \\ \vdots & & \ddots & \ddots & \ddots & \ddots & \ddots & & \vdots \\ \vdots & & & 0 & -1 & 2 & -1 & \ddots & \vdots \\ \vdots & & & & \ddots & \ddots & \ddots & \ddots & 0 \\ \vdots & & & & & \ddots & -1 & 2 & -1 \\ 0 & \cdots & \cdots & \cdots & \cdots & \cdots & 0 & -1 & 1\end{array}\right)
	\end{equation}
		 
		 \noindent The right-hand side of the variational form contains $M c_{1}$ since there is no source term $(f)$ and no boundary term from the integration by parts $\left(u_{x}=0\right.$ at $x=L$ and we compute as if $u_{x}=0$ at $x=0$ too). We must incorporate the Dirichlet boundary condition $c_{0}=a \sin \omega t_{n}$ by ensuring that this is the first equation in the linear system. To this end, the first row in $K$ and $M$ are set to zero, but the diagonal entry $M_{0,0}$ is set to 1 . The right-hand side is $b=M c_{1}$, and we set $b_{0}=a \sin \omega t_{n}$. Note that in this approach, $N=N_{n}$, and $c$ equals the unknown $u$ at each node in the mesh. We can write the complete linear system as
		 
	\begin{equation}
		\label{eqa246}
		c_{0}=a \sin \omega t_{n},
	\end{equation}

	\begin{equation}
	\label{eqa247}
		\frac{h}{6}\left(c_{i-1}+4 c_{i}+c_{i+1}\right)+\Delta t \frac{\alpha}{h}\left(-c_{i-1}+2 c_{i}+c_{i+1}\right)=\frac{h}{6}\left(c_{1, i-1}+4 c_{1, i}+c_{1, i+1}\right),
	\end{equation}

		 $i=1, \ldots, N_{n}-1$,
		 
	\begin{equation}
	\label{eqa248}
		\frac{h}{6}\left(c_{i-1}+2 c_{i}\right)+\Delta t \frac{\alpha}{h}\left(-c_{i-1}+c_{i}\right)=\frac{h}{6}\left(c_{1, i-1}+2 c_{1, i}\right), \quad i=N_{n} .
	\end{equation}
		 
		 The Dirichlet boundary condition can alternatively be implemented through a boundary function $B(x, t)=a \sin \omega t \varphi_{0}(x)$ :
		 $$
		 u^{n}(x)=a \sin \omega t_{n} \varphi_{0}(x)+\sum_{j \in \mathcal{I}_{x}} c_{j} \varphi_{\nu(j)}(x), \quad \nu(j)=j+1 .
		 $$
		 Now, $N=N_{n}-1$ and the $c$ vector contains values of $u$ at nodes $1,2, \ldots, N_{n}$. The right-hand side gets a contribution
		 
	\begin{equation}
	\label{249}
		\int_{0}^{L}\left(a\left(\sin \omega t_{n}-\sin \omega t_{n-1}\right) \varphi_{0} \varphi_{i}-\Delta t \alpha a \sin \omega t_{n} \nabla \varphi_{0} \cdot \nabla \varphi_{i}\right) \mathrm{d} x .
	\end{equation}

	\section[Analysis of the discrete equations]{Analysis of the discrete equations}
		\label{sec:sec_19_10}
		\noindent The diffusion equation $u_{t}=\alpha u_{x x}$ allows a (Fourier) wave component $u=$ $\exp (\beta t+i k x)$ as solution if $\beta=-\alpha k^{2}$, which follows from inserting the wave component in the equation. The exact wave component can alternatively be written as
	
	\begin{equation}
	\label{250}
		u=A_{\mathrm{e}}^{n} e^{i k x}, \quad A_{\mathrm{e}}=e^{-\alpha k^{2} \Delta t}
	\end{equation}

		\noindent Many numerical schemes for the diffusion equation has a similar wave component as solution:
		
	\begin{equation}
	\label{eqa251}
		u_{q}^{n}=A^{n} e^{i k x},
	\end{equation}

		\noindent where is an amplification factor to be calculated by inserting (\ref{eqa252}) in the scheme. We introduce $x=q h$, or $x=q \Delta x$ to align the notation with that frequently used in finite difference methods.
		
		A convenient start of the calculations is to establish some results for various finite difference operators acting on
		
	\begin{equation}
	\label{eqa252}
		u_{q}^{n}=A^{n} e^{i k q \Delta x}
	\end{equation}
		
		$$
		\begin{aligned}
			{\left[D_{t}^{+} A^{n} e^{i k q \Delta x}\right]^{n} } &=A^{n} e^{i k q \Delta x} \frac{A-1}{\Delta t} \\
			{\left[D_{t}^{-} A^{n} e^{i k q \Delta x}\right]^{n} } &=A^{n} e^{i k q \Delta x} \frac{1-A^{-1}}{\Delta t} \\
			{\left[D_{t} A^{n} e^{i k q \Delta x}\right]^{n+\frac{1}{2}} } &=A^{n+\frac{1}{2}} e^{i k q \Delta x} \frac{A^{\frac{1}{2}}-A^{-\frac{1}{2}}}{\Delta t}=A^{n} e^{i k q \Delta x} \frac{A-1}{\Delta t} \\
			{\left[D_{x} D_{x} A^{n} e^{i k q \Delta x}\right]_{q} } &=-A^{n} \frac{4}{\Delta x^{2}} \sin ^{2}\left(\frac{k \Delta x}{2}\right)
		\end{aligned}
		$$
		\textbf{Forward Euler discretization.   } We insert (\ref{eqa252}) in the Forward Euler scheme with P1 elements in space and $f=0$ (this type of analysis can only be carried out if $f=0$ ),
		
	\begin{equation}
	\label{253}
		\left[D_{t}^{+}\left(u+\frac{1}{6} h^{2} D_{x} D_{x} u\right)=\alpha D_{x} D_{x} u\right]_{q}^{n}
	\end{equation}

		We have
		$$
		\left[D_{t}^{+} D_{x} D_{x} A e^{i k x}\right]_{q}^{n}=\left[D_{t}^{+} A\right]^{n}\left[D_{x} D_{x} e^{i k x}\right]_{q}=-A^{n} e^{i k p \Delta x} \frac{A-1}{\Delta t} \frac{4}{\Delta x^{2}} \sin ^{2}\left(\frac{k \Delta x}{2}\right)
		$$
		The term $\left[D_{t}^{+} A e^{i k x}+\frac{1}{6} \Delta x^{2} D_{t}^{+} D_{x} D_{x} A e^{i k x}\right]_{q}^{n}$ then reduces to
		$$
		\frac{A-1}{\Delta t}-\frac{1}{6} \Delta x^{2} \frac{A-1}{\Delta t} \frac{4}{\Delta x^{2}} \sin ^{2}\left(\frac{k \Delta x}{2}\right)
		$$
		Or
		$$
		\frac{A-1}{\Delta t}\left(1-\frac{2}{3} \sin ^{2}(k \Delta x / 2)\right)
		$$
		Introducing $p=k \Delta x / 2$ and $C=\alpha \Delta t / \Delta x^{2}$, the complete scheme becomes
		$$
		(A-1)\left(1-\frac{2}{3} \sin ^{2} p\right)=-4 C \sin ^{2} p
		$$
		from which we find $A$ to be
		$$
		A=1-4 C \frac{\sin ^{2} p}{1-\frac{2}{3} \sin ^{2} p}
		$$\smallbreak 
		How does this $A$ change the stability criterion compared to the Forward Euler finite difference scheme and centered differences in space? The stability criterion is $|A| \leq 1$, which here implies $A \leq 1$ and $A \geq-1$. The former is always fulfilled, while the latter leads to
		$$
		4 C \frac{\sin ^{2} p}{1+\frac{2}{3} \sin ^{2} p} \leq 2
		$$
		The factor $\sin ^{2} p /\left(1-\frac{2}{3} \sin ^{2} p\right)$ can be plotted for $p \in[0, \pi / 2]$, and the maximum value goes to 3 as $p \rightarrow \pi / 2$. The worst case for stability therefore occurs for the shortest possible wave, $p=\pi / 2$, and the stability criterion becomes

	\begin{equation}
	\label{eqa254}
		C \leq \frac{1}{6} \Rightarrow \Delta t \leq \frac{\Delta x^{2}}{6 \alpha}
	\end{equation}

		\noindent which is a factor $1 / 3$ worse than for the standard Forward Euler finite difference method for the diffusion equation, which demands $C \leq 1 / 2$. Lumping the mass matrix will, however, recover the finite difference method and therefore imply $C \leq 1 / 2$ for stability.
		
		\noindent \textbf{Backward Euler discretization.   } We can use the same approach and insert (\ref{eqa252}) in the Backward Euler scheme with P1 elements in space and $f=0$ :

	\begin{equation}
		\label{eqa255}
		\left[D_{t}^{-}\left(u+\frac{1}{6} h^{2} D_{x} D_{x} u\right)=\alpha D_{x} D_{x} u\right]_{i}^{n} \text {. }
	\end{equation}

		Similar calculations as in the Forward Euler case lead to
		$$
		\left(1-A^{-1}\right)\left(1-\frac{2}{3} \sin ^{2} p\right)=-4 C \sin ^{2} p
		$$
		and hence
		$$
		A=\left(1+4 C \frac{\sin ^{2} p}{1-\frac{2}{3} \sin ^{2} p}\right)^{-1}
		$$
		\noindent \textbf{Comparing amplification factors. } It is of interest to compare $A$ and $A_{\mathrm{e}}$ as functions of $p$ for some $C$ values. Figure 48 display the amplification factors for the Backward Euler scheme corresponding a coarse mesh with $C=2$ and a mesh at the stability limit of the Forward Euler scheme in the finite difference method, $C=1 / 2$. Figures 49 and 50 shows how the accuracy increases with lower $C$ values for both the Forward Euler and Backward schemes, respectively. The striking fact, however, is that the accuracy of the finite element method is significantly less than the finite difference method for the same value of $C$. Lumping the mass matrix to recover the numerical amplification factor $A$ of the finite difference method is therefore a good idea in this problem. Remaining tasks:
		\begin{itemize}
			\item Taylor expansion of the error in the amplification factor $A_{\mathrm{e}}-A$
			\item Taylor expansion of the error $e=\left(A_{\mathrm{e}}^{n}-A^{n}\right) e^{i k x}$
			\item $L^{2}$ norm of $e$
		\end{itemize}
		
		\begin{figure}
			\centering
			\includegraphics[width=0.7\linewidth]{img_50}
			\caption{Comparison of coarse-mesh amplification factors for Backward Euler discretization of a 1D diffusion-equation.}
			\label{fig:img_48}
		\end{figure}
	
		\begin{figure}
			\centering
			\includegraphics[width=0.7\linewidth]{img_51}
			\caption{Comparison of fine-mesh amplification factors for Forward Euler discretization of a 1D diffusion equation.}
			\label{fig:img_49}
		\end{figure}
	
		\begin{figure}
			\centering
			\includegraphics[width=0.7\linewidth]{img_52}
			\caption{Comparison of fine-mesh amplification factors for Backward Euler discretization of a 1D diffusion equation.}
			\label{fig:img_50}
		\end{figure}



\chapter{Systems of differential equations}
\label{chap:chap_20}
\pagenumbering{arabic}

\noindent Many mathematical models involve $m+1$ unknown functions governed by a system of $m+1$ differential equations. In abstract form we may denote the unknowns by $u^{(0)}, \ldots, u^{(m)}$ and write the governing equations as
$$
\begin{gathered}
	\mathcal{L}_{0}\left(u^{(0)}, \ldots, u^{(m)}\right)=0 \\
	\vdots \\
	\mathcal{L}_{m}\left(u^{(0)}, \ldots, u^{(m)}\right)=0
\end{gathered}
$$
where $\mathcal{L}_{i}$ is some differential operator defining differential equation number $i$.\smallbreak
	\section[Systems of differential equations]{Variational forms}
		\label{sec:sec_20_1}
		\noindent There are basically two ways of formulating a variational form for a system of differential equations. The first method treats each equation independently as a scalar equation, while the other method views the total system as a vector equation with a vector function as unknown.
		
		Let us start with the one equation at a time approach. We multiply equation number $i$ by some test function $v^{(i)} \in V^{(i)}$ and integrate over the domain:
		
		\begin{equation}
		\label{eqa256}
			\int_{\Omega} \mathcal{L}^{(0)}\left(u^{(0)}, \ldots, u^{(m)}\right) v^{(0)} \mathrm{d} x=0 \\
		\end{equation}
			
		\begin{equation}
		\label{eqa257}
			\vdots \\
		\end{equation}
	
		\begin{equation}
		\label{eqa258}
			\int_{\Omega} \mathcal{L}^{(m)}\left(u^{(0)}, \ldots, u^{(m)}\right) v^{(m)} \mathrm{d} x=0
		\end{equation}
	
		\noindent Terms with second-order derivatives may be integrated by parts, with Neumann conditions inserted in boundary integrals. Let
		$$
		V^{(i)}=\operatorname{span}\left\{\psi_{0}^{(i)}, \ldots, \psi_{N_{i}}^{(i)}\right\},
		$$
		such that
		$$
		u^{(i)}=B^{(i)}(\boldsymbol{x})+\sum_{j=0}^{N_{i}} c_{j}^{(i)} \psi_{j}^{(i)}(\boldsymbol{x}),
		$$
		where $B^{(i)}$ is a boundary function to handle nonzero Dirichlet conditions. Observe that different unknowns live in different spaces with different basis functions and numbers of degrees of freedom.
		
		From the $m$ equations in the variational forms we can derive $m$ coupled systems of algebraic equations for the $\prod_{i=0}^{m} N_{i}$ unknown coefficients $c_{j}^{(i)}, j=$ $0, \ldots, N_{i}, i=0, \ldots, m$.\smallbreak
		
		The alternative method for deriving a variational form for a system of differential equations introduces a vector of unknown functions
		$$
		\boldsymbol{u}=\left(u^{(0)}, \ldots, u^{(m)}\right)
		$$
		a vector of test functions
		$$
		\boldsymbol{v}=\left(u^{(0)}, \ldots, u^{(m)}\right)
		$$
		with
		$$
		\boldsymbol{u}, \boldsymbol{v} \in \boldsymbol{V}=V^{(0)} \times \cdots \times V^{(m)} .
		$$
		With nonzero Dirichlet conditions, we have a vector $\boldsymbol{B}=\left(B^{(0)}, \ldots, B^{(m)}\right)$ with boundary functions and then it is $u-B$ that lies in $V$, not $u$ itself. \smallbreak
		The governing system of differential equations is written
		$$
		\mathcal{L}(\boldsymbol{u})=0,
		$$
		where
		$$
		\mathcal{L}(\boldsymbol{u})=\left(\mathcal{L}^{(0)}(\boldsymbol{u}), \ldots, \mathcal{L}^{(m)}(\boldsymbol{u})\right)
		$$
		The variational form is derived by taking the inner product of the vector of equations and the test function vector:
		
		\begin{equation}
			\label{eqa259}
			\int_{\Omega} \mathcal{L}(\boldsymbol{u}) \cdot \boldsymbol{v}=0 \quad \forall \boldsymbol{v} \in \boldsymbol{V} .
		\end{equation}
	
		Observe that (\ref{eqa259}) is one scalar equation. To derive systems of algebraic equations for the unknown coefficients in the expansions of the unknown functions, one chooses $m$ linearly independent $\boldsymbol{v}$ vectors to generate $m$ independent variational forms from (\ref{eqa259}). The particular choice $v=\left(v^{(0)}, 0, \ldots, 0\right)$ recovers (\ref{eqa256}), $\boldsymbol{v}=\left(0, \ldots, 0, v^{(m)}\right.$ recovers $(258)$, and $\boldsymbol{v}=\left(0, \ldots, 0, v^{(i)}, 0, \ldots, 0\right)$ recovers the variational form number i, $\int_{\Omega} \mathcal{L}^{(i)} v^{(i)} \mathrm{d} x=0$, in (\ref{eqa256})-(\ref{eqa258}).\bigbreak
	\section[A worked example]{A worked example}
		\label{sec:sec_20_2}
		\noindent We now consider a specific system of two partial differential equations in two space dimensions:
		
		\begin{equation}
			\label{eqa260}
			\mu \nabla^{2} w=-\beta \\
		\end{equation}
	
		\begin{equation}
			\label{eqa261}
			\kappa \nabla^{2} T=-\mu\|\nabla w\|^{2}
		\end{equation}
		
		\noindent The unknown functions $w(x, y)$ and $T(x, y)$ are defined in a domain $\Omega$, while $\mu, \beta$, and $\kappa$ are given constants. The norm in (\ref{eqa261}) is the standard Eucledian norm:
		$$
		\|\nabla w\|^{2}=\nabla w \cdot \nabla w=w_{x}^{2}+w_{y}^{2}
		$$
		The boundary conditions associated with (\ref{eqa260})-(\ref{eqa261}) are $w=0$ on $\partial \Omega$ and $T=T_{0}$ on $\partial \Omega$. Each of the equations (\ref{eqa260}) and (\ref{eqa261}) need one condition at each point on the boundary.
		
		The system (\ref{eqa260})-(\ref{eqa261}) arises from fluid flow in a straight pipe, with the $z$ axis in the direction of the pipe. The domain $\Omega$ is a cross section of the pipe, $w$ is the velocity in the $z$ direction, $\mu$ is the viscosity of the fluid, $\beta$ is the pressure gradient along the pipe, $T$ is the temperature, and $\kappa$ is the heat conduction coefficient of the fluid. The equation $(260)$ comes from the Navier-Stokes equations, and (261) follows from the energy equation. The term $-\mu\|\nabla w\|^{2}$ models heating of the fluid due to internal friction.
		
		Observe that the system (\ref{eqa260})-(\ref{eqa261}) has only a one-way coupling: $T$ depends on $w$, but $w$ does not depend on $T$, because we can solve (\ref{eqa260}) with respect to $w$ and then (\ref{eqa261}) with respect to $T$. Some may argue that this is not a real system of PDEs, but just two scalar PDEs. Nevertheless, the one-way coupling is convenient when comparing different variational forms and different implementations.
	\section[Identical function spaces for the unknowns]{Identical function spaces for the unknowns}
		\label{sec:sec_20_3}
		\noindent Let us first apply the same function space $V$ for $w$ and $T$ (or more precisely, $w \in V$ and $\left.T-T_{0} \in V\right)$. With
		$$
		V=\operatorname{span}\left\{\psi_{0}(x, y), \ldots, \psi_{N}(x, y)\right\}
		$$
		we write
		
		\begin{equation}
			\label{eqa262}
		w=\sum_{j=0}^{N} c_{j}^{(w)} \psi_{j}, \quad T=T_{0}+\sum_{j=0}^{N} c_{j}^{(T)} \psi_{j}
		\end{equation}
	
		\noindent Note that $w$ and $T$ in (\ref{eqa260})-(\ref{eqa261}) denote the exact solution of the PDEs, while $w$ and T (\ref{eqa262}) are the discrete functions that approximate the exact solution. It should be clear from the context whether a symbol means the exact or approximate solution, but when we need both at the same time, we use a subscript e to denote the exact solution.\bigbreak
		
		\noindent \textbf{Variational form of each individual PDE.   } Inserting the expansions (\ref{eqa262}) in the governing PDEs, results in a residual in each equation,
		
		\begin{equation}
			\label{eqa263}
			R_{w}=\mu \nabla^{2} w+\beta \\
		\end{equation}
	
		\begin{equation}
			\label{eqa264}
			R_{T}=\kappa \nabla^{2} T+\mu\|\nabla w\|^{2}
		\end{equation}
		
		\noindent A Galerkin method demands $R_{w}$ and $R_{T}$ do be orthogonal to $V$ :
		$$
		\begin{aligned}
			&\int_{\Omega} R_{w} v \mathrm{~d} x=0 \quad \forall v \in V \\
			&\int_{\Omega} R_{T} v \mathrm{~d} x=0 \quad \forall v \in V
		\end{aligned}
		$$
		Because of the Dirichlet conditions, $v=0$ on $\partial \Omega$. We integrate the Laplace terms by parts and note that the boundary terms vanish since $v=0$ on $\partial \Omega$ :
		
		\begin{equation}
			\label{eqa265}
			\int_{\Omega} \mu \nabla w \cdot \nabla v \mathrm{~d} x=\int_{\Omega} \beta v \mathrm{~d} x \quad \forall v \in V \\
		\end{equation}
	
		\begin{equation}
			\label{eqa266}
			\int_{\Omega} \kappa \nabla T \cdot \nabla v \mathrm{~d} x=\int_{\Omega} \mu \nabla w \cdot \nabla w v \mathrm{~d} x \quad \forall v \in V
		\end{equation}
		
		\noindent \textbf{Compound scalar variational form.   } The alternative way of deriving the variational from is to introduce a test vector function $v \in V=V \times V$ and take the inner product of $v$ and the residuals, integrated over the domain:
		$$
		\int_{\Omega}\left(R_{w}, R_{T}\right) \cdot \boldsymbol{v} \mathrm{d} x=0 \quad \forall \boldsymbol{v} \in \boldsymbol{V}
		$$
		With $\boldsymbol{v}=\left(v_{0}, v_{1}\right)$ we get
		$$
		\int_{\Omega}\left(R_{w} v_{0}+R_{T} v_{1}\right) \mathrm{d} x=0 \quad \forall v \in \boldsymbol{V} .
		$$
		Integrating the Laplace terms by parts results in
		
		\begin{equation}
			\label{eqa267}
			\int_{\Omega}\left(\mu \nabla w \cdot \nabla v_{0}+\kappa \nabla T \cdot \nabla v_{1}\right) \mathrm{d} x=\int_{\Omega}\left(\beta v_{0}+\mu \nabla w \cdot \nabla w v_{1}\right) \mathrm{d} x, \quad \forall v \in \boldsymbol{V} . \text { (267) }
		\end{equation}
	
		\noindent Choosing $v_{0}=v$ and $v_{1}=0$ gives the variational form (\ref{eqa265}), while $v_{0}=0$ and $v_{1}=v$ gives (\ref{eqa266})\smallbreak
		With the inner product notation, $(p, q)=\int_{\Omega} p q \mathrm{~d} x$, we can alternatively write (\ref{eqa265}) and (\ref{eqa266}) as
		$$
		\begin{aligned}
			&(\mu \nabla w, \nabla v)=(\beta, v) \quad \forall v \in V \\
			&(\kappa \nabla T, \nabla v)=(\mu \nabla w, \nabla w, v) \quad \forall v \in V
		\end{aligned}
		$$
		or since $\mu$ and $\kappa$ are considered constant,
		
		\begin{equation}
			\label{eqa268}
			\mu(\nabla w, \nabla v)=(\beta, v) \quad \forall v \in V \\
		\end{equation}
	
		\begin{equation}
		\label{eqa269}
			\kappa(\nabla T, \nabla v)=\mu(\nabla w \cdot \nabla w, v) \quad \forall v \in V
		\end{equation}
		
		\noindent \textbf{Decoupled linear systems.   } The linear systems governing the coefficients $c_{j}^{(w)}$ and $c_{j}^{(T)}, j=0, \ldots, N$, are derived by inserting the expansions (\ref{eqa262}) in (\ref{eqa265}) and (\ref{eqa266}), and choosing $v=\psi_{i}$ for $i=0, \ldots, N$. The result becomes
	
		\begin{equation}
			\label{eqa270}
			\sum_{j=0}^{N} A_{i, j}^{(w)} c_{j}^{(w)}=b_{i}^{(w)}, \quad i=0, \ldots, N, \\
		\end{equation}
	
		\begin{equation}
			\label{eqa271}
			\sum_{j=0}^{N} A_{i, j}^{(T)} c_{j}^{(T)}=b_{i}^{(T)}, \quad i=0, \ldots, N, \\
		\end{equation}
	
		\begin{equation}
			\label{eqa272}
			A_{i, j}^{(w)} =\mu\left(\nabla \psi_{j}, \nabla \psi_{i}\right) \\
		\end{equation}
	
		\begin{equation}
			\label{eqa273}
			b_{i}^{(w)} =\left(\beta, \psi_{i}\right), \\
		\end{equation}
	
		\begin{equation}
			\label{eqa274}
			A_{i, j}^{(T)} =\kappa\left(\nabla \psi_{j}, \nabla \psi_{i}\right), \\
		\end{equation}
	
		\begin{equation}
			\label{eqa275}
			b_{i}^{(T)} =\mu\left(\left(\sum_{j} c_{j}^{(w)} \nabla \psi_{j}\right) \cdot\left(\sum_{k} c_{k}^{(w)} \nabla \psi_{k}\right), \psi_{i}\right) .
		\end{equation}
	
		It can also be instructive to write the linear systems using matrices and vectors. Define $K$ as the matrix corresponding to the Laplace operator $\nabla^{2}$. That is, $K_{i, j}=\left(\nabla \psi_{j}, \nabla \psi_{i}\right)$. Let us introduce the vectors
		$$
		\begin{aligned}
			b^{(w)} &=\left(b_{0}^{(w)}, \ldots, b_{N}^{(w)}\right) \\
			b^{(T)} &=\left(b_{0}^{(T)}, \ldots, b_{N}^{(T)}\right) \\
			c^{(w)} &=\left(c_{0}^{(w)}, \ldots, c_{N}^{(w)}\right) \\
			c^{(T)} &=\left(c_{0}^{(T)}, \ldots, c_{N}^{(T)}\right)
		\end{aligned}
		$$
		The system (\ref{eqa270})-(\ref{eqa271}) can now be expressed in matrix-vector form as
		
		\begin{equation}
			\label{eqa276}
			\mu K c^{(w)}=b^{(w)} \\
		\end{equation}
	
		\begin{equation}
			\label{eqa277}
			\kappa K c^{(T)}=b^{(T)}
		\end{equation}
		
		We can solve the first system for $c^{(w)}$, and then the right-hand side $b^{(T)}$ is known such that we can solve the second system for $c^{(T)}$.\bigbreak
		
		\noindent \textbf{Coupled linear systems.   } Despite the fact that $w$ can be computed first, without knowing $T$, we shall now pretend that $w$ and $T$ enter a two-way coupling such that we need to derive the algebraic equations as one system for all the unknowns $c_{j}^{(w)}$ and $c_{j}^{(T)}, j=0, \ldots, N$. This system is nonlinear in $c_{j}^{(w)}$ because of the $\nabla w \cdot \nabla w$ product. To remove this nonlinearity, imagine that we introduce an iteration method where we replace $\nabla w \cdot \nabla w$ by $\nabla w_{-} \cdot \nabla w$, weing the $w$ computed in the previous iteration. Then the term $\nabla w_{-} \cdot \nabla w$ is linear in $w$ since $w_{-}$is known. The total linear system becomes
		
		\begin{equation}
			\label{eqa278}
			\sum_{j=0}^{N} A_{i, j}^{(w, w)} c_{j}^{(w)}+\sum_{j=0}^{N} A_{i, j}^{(w, T)} c_{j}^{(T)} =b_{i}^{(w)}, \quad i=0, \ldots, N, \\
		\end{equation}
	
		\begin{equation}
			\label{eqa279}
			\sum_{j=0}^{N} A_{i, j}^{(T, w)} c_{j}^{(w)}+\sum_{j=0}^{N} A_{i, j}^{(T, T)} c_{j}^{(T)} =b_{i}^{(T)}, \quad i=0, \ldots, N, \\
		\end{equation}
	
		\begin{equation}
			\label{eqa280}
			A_{i, j}^{(w, w)} =\mu\left(\nabla \psi_{j}, \psi_{i}\right), \\
		\end{equation}
	
		\begin{equation}
			\label{eqa281}
			A_{i, j}^{(w, T)} =0, \\
		\end{equation}
	
		\begin{equation}
			\label{eqa282}
			b_{i}^{(w)} =\left(\beta, \psi_{i}\right), \\
		\end{equation}

		\begin{equation}
			\label{eqa283}	
			A_{i, j}^{(w, T)} \left.=\mu\left(\left(\nabla \psi_{-}\right) \cdot \nabla \psi_{j}\right), \psi_{i}\right), \\
		\end{equation}
	
		\begin{equation}
			\label{eqa284}
			A_{i, j}^{(T, T)}=\kappa\left(\nabla \psi_{j}, \psi_{i}\right), \\
		\end{equation}
	
		\begin{equation}
			\label{eqa285}
			b_{i}^{(T)}=0 .
		\end{equation}
		
		\noindent This system can alternatively be written in matrix-vector form as
		
		\begin{equation}
			\label{eqa286}
			\mu K c^{(w)}=0 b^{(w)}, \\
		\end{equation}

		\begin{equation}
			\label{eqa287}
			L c^{(w)}+\kappa K c^{(T)}=0
		\end{equation}
	
		\noindent with $L$ as the matrix from the $\nabla w_{-} \cdot \nabla$ operator: $L_{i, j}=A_{i, j}^{(w, T)}$.\smallbreak
		The matrix-vector equations are often conveniently written in block form:
		$$
		\left(\begin{array}{cc}
			\mu K & 0 \\
			L & \kappa K
		\end{array}\right)\left(\begin{array}{c}
			c^{(w)} \\
			c^{(T)}
		\end{array}\right)=\left(\begin{array}{c}
			b^{(w)} \\
			0
		\end{array}\right)
		$$\smallbreak
		Note that in the general case where all unknowns enter all equations, we have to solve the compound system (\ref{eqa297})-(\ref{eqa298}) since then we cannot utilize the special property that (\ref{eqa270}) does not involve $T$ and can be solved first.
		
		When the viscosity depends on the temperature, the $\mu \nabla^{2} w$ term must be replaced by $\nabla \cdot(\mu(T) \nabla w)$, and then $T$ enters the equation for $w$. Now we have a two-way coupling since both equations contain $w$ and $T$ and therefore must be solved simultaneously Th equation $\nabla \cdot(\mu(T) \nabla w)=-\beta$ is nonlinear, and if some iteration procedure is invoked, where we use a previously computed $T_{-}$in the viscosity $\left(\mu\left(T_{-}\right)\right)$, the coefficient is known, and the equation involves only one unknown, $w$. In that case we are back to the one-way coupled set of PDEs.
		We may also formulate our PDE system as a vector equation. To this end, we introduce the vector of unknowns $\boldsymbol{u}=\left(u^{(0)}, u^{(1)}\right)$, where $u^{(0)}=w$ and $u^{(1)}=T$. We then have
		$$
		\nabla^{2} u=\left(\begin{array}{c}
			-\mu^{-1} \beta \\
			-\kappa^{-1} \mu \nabla u^{(0)} \cdot \nabla u^{(0)}
		\end{array}\right)
		$$
	\section[Different function spaces for the unknowns]{Different function spaces for the unknowns}
		\label{sec:sec_20_4}
		\noindent It is easy to generalize the previous formulation to the case where $w \in V^{(w)}$ and $T \in V^{(T)}$, where $V^{(w)}$ and $V^{(T)}$ can be different spaces with different numbers of degrees of freedom. For example, we may use quadratic basis functions for $w$ and linear for $T$. Approximation of the unknowns by different finite element spaces is known as \emph{mixed finite} element methods.\smallbreak
		We write
		$$
		\begin{aligned}
			&V^{(w)}=\operatorname{span}\left\{\psi_{0}^{(w)}, \ldots, \psi_{N_{w}}^{(w)}\right\}, 
			&V^{(T)}=\operatorname{span}\left\{\psi_{0}^{(T)}, \ldots, \psi_{N_{T}}^{(T)}\right\} .
		\end{aligned}
		$$
		The next step is to multiply (\ref{eqa260}) by a test function $v^{(w)} \in V^{(w)}$ and (\ref{eqa261}) by a $v^{(T)} \in V^{(T)}$, integrate by parts and arrive at
		
		
		\begin{equation}
			\label{eqa288}
			\int_{\Omega} \mu \nabla w \cdot \nabla v^{(w)} \mathrm{d} x=\int_{\Omega} \beta v^{(w)} \mathrm{d} x \quad \forall v^{(w)} \in V^{(w)} \\
		\end{equation}
	
		\begin{equation}
			\label{eqa289}
			\int_{\Omega} \kappa \nabla T \cdot \nabla v^{(T)} \mathrm{d} x=\int_{\Omega} \mu \nabla w \cdot \nabla w v^{(T)} \mathrm{d} x \quad \forall v^{(T)} \in V^{(T)}
		\end{equation}
	
		The compound scalar variational formulation applies a test vector function $\boldsymbol{v}=\left(v^{(w)}, v^{(T)}\right)$ and reads
		
		\begin{equation}
			\label{eqa290}
			\int_{\Omega}\left(\mu \nabla w \cdot \nabla v^{(w)}+\kappa \nabla T \cdot \nabla v^{(T)}\right) \mathrm{d} x=\int_{\Omega}\left(\beta v^{(w)}+\mu \nabla w \cdot \nabla w v^{(T)}\right) \mathrm{d} x
		\end{equation}
		
		\noindent valid $\forall \boldsymbol{v} \in \boldsymbol{V}=V^{(w)} \times V^{(T)}$.\smallbreak
		The associated linear system is similar to (\ref{eqa270})-(\ref{eqa271}) or (\ref{eqa297})-(\ref{eqa298}), except that we need to distinguish between $\psi_{i}^{(w)}$ and $\psi_{i}^{(T)}$, and the range in the sums over $j$ must match the number of degrees of freedom in the spaces $V^{(w)}$ and $V^{(T)}$. The formulas become
		
		\begin{equation}
			\label{eqa291}
			\sum_{j=0}^{N_{w}} A_{i, j}^{(w)} c_{j}^{(w)}=b_{i}^{(w)}, \quad i=0, \ldots, N_{w}, \\
		\end{equation}
		
		\begin{equation}
			\label{eqa292}
			\sum_{j=0}^{N_{T}} A_{i, j}^{(T)} c_{j}^{(T)}=b_{i}^{(T)}, \quad i=0, \ldots, N_{T}, \\
		\end{equation}
	
		\begin{equation}
			\label{eqa293}
			A_{i, j}^{(w)}=\mu\left(\nabla \psi_{j}^{(w)}, \psi_{i}^{(w)}\right) \\
		\end{equation}
	
		\begin{equation}
			\label{eqa294}
			b_{i}^{(w)}=\left(\beta, \psi_{i}^{(w)}\right), \\
		\end{equation}
	
		\begin{equation}
			\label{eqa295}
			A_{i, j}^{(T)}=\kappa\left(\nabla \psi_{j}^{(T)}, \psi_{i}^{(T)}\right) \\
		\end{equation}
	
		\begin{equation}
			\label{eqa296}
			b_{i}^{(T)}=\mu\left(\nabla w_{-}, \psi_{i}^{(T)}\right)
		\end{equation}
		
		In the case we formulate one compound linear system involving both $c_{j}^{(w)}$, $j=0, \ldots, N_{w}$, and $c_{j}^{(T)}, j=0, \ldots, N_{T}$,(\ref{eqa297})-(\ref{eqa298}) becomes

		\begin{equation}
			\label{eqa297}
			\sum_{j=0}^{N_{w}} A_{i, j}^{(w, w)} c_{j}^{(w)}+\sum_{j=0}^{N_{T}} A_{i, j}^{(w, T)} c_{j}^{(T)}=b_{i}^{(w)}, \quad i=0, \ldots, N_{w} \\
		\end{equation}
	
		\begin{equation}
			\label{eqa298}
			\sum_{j=0}^{N_{w}} A_{i, j}^{(T, w)} c_{j}^{(w)}+\sum_{j=0}^{N_{T}} A_{i, j}^{(T, T)} c_{j}^{(T)}=b_{i}^{(T)}, \quad i=0, \ldots, N_{T}, \\
		\end{equation}
	
		\begin{equation}
			\label{eqa299}
			A_{i, j}^{(w, w)}=\mu\left(\nabla \psi_{j}^{(w)}, \psi_{i}^{(w)}\right) \\
		\end{equation}
		
		\begin{equation}
			\label{eqa300}
			A_{i, j}^{(w, T)}=0 \\
		\end{equation}
	
		\begin{equation}
			\label{eqa301}	
			b_{i}^{(w)}=\left(\beta, \psi_{i}^{(w)}\right), \\
		\end{equation}
		
		\begin{equation}
			\label{eqa302}	
			A_{i, j}^{(w, T)}\left.=\mu\left(\nabla w_{-} \cdot \nabla \psi_{j}^{(w)}\right), \psi_{i}^{(T)}\right) \\
		\end{equation}

		\begin{equation}
			\label{eqa303}
			A_{i, j}^{(T, T)}=\kappa\left(\nabla \psi_{j}^{(T)}, \psi_{i}^{(T)}\right) \\
		\end{equation}
	
		\begin{equation}
			\label{304}	
			b_{i}^{(T)}=0
		\end{equation}
		
		\noindent The corresponding block form
		$$
		\left(\begin{array}{cc}
			\mu K^{(w)} & 0 \\
			L & \kappa K^{(T)}
		\end{array}\right)\left(\begin{array}{c}
			c^{(w)} \\
			c^{(T)}
		\end{array}\right)=\left(\begin{array}{c}
			b^{(w)} \\
			0
		\end{array}\right)
		$$
		has square and rectangular block matrices: $K^{(w)}$ is $N_{w} \times N_{w}, K^{(T)}$ is $N_{T} \times N_{T}$, while $L$ is $N_{T} \times N_{w}$,
	\section[Computations in 1D]{Computations in 1D}
		\label{sec:sec_20_5}
		\noindent We can reduce the system (\ref{eqa260})-(\ref{eqa261}) to one space dimension, which corresponds to flow in a channel between two flat plates. Alternatively, one may consider flow in a circular pipe, introduce cylindrical coordinates, and utilize the radial symmetry to reduce the equations to a one-dimensional problem in the radial coordinate. The former model becomes
		
		\begin{equation}
			\label{eqa305}
			\mu w_{x x}=-\beta, \\
		\end{equation}
	
		\begin{equation}
		\label{eqa306}
			kappa T_{x x}=-\mu w_{x}^{2},
		\end{equation}
		
		\noindent while the model in the radial coordinate $r$ reads
		
		\begin{equation}
			\label{eqa307}
			\mu \frac{1}{r} \frac{d}{d r}\left(r \frac{d w}{d r}\right)=-\beta \\
		\end{equation}
		
		\begin{equation}
			\label{eqa308}
			\kappa \frac{1}{r} \frac{d}{d r}\left(r \frac{d T}{d r}\right)=-\mu\left(\frac{d w}{d r}\right)^{2}
		\end{equation}
		
		\noindent The domain for (\ref{eqa305}) - (\ref{eqa306}) is $\Omega=[0, H]$, with boundary conditions $w(0)=$ $w(H)=0$ and $T(0)=T(H)=T_{0}$. For (\ref{eqa307})-(\ref{eqa308}) the domain is $[0, R]$ ( $R$ being the radius of the pipe) and the boundary conditions are $d u / d r=d T / d r=0$ for $r=0, u(R)=0$, and $T(R)=T_{0} .$\smallbreak
		\textbf{Calculations to be continued...}
	
\chapter{Exercises}
	\section*{Exercise 23: Refactor functions into a more general class}
		\label{sec:sec_21_23}
		Section \ref{sec:sec_11_2} displays three functions for computing the analytical solution of some simple model problems. There is quite some repetitive code, suggesting that the functions can benefit from being refactored into a class where the user can define the $f(x), a(x)$, and the boundary conditions in particular methods in subclasses. Demonstrate how the new class can be used to solve the three particular problems in Section \ref{sec:sec_11_2}.
		
		In the method that computes the solution, check that the solution found fulfills the differential equation and the boundary conditions. 
		Filename: \textbf{\texttt{uxx\_f\_sympy\_class.py}}.\bigbreak
	\section*{Exercise 24: Compute the deflection of a cable with sine functions}
		\label{sec:sec_21_24}
		\noindent A hanging cable of length $L$ with significant tension has a downward deflection $w(x)$ governed by
		Solve
		$$
		T w^{\prime \prime}(x)=\ell(x),
		$$
		where $T$ is the tension in the cable and $\ell(x)$ the load per unit length. The cable is fixed at $x=0$ and $x=L$ so the boundary conditions become $T(0)=T(L)=0$. We assume a constant load $\ell(x)=$ const.
		
		The solution is expected to be symmetric around $x=L / 2$. Formulating the problem for $x \in \Omega=[0, L / 2]$ and then scaling it, results in the scaled problem for the dimensionless vertical deflection $u$ :
		$$
		u^{\prime \prime}=1, \quad x \in(0,1), \quad u(0)=0, u^{\prime}(1)=0 .
		$$
		Introduce the function space spanned by $\psi_{i}=\sin ((i+1) \pi x / 2), i=1, \ldots, N$. Use a Galerkin and a least squares method to find the coefficients $c_{j}$ in $u(x)=$ $\sum_{j} c_{j} \psi_{j}$. Find how fast the coefficients decrease in magnitude by looking at $c_{j} / c_{j-1}$. Find the error in the maximum deflection at $x=1$ when only one basis function is used $(N=0)$.
		
		What happens if we choose basis functions $\psi_{i}=\sin ((i+1) \pi x)$ ? These basis functions are appropriate if we do not utilize symmetry and solve the problem on $[0, L]$. A scaled version of this problem reads
		$$
		u^{\prime \prime}=1, \quad x \in(0,1), \quad u(0)=u(1)=0 .
		$$
		Carry out the computations with $N=0$ and demonstrate that the maximum deflection $u(1 / 2)$ is the same in the problem utilizing symmetry and the problem covering the whole cable.
		Filename: \textbf{\texttt{cable\_sin.pdf}}. \bigbreak
	\section*{Exercise 25: Check integration by parts}
		Consider the Galerkin method for the problem involving $u$ in Exercise \hyperref[sec:sec_21_24]{24}. Show that the formulas for $c_{j}$ are independent of whether we perform integration by parts or not. Filename: \textbf{\texttt{cable\_integr\_by\_parts.pdf}}.\bigbreak
	\section*{Exercise 26: Compute the deflection of a cable with 2 P1 elements}
		Solve the problem for $u$ in Exercise \hyperref[sec:sec_21_24]{24} using two P1 linear elements. Filename: \textbf{\texttt{cable\_2P1.pdf}}.\bigbreak
	\section*{Exercise 27: Compute the deflection of a cable with 1 P2 element}
		Solve the problem for $u$ in Exercise \hyperref[sec:sec_21_24]{24} using one P2 element with quadratic basis functions. Filename: \textbf{\texttt{cable\_1P2.pdf.}} \bigbreak
	\section*{Exercise 28: Compute the deflection of a cable with a step load}
		\noindent We consider the deflection of a tension cable as described in Exercise \hyperref[sec:sec_21_24]{24}. Now the load is
		$$
		\ell(x)=\left\{\begin{array}{ll}
			\ell_{1}, & x<L / 2, \\
			\ell_{2}, & x \geq L / 2
		\end{array} \quad x \in[0, L] .\right.
		$$
		This load is not symmetric with respect to the midpoint $x=L / 2$ so the solution loses its symmetry and we must solve the scaled problem
		$$
		u^{\prime \prime}=\left\{\begin{array}{ll}
			1, & x<1 / 2, \\
			0, & x \geq 1 / 2
		\end{array} \quad x \in(0,1), \quad u(0)=0, u(1)=0\right.
		$$
		\begin{enumerate}
			\item[a)] Use $\psi_{i}=\sin ((i+1) \pi x), i=0, \ldots, N$ and the Galerkin method without integration by parts. Derive a formula for $c_{j}$ in the solution expansion $u=$ $\sum_{j} c_{j} \psi_{j}$. Plot how fast the coefficients $c_{j}$ tend to zero (on a log scale).
			\item[b)]Solve the problem with P1 finite elements. Plot the solution for $N_{e}=2,4,8$ elements.
		\end{enumerate}
		Filename: \textbf{\texttt{cable\_discont\_load.pdf}}.\bigbreak
	\section*{Exercise 29: Show equivalence between linear systems}
		\noindent Incorporation of Dirichlet conditions at $x=0$ and $x=L$ in a finite element mesh on $\Omega=[0, L]$ can either be done by introducing an expansion $u(x)=$ $U_{0} \varphi_{0}+U_{N_{n}} \varphi_{N_{n}}+\sum_{j=0}^{N} c_{j} \varphi_{\nu(j)}$, with $N=N_{n}-2$ and considering $u$ values at the inner nodes as unknowns, or one can assemble the matrix system with $u(x)=\sum_{j=0}^{N=N_{n}} c_{j} \varphi_{j}$ and afterwards replace the rows corresponding to known $c_{j}$ values by the boundary conditions. Show that the two approaches are equivalent.\bigbreak
	\section*{Exercise 30: Compute with a non-uniform mesh}
		\noindent Derive the linear system for the problem $-u^{\prime \prime}=2$ on $[0,1]$, with $u(0)=0$ and $u(1)=1$, using $\mathrm{P} 1$ elements and a non-uniform mesh. The vertices have coordinates $x_{0}=0<x_{1}<\cdots<x_{N}=1$, and the length of cell number $e$ is $h_{e}=x_{e+1}-x_{e}$.\smallbreak
		It is of interest to compare the discrete equations for the finite element method in a non-uniform mesh with the corresponding discrete equations arising from a finite difference method. Go through the derivation of the finite difference formula $u^{\prime \prime}\left(x_{i}\right) \approx\left[D_{x} D_{x} u\right]_{i}$ and modify it to find a natural discretization of $u^{\prime \prime}\left(x_{i}\right)$ on a non-uniform mesh. \textbf{\texttt{Filename: nonuniform\_P1 pdf}}.\bigbreak		
	\section*{Problem 31: Solve a 1D finite element problem by hand}
		\noindent The following scaled 1D problem is a very simple, yet relevant, model for convective transport in fluids:
		\begin{equation}
			\label{309}
			u^{\prime}=\epsilon u^{\prime \prime}, \quad u(0)=0, u(1)=1, x \in[0,1]
		\end{equation}
		\begin{enumerate}
			\item[a)] Find the analytical solution to this problem. (Introduce $w=u^{\prime}$, solve the first-order differential equation for $w(x)$, and integrate once more.)
			\item [b)] Derive the variational form of this problem.
			\item[c)] Introduce a finite element mesh with uniform partitioning. Use P1 elements and compute the element matrix and vector for a general element.
			\item[d)] Incorporate the boundary conditions and assemble the element contributions.
			\item[e)] Identify the resulting linear system as a finite difference discretization of the differential equation using
			$$	\left[D_{2 x} u=\epsilon D_{x} D_{x} u\right]_{i} $$
			\item[f)] Compute the numerical solution and plot it together with the exact solution for a mesh with 20 elements and $\epsilon=10,1,0.1,0.01$.
		\end{enumerate}
		\textbf{\texttt{Filename: convdiff1D\_P1.pdf}}.\bigbreak 
	\section*{Exercise 32: Compare finite elements and differences for a radially symmetric Poisson equation}
		\noindent We consider the Poisson problem in a disk with radius $R$ with Dirichlet conditions at the boundary. Given that the solution is radially symmetric and hence dependent only on the radial coordinate $\left(r=\sqrt{x^{2}+y^{2}}\right)$, we can reduce the problem to a 1D Poisson equation
		\begin{equation}
			\label{eqa310}
			-\frac{1}{r} \frac{d}{d r}\left(r \frac{d u}{d r}\right)=f(r), \quad r \in(0, R), u^{\prime}(0)=0, u(R)=U_{R} .
		\end{equation}
		\begin{enumerate}
			\item [a)] Derive a variational form of (\ref{eqa310}) by integrating over the whole disk, or posed equivalently: use a weighting function $2 \pi r v(r)$ and integrate $r$ from 0 to $R$.
			\item[b)] Use a uniform mesh partition with P1 elements and show what the resulting set of equations becomes. Integrate the matrix entries exact by hand, but use a Trapezoidal rule to integrate the $f$ term.
			\item[c)]Explain that an intuitive finite difference method applied to (\ref{eqa310}) gives
		\end{enumerate}
		$$
		\frac{1}{r_{i}} \frac{1}{h^{2}}\left(r_{i+\frac{1}{2}}\left(u_{i+1}-u_{i}\right)-r_{i-\frac{1}{2}}\left(u_{i}-u_{i-1}\right)\right)=f_{i}, \quad i=r h \text {. }
		$$\smallbreak
		For $i=0$ the factor $1 / r_{i}$ seemingly becomes problematic. One must always have $u^{\prime}(0)=0$, because of the radial symmetry, which implies $u_{-1}=u_{1}$, if we allow introduction of a fictitious value $u_{-1}$. Using this $u_{-1}$ in the difference equation for $i=0$ gives
		$$
		\begin{aligned}
			&\frac{1}{r_{0}} \frac{1}{h^{2}}\left(r_{\frac{1}{2}}\left(u_{1}-u_{0}\right)-r_{-\frac{1}{2}}\left(u_{0}-u_{1}\right)\right)= \\
			&\quad \frac{1}{r_{0}} \frac{1}{2 h^{2}}\left(\left(r_{0}+r_{1}\right)\left(u_{1}-u_{0}\right)-\left(r_{-1}+r_{0}\right)\left(u_{0}-u_{1}\right)\right) \approx 2\left(u_{1}-u_{0}\right)
		\end{aligned}
		$$
		if we use $r_{-1}+r_{1} \approx 2 r_{0}$.\smallbreak
		Set up the complete set of equations for the finite difference method and compare to the finite element method in case a Trapezoidal rule is used to integrate the $f$ term in the latter method.
		Filename:\textbf{\texttt{radial\_Poisson1D\_P1.pdf}}. \bigbreak
	\section*{Exercise 33: Compute with variable coefficients and P1 elements by hand}
		\noindent Consider the problem
		\begin{equation}
			\label{311}
			-\frac{d}{d x}\left(a(x) \frac{d u}{d x}\right)+\gamma u=f(x), \quad x \in \Omega=[0, L], \quad u(0)=\alpha, u^{\prime}(L)=\beta .
		\end{equation}
		We choose $a(x)=1+x^{2}$. Then
		\begin{equation}
			\label{312}
			u(x)=\alpha+\beta\left(1+L^{2}\right) \tan ^{-1}(x),
		\end{equation}
		is an exact solution if $f(x)=\gamma u$.\smallbreak
		Derive a variational formulation and compute general expressions for the element matrix and vector in an arbitrary element, using P1 elements and a uniform partitioning of $[0, L]$. The right-hand side integral is challenging and can be computed by a numerical integration rule. The Trapezoidal rule (101) gives particularly simple expressions. Filename: \textbf{\texttt{atan1D\_P1.pdf}}.\bigbreak
	\section*{Exercise 34: Solve a 2D Poisson equation using polynomials and sines}
		\noindent The classical problem of applying a torque to the ends of a rod can be modeled by a Poisson equation defined in the cross section $\Omega$ :
		$$
		-\nabla^{2} u=2, \quad(x, y) \in \Omega,
		$$
		with $u=0$ on $\partial \Omega$. Exactly the same problem arises for the deflection of a membrane with shape $\Omega$ under a constant load.\smallbreak
		For a circular cross section one can readily find an analytical solution. For a rectangular cross section the analytical approach ends up with a sine series. The idea in this exercise is to use a single basis function to obtain an approximate answer.\smallbreak
		We assume for simplicity that the cross section is the unit square: $\Omega=$ $[0,1] \times[0,1]$.
		\begin{enumerate}
			\item[a)] We consider the basis $\psi_{p, q}(x, y)=\sin ((p+1) \pi x) \sin (q \pi y), p, q=0, \ldots, n$. These basis functions fulfill the Dirichlet condition. Use a Galerkin method and $n=0$.
			\item[b)] The basis function involving sine functions are orthogonal. Use this property in the Galerkin method to derive the coefficients $c_{p, q}$ in a formula $u=\sum_{p} \sum_{q} c_{p, q} \psi_{p, q}(x, y) .$
			\item [c)] Another possible basis is $\psi_{i}(x, y)=(x(1-x) y(1-y))^{i+1}, i=0, \ldots, N$. Use the Galerkin method to compute the solution for $N=0$. Which choice of a single basis function is best, $u \sim x(1-x) y(1-y)$ or $u \sim \sin (\pi x) \sin (\pi y)$ ? In order to answer the question, it is necessary to search the web or the literature for an accurate estimate of the maximum $u$ value at $x=y=1 / 2$.
		\end{enumerate}
			Filename: \textbf{\texttt{torsion\_sin\_xy.pdf}}.\bigbreak
	\section*{Exercise 35: Analyze a Crank-Nicolson scheme for the diffusion equation}		
		\noindent Perform the analysis in Section \ref{sec:sec_19_10} for a $1 \mathrm{D}$ diffusion equation $u_{t}=\alpha u_{x x}$ discretized by the Crank-Nicolson scheme in time:
		$$
		\frac{u^{n+1}-u^{n}}{\Delta t}=\alpha \frac{1}{2}\left(\frac{u^{n+1}}{\partial x^{2}} \frac{u^{n}}{\partial x^{2}}\right)
		$$
		or written compactly with finite difference operators,
		$$
		\left[D_{t} u=\alpha D_{x} D_{x} \bar{u}^{t}\right]^{n+\frac{1}{2}}
		$$
		(From a strict mathematical point of view, the $u^{n}$ and $u^{n+1}$ in these equations should be replaced by $u_{e}^{n}$ and $u_{e}^{n+1}$ to indicate that the unknown is the exact solution of the PDE discretized in time, but not yet in space, see Section \ref{sec:sec_19_1}.) Make plots similar to those in Section \ref{sec:sec_19_10}. 
		Filename: \textbf{\texttt{fe\_diffusion.pdf}}.\bigbreak

%\begin{theindex}
%	\item 
%		\subitem 
%		\subitem 
%	\indexspace
%	\item 
%		\subitem 
%		\subitem 
%			\subsubitem 
%			\subsubitem 
%\end{theindex}

\end{document} 
